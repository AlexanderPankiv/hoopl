\newif\ifpagetuning \pagetuningtrue  % adjust page breaks

\newif\ifnoauthornotes\noauthornotesfalse

\newif\ifgenkill\genkillfalse  % have a section on gen and kill
\genkilltrue


\newif\ifnotesinmargin \notesinmargintrue 
\IfFileExists{notesinline.tex}{\notesinmarginfalse}{\relax}

\documentclass[blockstyle,preprint,natbib,nocopyrightspace]{sigplanconf}


\usepackage{alltt}
\usepackage{array}
\newcommand\lbr{\char`\{}
\newcommand\rbr{\char`\}}
 
\clubpenalty=10000
\widowpenalty=10000

\usepackage{verbatim} % allows to define \begin{smallcode}
\newenvironment{smallcode}{\par\unskip\small\verbatim}{\endverbatim}

\newcommand\lineref[1]{line~\ref{line:#1}}
\newcommand\linepairref[2]{lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\linerangeref[2]{\mbox{lines~\ref{line:#1}--\ref{line:#2}}}
\newcommand\Lineref[1]{Line~\ref{line:#1}}
\newcommand\Linepairref[2]{Lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\Linerangeref[2]{\mbox{Lines~\ref{line:#1}--\ref{line:#2}}}

\makeatletter

\newcommand\setlabel[1]{%
  \setlabel@#1!!\@endsetlabel
}
\def\setlabel@#1!#2!#3\@endsetlabel{%
  \ifx*#1*% line begins with label or is empty
     \ifx*#2*% line is empty
        \verbatim@line{}%
     \else
       \@stripbangs#3\@endsetlabel%
       \label{line:#2}%
     \fi
  \else
     \@stripbangs#1!#2!#3\@endsetlabel%
  \fi
}
\def\@stripbangs#1!!\@endsetlabel{%
  \verbatim@line{#1}%
}


\verbatim@line{hello mama}

\newcounter{codeline}
\newenvironment{numberedcode}
  {\endgraf
     \def\verbatim@processline{%
        \noindent
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\phantom{: \,}}}%
            \else
               \refstepcounter{codeline}%
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\llap{\arabic{codeline}}: \,}}%
            \fi
        \expandafter\setlabel\expandafter{\the\verbatim@line}%
        \the\verbatim@line\par}%
   \verbatim
   }
   {\endverbatim}

\makeatother

\newcommand\arrow{\rightarrow}

\newcommand\join{\sqcup}
\newcommand\slotof[1]{\ensuremath{s_{#1}}}
\newcommand\tempof[1]{\ensuremath{t_{#1}}}
\let\tempOf=\tempof
\let\slotOf=\slotof

\makeatletter
\newcommand{\nrmono}[1]{%
  {\@tempdima = \fontdimen2\font\relax
   \texttt{\spaceskip = 1.1\@tempdima #1}}}
\makeatother

\usepackage{times}  % denser fonts
\renewcommand{\ttdefault}{aett} % \texttt that goes better with times fonts
\usepackage{enumerate}
\usepackage{url}
\usepackage{graphicx}
%\usepackage{natbib}
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar

\usepackage[ps2pdf,bookmarksopen,breaklinks,pdftitle=dataflow-made-simple]{hyperref}

\newcommand\naive{na\"\i ve}

\usepackage{amsfonts}
\newcommand\naturals{\ensuremath{\mathbb{N}}}
\newcommand\true{\ensuremath{\mathbf{true}}}
\newcommand\implies{\supseteq}  % could use \Rightarrow?

\newcommand\PAL{\mbox{C{\texttt{-{}-}}}}
\newcommand\high[1]{\mbox{\fboxsep=1pt \smash{\fbox{\vrule height 6pt
   depth 0pt width 0pt \leavevmode \kern 1pt #1}}}}

% Put figures in boxes
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}



% Set \noauthornotestrue to suppress notes
%\newcommand{\qed}{QED}
\ifnotesinmargin
  \long\def\authornote#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \def\baselinestretch{0.8}\tiny
         \it #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \def\baselinestretch{0.8}\tiny
         \it #1\par}%
      \fi}
\else
  % Simon: please set \notesinmargingfalse on the first line
  \newcommand{\authornote}[1]{{\em #1}}
\fi
\ifnoauthornotes
  \def\authornote#1{\unskip\relax}
\fi

\newcommand{\simon}[1]{\authornote{SLPJ: #1}}
\newcommand{\norman}[1]{\authornote{NR: #1}}
\let\remark\norman
\def\finalremark#1{\relax}
% \let \finalremark \remark % uncomment after submission
\newcommand{\john}[1]{\authornote{JD: #1}}
\newcommand{\todo}[1]{\textbf{To~do:} \emph{#1}}

\newcommand\secref[1]{Section~\ref{sec:#1}}
\newcommand\secreftwo[2]{Sections \ref{sec:#1}~and~\ref{sec:#2}}
\newcommand\seclabel[1]{\label{sec:#1}}

\newcommand\figref[1]{Figure~\ref{fig:#1}}
\newcommand\figlabel[1]{\label{fig:#1}}


\newcommand{\CPS}{\textbf{StkMan}}    % Not sure what to call it.


\usepackage{code}   % At-sign notation

\input{timestamp}
\preprintfooter{\mdfivestamp}

\begin{document}
\title{Dataflow Optimization Made Simple}
%\subtitle{\today}

\authorinfo{Norman Ramsey}{Tufts University}{nr@cs.tufts.edu}
\authorinfo{Jo\~ao Dias}{Tufts University}{dias@cs.tufts.edu}
\authorinfo{Simon Peyton Jones}{Microsoft Research}{simonpj@microsoft.com}


\maketitle
 
\begin{abstract}
We present a Haskell library that makes it easy for compiler writers
to implement program transformations based on dataflow analyses.
The compiler writer must identify (a)~a family of logical assertions
on which the transformation will be based;
(b)~an \emph{approximate} representation of such assertions, which
must have a lattice structure such that every assertion can be increased at
most finitely many times;
(c)~transfer functions that approximate weakest preconditions or
strongest postconditions over the assertions; and
(d)~rewrite functions whose soundness is justified by the assertions.
To~guide compiler writers,
we show how dataflow analyses are related to
seminal work on program 
correctness. 
Finally, our library uses the algorithm of 
\citet{lerner-grove-chambers:2002}, which enables compiler writers to
compose very simple analyses and transformations in a way that achieves
the same precision as complex, handwritten
``super-analyses.''
Our library is the workhorse of a new
back end for the Glasgow Haskell Compiler.
\end{abstract}

\makeatactive   %  Enable @foo@ notation

\section{Introduction}

\ifpagetuning\enlargethispage{\baselineskip}\fi

If you write a compiler for an imperative language, you can exploit
many years' work on code improvement, also called
``optimization.''
But the work is typically presented
as a collection of apparently unrelated analyses and
transformations, each with its own name---and 
it's not always easy to % not everyone can easily
remember how something like
``copy propagation'' differs from ``constant propagation.''
Many presentations obscure fundamental principles of
code improvement.

%The contribution of this paper is to elucidate a large body of work on code
%improvement; the body of work known as ``dataflow optimization.''
This paper makes two contributions:
\begin{itemize}
\item
We show that the ad-hoc ``optimization zoo'' consists mostly of special
cases of reasoning techniques that have long been understood and used
by semanticists and functional programmers:
assertions about states, assertions about continuations, and
substitution of equals for equals (\secref{next-700}).
What distinguishes dataflow optimization from classic formal reasoning
about programs is that in dataflow optimization, all assertions are
computed automatically, and they are
\emph{approximated}. 
\item
We embody our ideas in a library that makes it not just
possible but \emph{easy} to adapt imperative, dataflow-based
code-improvement techniques to a purely functional compiler.
Analyses and transformations, which use the library,
are small, simple, and easy to get right.
The library itself
\ifpagetuning uses \else is built around \fi
 a sophisticated algorithm which is
hard to get right but is 
written once and reused.
\end{itemize}

We consider code improvements over low-level
imperative codes, including intermediate languages and machine
languages.
As \citet{benitez-davidson:portable-optimizer} have shown, all the
classical scalar and loop optimizations can be performed over such
codes.
Moreover, any compiled functional program is
translated to such a code.


We introduce the subject by analyzing and transforming example code 
(\secref{example:xforms}),
thinking about and justifying classical optimizations using
Hoare logic and substitution of equals for equals.
To~support our claim that we make dataflow optimization easy, 
we spend most of the paper explaining how
a to create new dataflow analyses or transformations
(\secref{making-simple}) and showing complete implementations of significant
analyses (\secref{example-analyses}) and transformations
(\secref{example-rewrites}) from the Glasgow Haskell Compiler.
We spend less space on our library's implementation (\secref{engine}).


\section{Dataflow analysis {\&} transformation by \rlap{example}}

\seclabel{example:transforms}
\seclabel{example:xforms}

In dataflow optimization, code-improving transformations are justified
by assertions about programs;
such assertions are often computed using
strongest postconditions or weakest liberal preconditions.
The most typical transformations are
insertion of assignments to unobserved variables,
substitution of equals for equals, 
and
removal of assignments to unobserved variables,
all of which preserve semantics.
Insertion and removal are often composed to achieve the effect called
``code motion,'' as by \citet{knoop:lazy-code-motion}, for example.
The examples below express classical code
improvements by composing small analyses and transformations.



\ifpagetuning \enlargethispage{1\baselineskip} \fi 
    % gets bottoms of columns to match

\subsection{Simple transformations}

\seclabel{constant-propagation}

Here is a sequence of assignments separated by assertions.
We compute the assertions by starting with the weakest possible
assertion (@true@) and computing strongest postconditions.
% \footnote
{Variables do not alias.}
\begin{verbatim}
    { true }
  x = 7;
    { x == 7 }
  y = 8: 
    { x == 7 && y == 8 }
  z = x + y;
\end{verbatim}
In the assignment to~@z@, the assertion @x == 7@ justifies
substituting 7~for~@x@, leaving @z = 7 + y@.  
This transformation is traditionally called ``constant propagation.''
We may also substitute 8~for~@y@.
Finally, because @7 + 8 == 15@, we may again substitute equals for
equals, leaving the final assignment as
\begin{verbatim}
  z = 15;
\end{verbatim}
The final transformation, although also an instance of substituting equals
for equals, has a different name: ``constant folding.''

\subsection{A complex transformation}

\finalremark{It's a pity that this transformation occupies nearly the
entire second page,  
and then plays no subsequent role in the paper whatsoever.
One possibility: move it to ``the next 700'' section, as a substantiating example
to the claims made there.
But then we'd need another example here... well the sink/reload example of
Section 4 might be perfect.}

\seclabel{induction-var-elim}


The loop optimization known as ``induction-variable elimination'' 
can be composed from simpler transformations.
We begin by showing a simple loop, FORTRAN style,
although the code is~C:
\begin{verbatim}
  struct pixel { double r, g, b; };
  double sum_r(struct pixel a[], int n) {
    double x = 0.0;
    int i;
    for (i = 0; i < n; i++)
      x += a[i].r;
    return x;
  }
\end{verbatim}
To explain the improvement we wish to make, we show the same
code at the machine level.
We write machine-level examples using our low-level compiler-target
language,~{\PAL} % tuning for line breaks
\cite{peyton-jones-ramsey:garbage-collection:inproceedings,peyton-jones-ramsey:exceptions}: 
\begin{verbatim}
  sum_r("address" bits32 a, bits32 n) {
       bits64 x; bits32 i;
       x = 0.0;
       i = 0;
   L1: if (i >= n) goto L2;
       x = %fadd(x, bits64[a+i*24]);
       i = i + 1;
       goto L1;
   L2: return x; 
  }
\end{verbatim}
The code improvement called ``induction-variable elimination''
replaces~@i@ with a new variable~@p@ so that we can avoid repeating
the computation @a+i*24@ on each iteration through the loop.
The new variable~@p@ is intended to satisfy the invariant
\begin{verbatim}
   { p == a + i * 24 }
\end{verbatim}
The variable @i@ is also used in the loop-termination test.
To rewrite that test, 
we introduce a new variable @lim@ satisfying
the invariant % line breaking
@lim == a + n * 24@,
so that @i >= n@ if and only if @p >= lim@.

We implement the code improvement as a sequence of transformations.
After each transformation, the observable behavior of the program is unchanged.
%
Our first transformation declares @p@ and @lim@ and inserts suitable
assignments. 
New code is \high{boxed}\,.
\begin{alltt}
  sum_r("address" bits32 a, bits32 n) \lbr
       bits64 x; bits32 i; \high{bits32 p, lim;}
       x = 0.0;
       i = 0; \high{p = a; lim = a + n * 24;}
   L1: if (i >= n) goto L2;
       x = %fadd(x, bits64[a+i*24]);
       i = i + 1; \high{p = p + 24;}
       goto L1;
   L2: return x; 
  \rbr
\end{alltt}

As written, the assignments to @p@~and~@lim@ have no
effect on the program, but they enable the compiler to establish the assertions
@p == a + i * 24@ and @(i >= n) == (p >= lim)@.
On~the basis of these assertions, the compiler substitutes equals for
equals, resulting in the new code in boxes below:
\begin{alltt}
  sum_r("address" bits32 a, bits32 n) \lbr
       bits64 x; bits32 i; bits32 p, lim;
       x = 0.0;
       i = 0; p = a; lim = a + n * 24;
   L1: if (\high{p >= lim}) goto L2;
       x = %fadd(x, bits64[\kern1pt{}\high{p}\kern1pt{}]);
       i = i + 1; p = p + 24;
       goto L1;
   L2: return x; 
  \rbr
\end{alltt}

At this point, the compiler switches from reasoning about states to
reasoning about continuations.
In~particular, we reason about whether the value of a variable can be
used by a continuation; this reasoning is called ``liveness analysis.''
A~\naive\ analysis would show that although @i@~is not live at
label~@L2@, it is nevertheless live immediately after 
the assignment
@i = i + 1@ in the loop body,
because the value of~@i@ could be used by the next iteration of the
loop.
But we use Lerner, Grove, and Chambers's
\citeyearpar{lerner-grove-chambers:2002} algorithm to
\emph{interleave} liveness analysis with 
``dead-assignment elimination.'' 
Dead-assignment elimination removes an assignment if the variable
assigned to is not live, that is, if it cannot be used by the
assignment's continuation.
As~explained by Lerner, Grove, and Chambers, no sequential
composition of liveness analysis and dead-assignment elimination can
get rid of these assignments to~@i@, but interleaving analysis with
transformation does the trick.\footnote
{An experienced reader might be tempted to modify the
liveness analysis so that
@i = i + 1@ is not considered a ``use'' of~@i@ if @i@~is itself
dead.
This~modification is tantamount to writing a single, combined dataflow
pass that understands \emph{both} liveness analysis and dead-code
elimination.
In~this particular example, writing a single, combined pass presents
few difficulties, but the approach does not scale:
most combined passes are more complicated than the examples shown here;
the cost of writing combined passes does not scale linearly with
the number of individual passes;
combined passes often cannot be composed;
and 
some combined passes require nonstandard, handwritten traversals of
the control-flow graph.
\citet{lerner-grove-chambers:2002} discuss these issues in detail;
\citet{click-cooper} present a handwritten combined pass of
significant complexity.}
\secref{dfengine} describes our implementation of their interleaving
method, which eliminates the boxed assignments to~@i@:
\begin{alltt}
sum_r("address" bits32 a, bits32 n) \lbr
     bits64 x; \high{bits32 i;} bits32 p, lim;
     x = 0.0;
     \high{i = 0;} p = a; lim = a + n * 24;
 L1: if ({p >= lim}) goto L2;
     x = %fadd(x, bits64[{p}]);
     \high{i = i + 1;} p = p + 24;
     goto L1;
 L2: return x; 
\rbr
\end{alltt}

After the insertion of assignments to @p@~and~@lim@, the substitution
of equals for equals, the removal of newly dead assignments
to~@i@, we have ``eliminated the induction variable:''
\begin{alltt}
sum_r("address" bits32 a, bits32 n) \lbr
     bits64 x; bits32 p, lim;
     x = 0.0;
     p = a; lim = a + n * 24;
 L1: if ({p >= lim}) goto L2;
     x = %fadd(x, bits64[{p}]);
     p = p + 24;
     goto L1;
 L2: return x; 
\rbr
\end{alltt}

  
\section {Making dataflow simple}

\seclabel{making-simple}

\seclabel{create-analysis}

The goal of dataflow optimization is to compute valid
assertions, then use those assertions to justify code-improving
transformations.
%
% only Don Knuth knows why, but the paragraph break gets us three
% bullets on this page where a single paragraph gets only two!
%
Assertions are represented as
\emph{dataflow facts}.
Dataflow facts relate to
 traditional 
program logic as follows:
\begin{itemize}
\item
A dataflow fact is usually equivalent to an assertion about program state or
about a continuation.
For example, in \secref{constant-propagation}, @x == 7@ is a dataflow
fact that describes the program state. 


%%  There are two kinds of dataflow facts.
%%  The first kind is an assertion about the paths from the procedure
%%  entry to a program point;
%%  these facts are computed by a forward dataflow analysis.
%%  A common special case is an assertion about state at a program point,
%%  such as the assertion @x == 7@ in \secref{constant-propagation}.
%%  % Assertions about program state are usually sufficient to show that
%%  % a transformation preserves semantics,
%%  % but to decide whether a transformation will improve the code,
%%  % we sometimes need an assertion that describes how the program
%%  % state was established.
%%  
%%  The second kind of dataflow fact is an assertion about paths from the program point
%%  to the procedure exit;
%%  these facts are computed by a backward dataflow analysis.
%%  In the parlance of functional programmers, these dataflow facts are assertions on
%%  \emph{continuations}.
%%  
%%  Assertions about state are easy to formalize, but path properties are harder;
%%  we describe path properties informally in our assertions.
\item
A~set of dataflow facts forms a lattice.
To ensure that analysis terminates,
it is enough if
no fact has more than finitely many distinct facts above it.
\item
Each analysis or transformation may use a different lattice of
dataflow facts.
\end{itemize}

An assertion about a continuation is an assertion about paths
\emph{from} a program point 
to the procedure {exit};
such assertions are established by a \emph{backward dataflow analysis}.
An~assertion about paths \emph{to} a program point from the procedure
{entry} is established by a \emph{forward dataflow analysis}.
In an important special case,
an assertion may say simply
that all paths to a point establish a predicate, such as @x == 7@
above, which describes the program 
state at that point.


A~program point is represented as an edge in
a \emph{control-flow graph}.\footnote
{We discuss only intraprocedural optimization;
interprocedural optimizations are the work of the GHC~inliner
\cite{peyton-jones:secrets-inliner}.} 
The edges connect nodes, each of which represents a label, an assignment, or
a control transfer.

To write a dataflow analysis, the compiler
writer must 
\begin{itemize}
\item
Choose a representation~$F$ of dataflow facts and a logical interpretation
thereof.
\item
Implement lattice operations over~$F$.
\item
Write \emph{transfer functions} that relate dataflow facts before and
after each type of node.
\end{itemize}

To write a transformation based on an analysis, the compiler writer
must create a \emph{rewrite function}, which is presented with a
flow-graph node and with the dataflow facts on the edges coming
into that node.
The function either suggests that the node should be replaced with a
fresh subgraph, or it leaves the node alone.
The rewrite function uses incoming facts to guarantee that
any proposed replacement preserves semantics.
For example, in \secref{constant-propagation} the fact @x == 7@ is
used to justify replacing @z = x + y@ with @z = 7 + y@.

Our library defines the types of lattice operations,
control-flow graphs, transfer functions, and rewrite functions.
\emph{All}~these types are parameterized by the types of
nodes in the control-flow graph and the types of dataflow facts, so
the library can be used with many intermediate languages and
dataflow facts.
Moreover, client code may define many different analyses, involving
different types of dataflow facts, all operating over a single type of
graph. 

In~addition to the abstractions listed above, the main part of our
dataflow library is the \emph{dataflow engine}:  
polymorphic functions that compute 
dataflow facts (\secref{zdfSolveFwd}) and implement
semantics-preserving transformations (\secref{rewrites}).
These functions are higher-order, taking lattice operations, transfer
functions, and rewrite functions as arguments.
Their implementation is sketched in \secref{dfengine}.

%%  Given lattice operations and transfer functions, our
%%  polymorphic, reusable \emph{dataflow engine} computes a
%%  set of dataflow facts, one for each edge in the control-flow graph 
%%  (\secref{zdfSolveFwd}).
%%  The set of facts is a fixed point of the transfer functions.
%%  Given, in addition, a rewrite function for each type of node,
%%  the dataflow engine implements a semantics-preserving transformation
%%  (\secref{rewrites}). 

\begin{figure}
\begin{code}
data ChangeFlag = NoChange | SomeChange
data TxRes a    = TxRes ChangeFlag a
data DataflowLattice a = DataflowLattice
 {fact_bot        :: a,
  fact_add_to     :: a -> a -> TxRes a,
  fact_name       :: String } -- for debugging
\end{code}
\caption{Representation of a dataflow lattice} \figlabel{lattice-type} \figlabel{lattice}
\end{figure}


\subsection{Dataflow lattices}


As an example, 
we present a lattice of facts about constant propagation.
At any program point, a standard constant-propagation analysis
computes exactly one of three
facts about a variable~$x$:
\begin{itemize}
\item
The analysis shows that
$x = k$, where $k$~is a compile-time constant of type @Const@.
\item
The analysis shows that $x$~is \emph{not} a compile-time constant.
We~notate this fact as $x = \top$.
\item
The analysis shows nothing about~$x$, which we notate $x=\bot$.
\end{itemize}
The bottom element of the lattice is~$x=\bot$, and
the join operation \emph{approximates} disjunction.
In the lattice, a~disjunction of two inconsistent facts is represented by~$x=\top$.
Here are some examples:
\begin{itemize}
\item
$i = 7 \lor i=\bot \equiv i=7$ (no loss of information)
\item
$i = 7 \lor i= 7 \equiv  i=7$ (no loss of information)
\item
$i = 7 \lor i = 8 \equiv i = \top$ (loss of information)\footnote
{Client code determines how much information is lost.
For example, in a similar analysis for a functional language,
you might wish to track whether a value~$v$ is known to
be the application of a value constructor~$C_i$.
In this analysis, there's no need to limit the representation to a
single constructor or to~$\top$;
you could choose to represent such facts as ``$v$~is
the application of a constructor drawn from the set $\{C_1, C_2,
C_4\}$.''
Because every algebraic data type has finitely many constructors,
there are finitely many sets and therefore finitely many facts in the
lattice, so a dataflow analysis over this lattice would always reach a
fixed point.
}
%%  The compiler writer gets to choose how much information is lost;
%%  in a different analysis, it might be useful to 
%%  choose a lattice which can say
%%  that the value of a variable is one of,
%%  say, at most three constants.

\end{itemize}

The lattice  used by the analysis is the Cartesian product of the
lattices for all the local variables.
We~represent this lattice as a finite map from variable
to a value of type @Maybe Const@.
If~a variable $x$ is not in the domain of the map then $x=\bot$;
if $x$~maps to @Nothing@ then $x=\top$; if $x$~maps to $@Just@\;k$ then
$x=k$.


The dataflow engine uses the lattice join operation in a stylized way.
Joins occur at labels.
If~$f_{\mathit{id}}$ is the fact currently associated with the
label~$\mathit{id}$, 
and if a transfer function propagates a new fact~$f_{\mathit{new}}$
into the label~$\mathit{id}$, 
the dataflow engine replaces $f_{\mathit{id}}$ with
the join  $f_{\mathit{new}} \join f_{\mathit{id}}$.
Furthermore, the dataflow engine wants to know if
  $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$,
because if not, it has not reached a fixed point.

In the example above, in any one program there are only finitely many variables;
only finitely many facts are computed at any program point;
and any one fact can increase at most twice.
These properties
ensure that the dataflow engine will
reach a fixed point.



When computing a join, 
it is typically cheap to learn if the join
is equal to one of the arguments.
We therefore use the nonstandard representation of lattice operations
shown in \figref{lattice}.
The join operation~$\join$ and equality test~$=$ are represented by a
single function called @fact_add_to@.
The term $@fact_add_to@\;f_{\mathit{new}}\;f_{\mathit{id}}$ is equal to
$@TxRes NoChange@\; f_{\mathit{id}}$ if $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$
and is equal to
$@TxRes SomeChange@\; (f_{\mathit{new}} \join f_{\mathit{id}})$ otherwise.
The @fact_bot@ value is the bottom element, 
and @fact_name@  is used for debugging.

\begin{figure}
\begin{code}
newtype LastOuts a = LastOuts [(BlockId, a)]
data ForwardTransfers mid last a = ForwardTransfers
 {ft_first_out  :: BlockId -> a -> a,
  ft_middle_out :: mid     -> a -> a,
  ft_last_outs  :: last    -> a -> LastOuts a} 

data BackTransfers mid last a = BackTransfers
 {bt_first_in  :: BlockId -> a              -> a,
  bt_middle_in :: mid     -> a              -> a,
  bt_last_in   :: last    -> (BlockId -> a) -> a} 
\end{code}
\caption{Transfer functions for forward and backward analyses.}
\figlabel{transfers}
%
% elided: 
%    ft_exit_out   ::            a -> a
%
\end{figure}



\subsection{Transfer functions} \seclabel{tffuns}

A~transfer function is presented with dataflow facts on edges coming
into a node, and it computes dataflow facts on outgoing edges.
To~understand transfer functions, we must 
understand how the library organize the nodes and edges of a control-flow graph.

A~control-flow graph is a collection of labelled \emph{basic blocks}.
Each basic block is a sequence beginning with a \emph{first node},
containing zero or more \emph{middle nodes},
and ending in a \emph{last node}.
A~first node is always a label;
a~typical middle node assigns to a register or memory
location;
a~typical last node is a conditional, unconditional, or indirect branch.
The client code choose the types of middle and last nodes, and in this
way defines its intermediate
representation.




First nodes are the only targets of control transfers;
middle nodes never perform control transfers;
and
last nodes always perform control transfers.
Hence, a~first node has arbitrarily many predecessors and exactly one
successor;
a~middle node has exactly one predecessor and one successor;
and a last node has exactly one predecessor and arbitrarily many
successors. 

These constraints on number of predecessors and successors determine
the signatures of 
transfer functions, 
which are shown in \figref{transfers}.
For each type of node (first, middle, last) and for each kind of
analysis (forward, backward), there is a distinct transfer function.
Functions are grouped by kind of analysis, and each group is
parameterized over a dataflow fact of type~@a@ and over the types
@mid@ and @last@ of middle and last nodes.  


The type @BlockId@ represents a label.
Because a fact in a forward analysis typically represents an assertion
about program state,
 and because passing a label does not change
program state, the transfer function @ft_first_out@ is typically 
@flip const@---a variation on
the
identity function.\footnote
{One counterexample is in an
analysis, similar to a dominator analysis,
which we use to help convert {\PAL} to continuation-passing style.
The  analysis
helps us not to duplicate code that is common to multiple continuations.
}
For a middle node, the transfer function @ft_middle_out@ is given a
node and a precondition and returns an approximation of the strongest
postcondition. 
For a last node, different postconditions may be propagated to
different successors; for example, the true and false successors of a
conditional branch may accumulate information implied by the truth or
falsehood of the condition.
A~collection of (successor, fact) pairs is represented by a value of
type @LastOuts a@.




In a forward analysis, the dataflow engine starts with the fact at the
beginning of a block and applies transfer functions to the nodes in
that block until eventually the transfer function for the last node
computes the facts that are propagated to the block's successors.
For example, in the basic block
\begin{verbatim}
  L1: x = 7;
      y = 8;
      z = x + y;
      goto L2;
\end{verbatim}
a forward analysis would propagate the fact 
$@x == 7@ \land @y == 8@$, which we will call $f_{\mathit{new}}$,
along the edge to~@L2@. 
%%  \remark{We've elected not to get to the level of detail where
%%  we show how propagating a fact~$f$ through \mbox{@x = 7;@} results in a new
%%  fact either $(f \setminus @x@) \land @x == 7@$.}
The dataflow engine then \emph{replaces} the current fact
at~@L2@~($f_{\mathtt{L2}}$) with the lattice join $f_{\mathit{new}}
\join f_{\mathtt{L2}}$. 
The dataflow engine iterates over the blocks repeatedly, creating new
facts~$f$ and joining them with facts $f_{\mathit{id}}$ until
\mbox{$f \join f_{\mathit{id}} = f_{\mathit{id}}$} at every label~$\mathit{id}$.
When the facts at labels stop changing, the dataflow
engine has reached a fixed point.
%\remark{Promissory note: compose this analysis with two
%transformations: constant propagation and constant folding}


\ifpagetuning\enlargethispage{0.5\baselineskip}\fi


\subsection{Running the dataflow engine}

\seclabel{zdfSolveFwd}

Given lattice operations of type @DataflowLattice a@
and transfer functions of type @ForwardTransfers m l a@,
the compiler writer can run the analysis by calling the
dataflow-engine function @zdfSolveFwd@:
\begin{code}
 zdfSolveFwd 
  :: PassName               -- Name of this analysis
  -> DataflowLattice a      -- Lattice
  -> ForwardTransfers m l a -- Transfer functions
  -> a                      -- Input fact
  -> Graph m l              -- Control-flow graph
  -> ForwardFixedPoint m l a
\end{code}
The function is polymorphic in the types of middle and last nodes
@m@~and~@l@ and in the type of the dataflow fact~@a@.
The first three arguments characterize the analysis.
The next argument is the dataflow fact that holds on entry to the
graph;
because a procedure's caller may establish some facts about
parameters or about the stack,
this fact
is not always~$\bot$.
The last argument to @zdfSolveFwd@ is the graph, and the result is a 
fixed point.\footnote
{The type of @zdfSolveFwd@ has pedagogical lies;
truth is told in~\secref{engine-truth}.}
%%  \remark{One of the lies is
%%  that I've deliberately conflated the three representations of graphs.}

The @ForwardFixedPoint@ data structure is a big bag
of information about the solution.
The most significant information is
a finite map from each block label to the dataflow fact that holds at
the label, which is extracted using function @zdfFpFacts@:
% Simon really wants these type signatures!
\begin{verbatim}
type BlockEnv a = Data.Map BlockId a
zdfFpFacts :: ForwardFixedPoint m l a -> BlockEnv a
\end{verbatim}






\iffalse
% never tell the whole truth!
\begin{code}
class DataflowSolverDirection
        transfers fixedpt where
  zdfSolveFrom :: (DebugNodes m l, Outputable a)
    => BlockEnv a        -- Init facts
    -> PassName          -- Analysis name
    -> DataflowLattice a -- Lattice
    -> transfers m l a   -- Transfers
    -> a                 -- Input fact
    -> Graph m l         -- CFG
    -> DFMonad (fixedpt m l a ())
\end{code}
\fi



%%  The main contribution of this paper is to present an interface which
%%  enables many powerful program transformations based on dataflow
%%  analysis while keeping the individual dataflow passes as simple as
%%  possible.
%%  We keep the \emph{concepts} simple by relating dataflow facts and
%%  transfer functions to classic work in program correctness, as
%%  discussed in \secref{next-700}.
%%  We keep the \emph{implementations of dataflow passes} simple by pushing
%%  as much work as
%%  possible into the dataflow engine, which is implemented just once.
%%  We have also made the dataflow engine and its interface polymorphic in
%%  the types of 
%%  the nodes that appear in the control-flow graph \secref{polymorphic-framework}.
%%  Parametricity ensures separation of concerns between the dataflow
%%  engine and the individual dataflow passes.


\section{Example analysis passes}

\seclabel{example-analyses}


\newcommand\T{\rule{0pt}{0.6ex}}
\newcommand\B{\rule[-0.05ex]{0pt}{0pt}}
\newcolumntype{C}{>{\begin{minipage}{5.35in}}l<{\end{minipage}}} % code
\newcolumntype{L}{>{\Large\bfseries}m{1.3in}<{\centering}}       % label
\newenvironment{codetable}
  {\setcounter{codeline}{0}%
   \let\code=\numberedcode
   \let\endcode=\endnumberedcode
   \begin{tabular}{CL}%
  }
  {\end{tabular}}

\begin{figure*}
\setcounter{codeline}{0}
\begin{codetable}
\T\begin{numberedcode}
!UniverseMinus!data AvailVars = UniverseMinus VarSet
!AvailVars!               | AvailVars     VarSet
!extendAvail!extendAvail  :: AvailVars -> LocalVar  -> AvailVars  -- add var to set
delFromAvail :: AvailVars -> LocalVar  -> AvailVars  -- remove var from set
elemAvail    :: AvailVars -> LocalVar  -> Bool       -- set membership
interAvail   :: AvailVars -> AvailVars -> AvailVars  -- set intersection
!smallerAvail!smallerAvail :: AvailVars -> AvailVars -> Bool       -- compare sizes
\end{numberedcode}%
\B
& Dataflow fact and operations\\
\hline

\T\begin{code}
availVarsLattice :: DataflowLattice AvailVars
availVarsLattice = DataflowLattice "reloaded registers" empty add
    where empty = UniverseMinus emptyVarSet
          add new old = let join = interAvail new old in
                        if join `smallerAvail` old then aTx join else noTx join
\end{code}%
\B
& Lattice\\
\hline

%%  \T\begin{code}
%%  agen  :: UserOfLocalVars    a => a -> AvailVars -> AvailVars
%%  akill :: DefinerOfLocalVars a => a -> AvailVars -> AvailVars
%%  agen  a avail = foldVarsUsed extendAvail  avail a
%%  akill a avail = foldVarsDefd delFromAvail avail a
%%  \end{code}\B
%%  & Gen/Kill \mbox{functions}\\
%%  \hline
%%  
\T\begin{code}
avail_vars_transfer :: ForwardTransfers Middle Last AvailVars
!avail.first!avail_vars_transfer = ForwardTransfers (flip const) middleAvail lastAvail

middleAvail :: Middle -> AvailVars -> AvailVars
!reload1!middleAvail (MidAssign (CmmLocal x) (CmmLoad l) avail
!reload2!                 | l `isStackSlotOf` x = extendAvail avail x
!assign.avail.1!middleAvail (MidAssign lhs _expr) avail = 
!assign.avail.2!  foldVarsDefd delFromAvail avail lhs  -- remove variables defined in 'lhs'
!store.avail.spill.1!middleAvail (MidStore l (CmmReg (CmmLocal x))) avail
!store.avail.spill.2!                 | l `isStackSlotOf` x = avail
!store.avail.otherslot.1!middleAvail (MidStore l _) avail 
!store.avail.otherslot.2!  | isStackSlot l = delFromAvail avail (variableOfSlot l)
!store.avail.other!middleAvail (MidStore {}) avail = avail

lastAvail :: Last -> AvailVars -> LastOuts AvailVars
!avail.LastCall!lastAvail (LastCall _ (Just k) _ _) _ = LastOuts [(k, AvailVars emptyVarSet)]
lastAvail l avail = LastOuts $ map (\id -> (id, avail)) $ succs l
\end{code}%
\B
& Transfer \mbox{functions}\\
\hline

\T\begin{code}
cmmAvailableVars :: Graph Middle Last -> BlockEnv AvailVars
cmmAvailableVars g = zdfFpFacts soln
!avail.solve.1!  where soln = zdfSolveFwd "available variables" availVarsLattice 
!avail.solve.2!               avail_vars_transfer (fact_bot availVarsLattice) g
\end{code}%
\B
& Available-variables analysis\\
%\hline

\end{codetable}
% \caption{Available-variable analysis}
\caption{Dataflow analysis pass to compute available variables}
\figlabel{avail-all}
\figlabel{avail}
\figlabel{avail-lattice}
\figlabel{avail-gen-kill}
\figlabel{avail-transfers}
\figlabel{avail-running}
\end{figure*}
% Old captions' text:
% The dataflow fact for the available-reload analysis describes
%   the set of registers for which a reload is available.
%   We list the types of the functions that manipuate sets of available registers,
%   as well as the definition of the lattice.
% The standard gen and kill functions for available expressions
% The transfer functions for the available-reloads analysis.
% Running the available-reloads analysis and extracting the results with \texttt{zdfFpFacts}
% The rewrite functions to insert redundant reloads immediately before uses

% Probably no space for the implementations:
% interAvail (UniverseMinus s) (UniverseMinus s') =
%   UniverseMinus (s `plusVarSet`  s')
% interAvail (AvailVars     s) (AvailVars     s') =
%   AvailVars (s `timesVarSet` s')
% interAvail (AvailVars     s) (UniverseMinus s') =
%   AvailVars (s  `minusVarSet` s')
% interAvail (UniverseMinus s) (AvailVars     s') =
%   AvailVars (s' `minusVarSet` s )
% 
% smallerAvail (AvailVars _) (UniverseMinus _) = True
% smallerAvail (UniverseMinus _) (AvailVars _) = False
% smallerAvail (AvailVars     s) (AvailVars    s')  =
%   sizeVarSet s < sizeVarSet s'
% smallerAvail (UniverseMinus s) (UniverseMinus s') =
%   sizeVarSet s > sizeVarSet s'
% 
% extendAvail (UniverseMinus s) r =
%   UniverseMinus (deleteFromVarSet s r)
% extendAvail (AvailVars     s) r =
%   AvailVars (extendVarSet s r)
% 
% delFromAvail (UniverseMinus s) r =
%   UniverseMinus (extendVarSet s r)
% delFromAvail (AvailVars     s) r =
%   AvailVars (deleteFromVarSet s r)
% 
% elemAvail (UniverseMinus s) r =
%   not $ elemVarSet r s
% elemAvail (AvailVars     s) r =
%   elemVarSet r s


We claim that our library
makes it easy to write compiler passes based on dataflow.
In~this section we provide evidence for that claim by showing
implementations of two analyses;
related transformations appear in \secref{example-rewrites}. 
The example analyses help solve a real problem in the Glasgow Haskell
Compiler:
because most calls are tail calls, GHC uses no 
callee-saves registers.
Therefore, at each (rare) non-tail call, all live
variables must be spilled to the stack.
In order to reduce register pressure,
such variables are spilled as early as possible and reloaded as late as possible.

To illustrate the results of the example analyses and transformations,
here is a contrived example program in the style of \secref{example:xforms}:
\begin{alltt}
f (bits32 a) \lbr
  bits32 w, x, y, z;  // local variables
  x = a * a;
  w = a + a + a;
  y = g(w);           // call; x must be spilled
  z = y + y;
  if (y > 0) \lbr
    return z;
  \rbr else \lbr
    return z + x;
  \rbr
\rbr
\end{alltt}
A~spill and a reload should be inserted as follows:%
\seclabel{spill-reload-example} % space is deliberate
\newcommand\bigstrut{%
  \leavevmode\vrule width 0pt height 11pt depth 6pt }
\begin{alltt}
f (bits32 a) \lbr
  bits32 w, x, y, z;  
  x = a * a;
  \high{SPILL x;}
  w = a + a + a;    // no register pressure from x
  y = g(w);
  z = y + y;        // no register pressure from x
  if (y > 0) \lbr
    return z;       // x does not need reloading
  \rbr else \lbr
    \high{RELOAD x;}
    return z + x;
  \rbr
\rbr
\end{alltt}
Although the @SPILL@ and @RELOAD@ operations are introduced because of
the call to @g(a)@, they are moved as far from the call as possible:
the variable~@x@ is spilled immediately after being assigned @a * a@,
and @x@ is reloaded not
immediately after the call to~@g@, but just before its use in the
expression @z + x@.
 On the control-flow path to @return z@, @x@~needn't be reloaded
at all.

\ifpagetuning\enlargethispage{0.5\baselineskip}\fi 
  % make adjacent columns match



Spills and reloads are inserted in the right places
by a sequence of three dataflow passes:
\begin{enumerate}
\item
\label{insert-spills}
A backward analysis computes liveness
to identify the variables that should be spilled at call sites.
An accompanying transformation inserts reloads immediately after each call
site and inserts spills not immediately before call sites, but
rather immediately after the appropriate reaching definitions.
\item
\label{reload-duplication}
A forward analysis finds uses of variables that have been reloaded
from the stack, and an accompanying transformation
inserts redundant reloads before the uses.
By keeping variables on the stack longer, this pass reduces register pressure.
% \simon{Why is the second pass a second pass?  The first
% pass added spills and reloads in; could the first pass not have 
% added the reloads immediately before the
% reloaded variables are used as well?  Maybe the text can say?
% I think the answer is that this is a \emph{forwards} analysis, since
% it is propagating forward the information about which variables currently
% have an up-to-date stack slot.}
\item
\label{remove-dead-reloads}
A backward analysis computes liveness,
and an accompanying transformation, dead-assignment elimination,
removes redundant reloads.
\end{enumerate}
Pass~\ref{insert-spills} is not shown in this paper.
Passes
\ref{reload-duplication}~and~\ref{remove-dead-reloads} cooperate to ``sink''
the reloads as far as possible from the call site.
The analyses used in
passes~\ref{reload-duplication}~and~\ref{remove-dead-reloads}
are described in \secreftwo{avail}{liveness};
the transformations are described in
\secreftwo{sink-reloads}{dead-code-elimination}.
%\simon{I would put the forward references
%to the sub-sections in the bullets themselves; they are more easy to
%find there.
%  No room, and not worthy of a bullet! ---NR}



\subsection{Available variables: a forward analysis} 

\seclabel{avail}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  desperately trying to get figures to emerge in a decent order
%
\begin{figure*}
\begin{codetable}
\T\begin{code}
!Live!type Live = VarSet
\end{code}%
\B
& Dataflow fact\\
\hline

\T\begin{code}
!liveLattice!liveLattice :: DataflowLattice Live
liveLattice = DataflowLattice "live LocalReg's" emptyVarSet add
  where add new old =
          let join = unionVarSets new old in
!liveLattice.end!          (if sizeVarSet join > sizeVarSet old then aTx else noTx) join
\end{code}%
\B
& Lattice\\
\hline

%%  \T\begin{code}
%%  gen  :: UserOfLocalVars    a => a -> Live -> Live
%%  kill :: DefinerOfLocalVars a => a -> Live -> Live 
%%  gen  a live = foldVarsUsed extendVarSet  live a
%%  kill a live = foldVarsDefd delFromVarSet live a
%%  \end{code}\B
%%  & Gen/Kill \mbox{functions}\\
%%  \hline

\T\begin{code}
liveTransfers :: BackwardTransfers Middle Last Live
liveTransfers = BackwardTransfers (flip const) middleLiveness lastLiveness

middleLiveness :: Middle -> Live -> Live
lastLiveness   :: Last -> (BlockId -> Live) -> Live
!middleLiveness!middleLiveness m = addUsed m . remDefd m
!lastLiveness!lastLiveness   l = addUsed l . remDefd l . lastLiveOut l 

!liveness.addUsed.sig!addUsed :: UserOfLocalVars    a => a -> Live -> Live
remDefd :: DefinerOfLocalVars a => a -> Live -> Live 
addUsed a live = foldVarsUsed extendVarSet  live a
!liveness.remDefd.def!remDefd a live = foldVarsDefd delFromVarSet live a

lastLiveOut :: Last -> (BlockId -> Live) -> Live
lastLiveOut l env = last l 
  where
!live.lastBranch!    last (LastBranch id)        = env id 
!live.condBranch!    last (LastCondBranch _ t f) = unionVarSets (env t) (env f)
!live.lastSwitch!    last (LastSwitch _ tbl)     = unionManyVarSets $ map env (catMaybes tbl)
!live.lastCall!    last (LastCall { })         = emptyVarSet
\end{code}%
\B
& Transfer \mbox{functions}\\
\hline

\T\begin{code}
cmmLiveness :: Graph Middle Last -> BlockEnv Live
cmmLiveness g = zdfFpFacts soln
!live.zdfSolveBwd!   where soln = zdfSolveBwd "liveness" liveLattice liveTransfers emptyVarSet g
\end{code}%
\B
& Liveness \mbox{analysis}\\
\end{codetable}
\caption{Liveness analysis}
\figlabel{liveness-all}
\figlabel{liveness}
\figlabel{live-lattice}
\figlabel{live-transfers}
\figlabel{live-running}
\end{figure*}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



To~understand the analysis, you must know that for each variable~$x$,
there is a stack slot~\slotof x that is used to save the value of~$x$.
%
If the variable and the stack slot hold the same value,
that is if $x = \slotof x$,
then it is safe to insert a reload.


\ifpagetuning\enlargethispage{0.5\baselineskip}\fi

To sink a reload of a variable $x$, we insert redundant reloads immediately
before uses of~$x$.
%We use an analysis that identifies not only when it is safe to insert
%such a reload, but when the inserted reload will make an earlier reload redundant.
%Specifically, 
It is \emph{profitable} to insert a reload before a use of~$x$ only if, 
on every path to the use, the most recent definition of $x$~is a reload from
$\slotof x$.
The dataflow fact computed by our analysis is the set of variables for
which it is both 
safe and profitable to insert a reload.
The lattice-join operation is set intersection,\footnote
{Because the assertion of interest is an ``all-paths'' property.}
and the bottom element
is the universal set containing all variables.
It~is optimistic to assume that any variable can be safely reloaded from the stack,
but this optimistic assumption is what enables us to improve the
code---and if the assumption is not justified, the transfer functions
will correct it before
the analysis reaches a fixed point.

\ifpagetuning\enlargethispage{\baselineskip}\fi

Because universal sets can be awkward to manipulate, we represent a
set either as
$\mathtt{UniverseMinus}\;s$, which stands for all variables except
those in the set~$s$,
or $\mathtt{AvailVars}\;s$, which stands for the variables in the set~$s$.
(\figref{avail-lattice}, \linepairref{UniverseMinus}{AvailVars}).
%
%
The bottom element is @UniverseMinus emptyVarSet@.
To manipulate sets of variables, we provide the functions declared in
\linerangeref{extendAvail}{smallerAvail} of \figref{avail}.



%%  Among many other uses, an available-expressions analysis can be~used
%%  in code-motion optimizations.
%%  For example, when making a function call, we insert
%%  spills and reloads to save and restore the values of local variables
%%  around the call site.
%%  It is easy to insert the reloads at the function-call return site,
%%  but to avoid register pressure, it would be better to leave the variable
%%  where it was spilled on the stack.
%%  Rather than complicating the code that inserts the spills and
%%  reloads around call sites,
%%  we write an analysis to insert a redundant reload immediately
%%  before a reloaded variable is used.
%%  Then, we rely on a dead-code elimination to~remove
%%  the early reload.


%%In~the lattice of available reloads (\figref{avail-lattice}),
%%the~join is computed by taking the intersection of two sets
%%of available reloads,
%%The join operation (@add@) returns a \emph{transaction} @aTx@
%%if the join has not yet reached a fixed point;
%%otherwise, it returns @noTx@ to indicate that the dataflow fact
%%did not change.
%%The bottom element of the lattice is the universe of all variables,
%%reflecting the initial assumption that all variables
%%have available reloads.
%%\john{Shouldn't it be @AvailRels@ instead of @AvailVars@?}




\ifpagetuning\enlargethispage{0.5\baselineskip}\fi

The most interesting part of the pass is the @middleAvail@ transfer
function in \figref{avail-transfers}.\finalremark
{Let us revise the paper to pretend that global variables
don't exist.}
This transfer function is specialized to the type of @Middle@ node
used in~GHC.
In~GHC, a~middle node is either @MidAssign@, which assigns to a local
or global variable, or @MidStore@, which assigns to memory.
\par
\ifpagetuning{\enlargethispage{\baselineskip}}\fi
%\kern-8pt
\begin{itemize}
\item
Lines \ref{line:reload1} and \ref{line:reload2}
identify an assignment that reloads local
variable~@x@ from its stack slot.\finalremark{I propose the compiler be
modified to use @isStackSlotOf@ as I've written. JD~approves.}
After such an assignment, $x = \slotof x$,
and the last definition of $x$ is a reload,
so @x@~is added to the set of available variables.\remark{Check these bullets against the figure for correctness}
% No other variable is affected.
\item
On \linepairref{assign.avail.1}{assign.avail.2},
an assignment to a local variable means that the
variable is no longer necessarily equal to the value in its stack
slot, so if @lhs@ is a local variable, it is removed from the set of
available variables.
Function @foldVarsDefd@ is a function which, in this
instance, calls @delFromAvail@ if @lhs@ is a local variable and does
nothing if @lhs@ is a global variable.
%% \remark{To Simon and John: I've removed ``overloaded but am a bit nervous
%% because @foldVarsDef@ shows up as overloaded later in the paper, and I
%% had thought to signpost it here.  Your thoughts?}
\item 
There are three cases for @MidStore@ nodes.
\Linepairref{store.avail.spill.1}{store.avail.spill.2}
match a node that spills a variable~$\mathtt{x}$ to the stack.
After such a node, $\mathtt{x} = \slotOf {\mathtt{x}}$,
but the node is not a reload instruction,
so @x@~is not added to the set of available variables.
%
\Linepairref{store.avail.otherslot.1}{store.avail.otherslot.2}
match a node that writes any \emph{other} value to a stack slot,
after which the variable associated with that slot is no longer available.
%
\Lineref{store.avail.other} matches a store to a location that is not
a stack slot, which leaves the set of available variables unchanged.




% \footnote
% {Although @MidStore@ may overwrite a stack slot \slotof x, GHC
% carefully arranges that all stores to \slotof x have the form
% $\slotof{\mathtt{x}}= @x@$.
% These stores could be used to extend the set of available variables,
% but it is not useful to do so.}
% \simon{Why not? asks the reader.  Perhaps
% because such saves immediately precede calls?}
% \remark{How do we know that @MidStore@ doesn't
% destroy a stack slot??  I've put in a footnote but it will probably be
% simpler to fix the code.}
% \simon{Ah, this harks backe to the start of this sub-section, where
% we say ``to understand the analysis, you must know that...''.  Another
% thing you must know is that $s_x$ is used exclusively for $x$.  That's
% all, I think.}
\end{itemize}
%% \john{I've expunged MidComment from our example} % well done ----NR


The transfer function for a last node checks to see if the node is a
function call (\lineref{avail.LastCall}); if so, the set of
available variables at the call's continuation is empty.
Other last nodes do not change values of variables or stack slots, 
so the set of available variables remains unchanged.
%
A~first node has no effect on program state, so its transfer function
is @flip const@ (\lineref{avail.first}).

%%Using the @agen@ and @akill@ functions, we define the transfer functions
%%for the available-reloads analysis (see~\figref{avail-transfers}):
%%\begin{itemize}
%%\item \emph{First nodes}:
%%  A first node cannot define a variable,
%%  so the set of available reloads is the same before and after a first node.
%%  We return the set unchanged using @flip const@.
%%\item \emph{Middle nodes}:
%%  If a middle node reloads a value from a register's slot on the stack (@RegSlot@),
%%  then we add the register the register to the set of available reloads.
%%  For any other assignment to a register, we remove the register from the
%%  set of available reloads.
%%  Similarly, because a function call can overwrite the value of any local variable,
%%  the set of available reloads is empty after a~function call.
%%  Any other middle node leaves the set of available reloads unchanged.
%%\item \emph{Last nodes}:
%%  If the last node is a function call, the outgoing set of available reloads
%%  is empty for every successor basic block.
%%  Otherwise, the last node cannot modify any variables,
%%  so the set of available reloads remains unchanged.
%%\end{itemize}

Given the lattice and the transfer functions,
we perform the available-variables analysis by calling
the dataflow-engine function @zdfSolveFwd@ (\figref{avail},
\linerangeref{avail.solve.1}{avail.solve.2}). 
%The function @zdfFpFacts@ returns 
%a finite map from basic-block IDs to the set of available variables
%at the beginning of each block.
Except for the implementations of the set operations on
\linerangeref{extendAvail}{smallerAvail}, 
\figref{avail} shows the \emph{entire} analysis.

\subsection{Liveness: a backward analysis} 

\seclabel{liveness}

The assertion computed by 
a backward dataflow analysis applies to a
\emph{continuation} at a program point, not to a state.
The classic example is liveness analysis;
the logical fact of interest is that at a particular program point,
the answer produced by the continuation does not depend on
the value of a particular variable~$x$.
If~so, $x$~is said to be \emph{dead} at that point.
If the answer produced by the continuation \emph{might} depend on the
value of~$x$, $x$~is \emph{live}.\footnote
{Liveness cannot be decided accurately; it reduces to the halting problem.
As usual, we approximate liveness by reachability.}

In a modern compiler, liveness analysis supports many program
transformations,
including
dead-assignment elimination,
which removes assigments to dead variables, 
and register allocation, which
ensures that if two variables are 
live at the same time, they are not assigned to the same register. 

The dataflow fact we use to represent liveness assertions is the set of
live variables (\figref{live-lattice}, \lineref{Live}).
The bottom element of the lattice is the empty set, and the join
operation is set union (\figref{live-lattice},
\linerangeref{liveLattice}{liveLattice.end}); 
a~variable is deemed live after a node if it is live on \emph{any} edge leaving that
node.

The transfer functions for liveness rely on two auxiliary functions
@addUsed@ and @remDefd@ (\figref{liveness}, 
\linerangeref{liveness.addUsed.sig}{liveness.remDefd.def}).
A~transfer function is given a set of variables live on the edges
going out of the node, and it removes from that set any variable
defined by the node, then adds to the set any variable used by the
node.
The~idea is that if a node has the assignment
\begin{verbatim}
  i = n - 1;
\end{verbatim}
then @i@ is not live just before the node, since if we start the
program just before the assignment @i = 0@, the answer cannot 
depend on the value of~@i@, which is about to be overwritten.
However, the answer might well depend on the value of~@n@, so
@n@~is considered live before the assignment.
If a variable appears on both left- and right-hand sides of an
assignment, as in
\begin{verbatim}
  i = i + 1;
\end{verbatim}
then the answer might well depend on~@i@, so @i@~is considered live
before the assignment.
For this reason the transfer functions in \figref{liveness}
(\linepairref{middleLiveness}{lastLiveness}) 
remove defined variables \emph{before} adding used variables.

For a last node, function @lastLiveOut@ consults the solution in
progress to find out what variables are live at the \emph{successors} of a
last node. 
For an unconditional branch, we look up the live set at the label
branched to (\lineref{live.lastBranch});
for a conditional branch, we look at both true and false edges
(\lineref{live.condBranch}), 
 and
for a switch, we consider every possible target of the
branch (\lineref{live.lastSwitch}).
The~remaining case (\lineref{live.lastCall}) is a call, 
and since a call destroys the values of all local variables, no
local variables are live at its continuation.

Given the lattice and the transfer functions,
we perform the liveness analysis by calling
the dataflow-engine function @zdfSolveBwd@ (\figref{liveness},
\lineref{live.zdfSolveBwd}). 
%The function @zdfFpFacts@ returns 
%a finite map from basic-block IDs to the set of live variables
%at the beginning of each block.
\figref{avail} shows the \emph{entire} analysis.
(The overloaded functions
@foldVarsUsed@ and @foldVarsDefd@
are used throughout the back end and
should not be considered part of liveness analysis.)
%
% types of fold functions agreed to in principle, but in practice
% there is no good place to put them.



\ifgenkill
\section{Writing transfer functions using {\mdseries\texttt{gen}} and
{\mdseries\texttt{kill}}}

%\remark{I've tried to rewrite this section without the abstract guff;
%Simon, can you let us know what you think?}


\seclabel{gen-kill}

If you pick up a compiler textbook, you might see dataflow
optimization explained in terms of functions called \texttt{gen} and
\texttt{kill}, 
which say what dataflow facts are ``generated'' and
``killed'' by each node.
Our design is compatible with explanations based on \texttt{gen} and
\texttt{kill};
first you define functions \texttt{gen} and \texttt{kill}, then you
use them to write the transfer functions.
%%  Using our design, you can stillIt's~possible to write transfer functions i thi
%%  In~a forward analysis, for example an assignment might establish an
%%  assertion (\texttt{gen}), or it might change state in such a way that an
%%  assertion no longer holds (@kill@), or both.
%%  If~you want to transliterate specifications that use @gen@ and @kill@,
%%  it's easy.
For example, the functions @addUsed@ and @remDefd@ in \figref{liveness}'s
liveness analysis correspond directly to the traditional @gen@ and
@kill@ functions.
In the available-variables analysis of \figref{avail}, @gen@ and
@kill@ can be defined as follows:
\begin{smallcode}
gen  :: UserOfLocalVars    a => a -> AvailVars -> AvailVars
kill :: DefinerOfLocalVars a => a -> AvailVars -> AvailVars
gen  a avail = foldVarsUsed extendAvail  avail a
kill a avail = foldVarsDefd delFromAvail avail a
\end{smallcode}
Using @gen@ and @kill@ entails no loss of efficiency;
for example, on \lineref{reload2} of \figref{avail}, GHC~would identify and
inline the type-specific instance of @foldVarsUsed@, which for the
case of a local variable is the identity function, so @gen x avail@
would reduce to @extendAvail avail x@.

\fi

\section{Using dataflow facts to rewrite graphs\ifpagetuning\else, with examples\fi}

\seclabel{rewrites}

\seclabel{example-rewrites}

\begin{figure}
\begin{code}
type Rewrite mid last = Maybe (Graph mid last)
data ForwardRewrites mid last a = ForwardRewrites
 {fr_first  :: BlockId -> a -> Rewrite mid last,
  fr_middle :: mid     -> a -> Rewrite mid last,
  fr_last   :: last    -> a -> Rewrite mid last} 

data BackwardRewrites mid last a = BackwardRewrites
 {br_first  :: BlockId  -> a  -> Rewrite mid last,
  br_middle :: mid      -> a  -> Rewrite mid last,
  br_last   :: last ->
               (BlockId -> a) -> Rewrite mid last} 
\end{code}
\caption{Types of forward and backward rewrite functions.}
\figlabel{rewrites}
\end{figure}


\begin{figure*}
\begin{codetable}
\T\begin{code}
availRewrites :: ForwardRewrites Middle Last AvailVars
availRewrites = ForwardRewrites first middle last
!avail.rewrites.first!  where first _ _ = Nothing
        middle m avail = maybe_reload_before avail m (mkMiddle m)
!avail.rewrites.last!        last   l avail = maybe_reload_before avail l (mkLast l)
!maybe.reload.before.1!        maybe_reload_before avail node tail =
            let used = filterVarsUsed (elemAvail avail) node
            in  if isEmptyVarSet used then Nothing
!maybe.reload.before.2!                else Just $ reloadTail used tail
        reloadTail vars t = foldl rel t $ varSetToList vars
          where rel t r = mkMiddle (reload r) <*> t
\end{code}%
\B
& Rewrite \mbox{functions}\\
\hline

\T\begin{code}
insertLateReloads :: (Graph Middle Last) -> DFMonad (Graph Middle Last)
insertLateReloads g = liftM zdfFpContents result
!insertLateReloads.1!  where result = zdfRewriteFwd RewriteShallow "insert late reloads"
                               availVarsLattice avail_vars_transfer
!insertLateReloads.2!                               availRewrites (fact_bot availVarsLattice) g
\end{code}%
& Insert late reloads\\
\end{codetable}
\caption{Late-reload insertion, which relies on the analysis of \figref{avail}}
\figlabel{avail-rewrites}
\end{figure*}


We compute dataflow facts in order to enable code-improving
transformations.
The heart of a transformation is a triple of
\emph{rewrite functions};
type declarations for rewrite functions are
shown in \figref{rewrites}. 
%
A~rewrite function is given a dataflow fact and a node.
It~may choose to replace the node with a new graph~$g$, in which case it
returns $@Just@\;g$, or it may do nothing, in which case it returns @Nothing@.
The assertions represented by incoming dataflow facts must be
sufficient to ensure that the
rewrite preserves semantics.
%%  
%%  
%%  If~a rewrite function returns $@Just@\;g$, the incoming dataflow fact
%%  must guarantee that replacing the node with~$g$ does not change the
%%  observable behavior of the program.
%%  

To write a program transformation,
the compiler writer must 
\begin{itemize}
\item
Create a dataflow lattice and transfer functions for the supporting
analysis, as described in \secref{create-analysis}. 
\item
Create rewrite functions for first, middle, and last nodes.
\end{itemize}
The
compiler writer can then use the 
dataflow-engine function @zdfRewriteFwd@ to transform a control-flow
graph:
\begin{code}
  zdfRewriteFwd 
    :: RewritingDepth         -- Rewrite recursively?
    -> PassName               -- Name of this pass
    -> DataflowLattice a      -- Lattice
    -> ForwardTransfers m l a -- Transfer functions
    -> ForwardRewrites m l a  -- Rewrite functions
    -> a                      -- Input fact
    -> Graph m l              -- Control-flow graph
    -> DFMonad (ForwardFixedPoint m l a (Graph m l))
\end{code}
%%  
%%  class DataflowSolverDirectiontransfers fixedpt =>
%%        DataflowDirection
%%          transfers fixedpt rewrites where
%%    zdfRewriteFwd :: (DebugNodes m l, Outputable a)
%%      => RewritingDepth    -- Recursive rewrites?
%%      -> BlockEnv a        -- Init facts
%%      -> PassName          -- Analysis name
%%      -> DataflowLattice a -- Lattice
%%      -> transfers m l a   -- Transfers
%%      -> rewrites m l a    -- Input fact
%%      -> a                 -- Input fact
%%      -> Graph m l         -- CFG
%%      -> DFMonad (fixedpt m l a (Graph m l))
%%
%%
Function @zdfRewriteFwd@ is like @zdfSolveFwd@ in
\secref{zdfSolveFwd}, but it uses and produces extra
information.\seclabel{engine-truth} 
\begin{itemize}
\item
The @RewritingDepth@ parameter controls recursive rewriting;
if~a graph produced by a rewrite function should not be further rewritten,
rewriting is \emph{shallow};
if~a graph produced by a rewrite function can be rewritten again,
rewriting is \emph{deep}.
Deep rewriting is essential to achieve the results of
\citet{lerner-grove-chambers:2002}, e.g., to remove the induction
variable from the loop in the example in \secref{induction-var-elim}.
When deep rewriting is used, the rewrite functions are obligated to
ensure that the graphs they produce cannot be written indefinitely.
\item
Function @zdfRewriteFwd@ requires rewrite functions as well as transfer
functions.
\item
The type constructor @ForwardFixedPoint@ has a fourth
type parameter,\footnote
{We lied to you earlier.}
which is a value contained in the fixed point.
The~value can be extracted using function @zdfFpContents@, which has
type @ForwardFixedPoint m l a b -> b@.
Here the type parameter~@b@ is instantiated to @Graph m l@: the fixed point
contains the rewritten graph.
In~@zdfSolveFwd@, @b@~is instantiated with
the unit type~@()@.
%%% \simon{I'm afraid I do not understand this bullet at all.}
\item
The computation is monadic in the \emph{dataflow monad}, written
@DFMonad@.
This monad keeps track of the current dataflow fact at each label,
provides a supply of fresh labels, 
and
also helps implement the fault-isolation strategy described
in \secref{vpoiso} below.
Because @zdfRewriteFwd@ and @zdfSolveFwd@ share much of their
implementation, the truth 
about @zdfSolveFwd@ is that it also returns a value of monadic type.
\end{itemize}


\seclabel{dfengine-spec}

Function
@zdfRewriteFwd@ implements
interleaved analysis and transformation
in two phases \citep{lerner-grove-chambers:2002}:\seclabel{solver-phase}
\begin{itemize}
\item
In the first phase, when a rewrite function proposes to replace a
node, the replacement graph is analyzed recursively, and the results
of that analysis are used as the new dataflow
fact(s) flowing out of the original node.
Then the replacement
graph is \emph{thrown away}; only the facts remain.
If,~during iteration, the original node is analyzed again, perhaps
with a different input fact, the rewrite function may propose
a different replacement or even no replacement at all.

The first phase is called the \emph{solver}.
It computes a fixed point of the dataflow analysis
\emph{as if} nodes were replaced, while never actually replacing a node.
%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixpoint. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }
\item
Once the solver is complete, the resulting fixed point is sound,
and the facts in the fixed point are used by the second phase, in~which
no dataflow facts change, but
each replacement proposed by a rewrite function is actually
performed.
This phase is called the \emph{rewriter}.
\end{itemize}

%The solver executes a complicated algorithm with complicated control-flow
%that depends on the lattice, transfer functions, and rewrite functions;
Facts computed by the solver depend on graphs produced by rewrite
functions, which in turn depend on facts computed by the solver.
How~do we know this algorithm is sound, or even if it terminates?
A~proof requires a POPL paper
\cite{lerner-grove-chambers:2002}, but we can give some
intuition:
\begin{itemize} 
\item
The algorithm is sound because, given the incoming dataflow facts,
each rewrite must preserve the observable semantics of the program.
Therefore, a sound analysis of the rewritten graph
can only generate dataflow facts that could have been
generated by a more complicated analysis of the original graph.
\item
No matter what the transfer functions and rewrite functions do,
the dataflow engine uses the dataflow lattice's join operation to ensure that
facts at labels always increase. 
As~long as the lattice permits no fact to increase infinitely many
times, analysis is guaranteed to terminate.
\end{itemize}
Thus to guarantee soundness and termination, client code must supply a
lattice with no infinite ascending chains, sound transfer functions,
and sound rewrite functions.
Moreover, either the rewrite functions do not return replacement graphs that contain
nodes which are rewritten indefinitely,
or the client code specifies shallow rewriting, which ensures that the
dataflow engine never rewrites a replacement graph.

Why bother with such a complex algorithm?
\citet{lerner-grove-chambers:2002} write
\begin{quote}
\emph{Previous efforts to exploit [the mutually beneficial
interactions of dataflow analyses] either (1)~iteratively performed
each individual analysis until no further improvements are discovered
or (2)~developed [handwritten] ``super-analyses'' that manually\remark
{We could condense the quote}
combine conceptually separate analyses. We have devised a new approach
that allows analyses to be defined independently while still enabling
them to be combined automatically and profitably. Our approach avoids
the loss of precision associated with iterating individual analyses
and the implementation difficulties of manually writing a
super-analysis.}
\end{quote}



%%  \simon{But do we apply rewrites even before the analysis reaches a fixed point?
%%  If so, what property do the rewrites have to satisfy to ensure soundness?
%%  If not, even a single rewrite might destroy the fixed-point property of the
%%  current facts.  Or perhaps we iterate the analysis to a fixpoint, and only \emph{then}
%%  do rewriting? If so, do we need the transfer functions at that stage?
%%  
%%  Also the fixed-point of the analysis relies on upward chains. What if
%%  the rewrite pushed it downward?  Or is it the case that a rewrite must
%%  change a node $n$ into a graph $g$ so 
%%  that $\mathit{fwdtrans}(n) \leq \mathit{fwdtrans}(g)$?
%%  
%%  Also the fixpoint calculation requires multiple passses; do the 
%%  rewrites then apply multiple times?
%%  
%%  I'm deliberately playing the role of the reader here, and not peeking at
%%  the code.  I don't think it's enough to say ``go look at Chambers paper''; 
%%  I suggest we say enough (half a column would do it) to address the obvious
%%  questions and point to Chambers for details.
%%  
%%  
%%  \textbf{NR}: Good questions, but let's have a forward reference to \secref{dfengine}}


In~practice, interleaving analysis with transformation makes it
possible to create startlingly simple transformations.
In the rest of this section we present two examples:
\secref{sink-reloads} shows how to insert a reload instruction just
before each use of each spilled variable, and
\secref{dead-code-elim} shows how to eliminate dead assignments.
When these two transformations are run in sequence, the effect is to
sink reloads and produce programs like the example shown in
\secref{spill-reload-example}. 





%%  After defining the lattice, the transfer functions, and the rewrite functions,
%%  the client runs the analysis by invoking the dataflow framework
%%  (see~\figref{framework-fns}).
%%  The function @zdfSolveFrom@ performs an analysis on an input control-flow graph,
%%  using a dataflow lattice and a set of transfer functions.
%%  The additional arguments to the function provide
%%  the name of the analysis,
%%  the initial set of dataflow facts (usually empty),
%%  and the initial fact (usually bottom)
%%  that flows into either the entry or exit of the graph,
%%  depending on whether the transfers define a forward or backward analysis.
%%  The result of the function is the fixed point of the analysis,
%%  which stores the dataflow fact on entry to each basic block.
%%  \john{Maybe we should export a simple version of these functions to clients?
%%    Do they always do the obvious things with the initial facts and in-fact?
%%    Initial facts can reuse results of a previous analysis, but then you lose
%%    interleaving.}
%%  
%%  To combine an analysis and a transformation,
%%  the client calls the @zdfRewriteFrom@ function,
%%  which takes the same arguments as @zdfSolveFrom@,
%%  with the addition of the set of rewrite functions
%%  and a parameter (@RewritingDepth@) that decides whether the result
%%  of a rewrite function should be considered for further rewriting.
%%  The result of the function is not only the fixed point of the
%%  analysis interleaved with the transformation,
%%  but also the transformed control-flow graph.
%%  



\begin{figure*}
\begin{codetable}
\T\begin{code}
deadRewrites = BackwardRewrites nothing middleRemoveDeads nothing
!deadRewrites.1!  where nothing _ _ = Nothing
        middleRemoveDeads :: Middle -> CmmLive -> Maybe (Graph Middle Last)
!elim.dead.1!        middleRemoveDeads (MidAssign (CmmLocal x) _) live
!elim.dead.2!            | not (x `elemVarSet` live) = Just emptyGraph
!deadRewrites.2!        middleRemoveDeads _ _ = Nothing
\end{code}%
\B
& Rewrite \mbox{functions}\\
\hline

\T\begin{code}
removeDeadAssignments :: (Graph Middle Last) -> DFMonad (Graph Middle Last)
removeDeadAssignments g = liftM zdfFpContents result
!rewriteBwd.1!     where result = zdfRewriteBwd RewriteDeep "dead-assignment elim"
!rewriteBwd.2!                                  liveLattice liveTransfers rewrites emptyVarSet g
\end{code}%
& \mbox{Dead-code} elimination\\
\end{codetable}
\caption{Dead-assignment elimination, which relies on the analysis of
\figref{liveness}} 
\figlabel{dead-elim}
\end{figure*}


\subsection{Sinking reloads: a forward transformation}

\finalremark{Incidentally, I wonder if we should
use record notation when constructing @ForwardRewrites@?}

\seclabel{sink-reloads}

We can use the available-variables analysis of \secref{avail} to
insert reloads
immediately before uses of variables.
The transformation is implemented by the rewrite functions on
\linerangeref{avail.rewrites.first}{avail.rewrites.last} of \figref{avail-rewrites}.
A~first node uses no variables and so is never rewritten.
For middle and last nodes, @maybe_reload_before@ 
(\linerangeref{maybe.reload.before.1}{maybe.reload.before.2})
computes @used@, which is the set
of variables used in the node which are both safe and profitable to
reload. 
If that set is not empty, function
@reloadTail@ replaces @node@ with a new graph in which @node@ is
preceded by a (redundant) reload for each variable in the set~@used@.
%%%%% This rewrite function \emph{must} be used with shallow
%%%%% rewriting. % redundant

\figref{avail} uses these functions to create graphs:
\begin{code}
mkLabel  :: BlockId -> Graph m l
mkMiddle :: m       -> Graph m l
mkLast   :: l       -> Graph m l
(<*>)    :: Graph m l -> Graph m l -> Graph m l
\end{code}
%% \simon{Aha!  Here is where we lie by pretending that @Graph@ = @AGraph@.  Ok.}
%% \simon{Does @mkLast@ really have a @LastNode l@ context? How does it use it?}
%%        Yes, it really does, but it's used only for prettyprinting
%%        the graph (for debugging).  So I've expunged it from the
%%        paper. ---NR
The infix @<*>@ function is graph concatenation.

The transformation is implemented by the call to @zdfRewriteFwd@
on \linerangeref{insertLateReloads.1}{insertLateReloads.2} of \figref{avail-rewrites}.
Rewriting is shallow, which is to say that a graph returned by
@maybe_reload_before@ is not itself rewritten.
(If~it \emph{were} rewritten, a nonempty @used@ set would make the
compiler insert an infinite sequence of reloads before @node@.)
Once the reloads are inserted, the original reloads immediately
following the call site are dead, and they can be eliminated by our
next transformation, dead-assignment elimination.

\subsection{Dead-assignment elimination: a backward \rlap{transformation}}


\seclabel{dead-code-elimination}
\seclabel{dead-code-elim}

\seclabel{bwd-rewrite}


\def\liveout{$\mathit{live_{out}}$}

We use the liveness analysis of \secref{liveness} to identify
assignments which modify only variables that are not used.
Such \emph{dead assignments} can be removed without changing the
observable behavior of the program.
The removal is implemented by the rewrite functions on
\linerangeref{deadRewrites.1}{deadRewrites.2} of \figref{dead-elim}. 
First and last nodes are not assignments and are therefore never
rewritten.
A~middle node is rewritten to the empty graph if and only if it is an
assignment to a dead variable (\linerangeref{elim.dead.1}{elim.dead.2}).
On \linepairref{rewriteBwd.1}{rewriteBwd.2}, we call @zdfRewriteBwd@, and
that is all there is to it.
\finalremark{JD: Need to run this version of the code in anger.}
%
\remark{In this space we should have some guff about
composing transformations, which should refer to the example on
eliminating the induction variable.
More generally, list some places dead-assignment elim is used and
include \secref{induction-var-elim}.
}




\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
!forward.sol.sig!   forward_sol :: LastNode l => (forall a . Fuel -> Maybe a -> Maybe a) -> RewritingDepth -> PassName 
               -> BlockEnv a -> ForwardTransfers m l a -> ForwardRewrites m l a
               -> a -> Graph m l -> Fuel
               -> DFM a (ForwardFixedPoint m l a (), Fuel)
!forward.sol.args!   forward_sol squash depth name start_facts transfers rewrites = ... setAllFacts start_facts ...
!solve.1!     where solve :: a -> Graph m l -> Fuel -> DFM a (a, Fuel)
           solve in_fact g fuel = do { fuel <- run "forward" name set_successor_facts blocks fuel
                                     ; b <- getExitFact                                          
!solve.*!                                     ; return (b, fuel) }                                        
             where blocks = postorder_dfs g
!set.successor.facts.1!                   set_successor_facts (Block id tail) fuel =
!solver.getFact!                     do { idfact <- getFact id
                        ; (last_outs, fuel) <-
                            ... solve_tail (ft_first_out transfers id idfact) tail fuel ...
!solver.set.or.save!                        ; set_or_save last_outs
!set.successor.facts.*!                        ; return fuel }

!solver.tail.1!           solve_tail in' (ZTail m t) fuel =
!solver.fr.middle!             case squash fuel $ fr_middle rewrites m in' of
!solver.no.rewrite!               Nothing -> solve_tail (ft_middle_out transfers m in') t fuel
!solver.rewrite.1!               Just g -> do { (a, fuel) <- subAnalysis $
                                 case depth of
!solver.rewrite.deep!                                   RewriteDeep -> solve in' g (oneLessFuel fuel)
!solver.rewrite.shallow.1!                                   RewriteShallow -> do { a <- anal_f getExitFact in' g
!solver.rewrite.*!                                                        ; return (a, oneLessFuel fuel) }
!solver.tail.*!                            ; solve_tail a t fuel }
\end{numberedcode}
\caption{Excerpts from the forward solver}
\figlabel{solver-excerpts}
\end{figure*}



\section{The dataflow engine}
\seclabel{engine}
\seclabel{dfengine}

Above, in sections \ref{sec:making-simple}
through~\ref{sec:rewrites},
we use our library to create analyses and transformations.
In this section, we sketch the implementation of the main part of our
library: the dataflow engine.
Despite the implicit claim in the title of this paper,
the dataflow engine is not simple.
The \emph{interface} (lattice, transfer functions, rewrite functions) is simple,
and as shown above, compiler passes written \emph{using} the engine
are simple.
But~the implementation is complex.
The purpose of this section is to show that the complexity is \emph{manageable}.


The dataflow engine is implemented in two layers.
The lower layer comprises the \emph{fuel monad} and the \emph{inner dataflow monad}.
The fuel monad provides a way to suppress
optimization selectively in order to isolate faults, as discussed
below (\secref{vpoiso}). 

The inner dataflow monad has type @DFM a@, where @a@~is the type of a
dataflow fact.
It~is a state monad which keeps track of the values of dataflow facts
as the engine iterates.
Its~operations include % listed as per SLPJ!
\begin{code}
getFact     :: BlockId -> DFM a a
getExitFact :: DFM a a
getAllFacts :: DFM a (BlockEnv a)
set_or_save :: LastOutFacts a -> DFM a ()
subAnalysis :: DFM a b -> DFM a b
\end{code}
The @get@ operations have corresponding @set@ operations and vice
versa.
The~most interesting operation is @subAnalysis@, which
runs an analysis in a fresh copy of the current state, then reverts to
the previous state.
%%  Using pure code and a monad makes this version much easier to get
%%  right than our Objective Caml version
%%  \cite{ramsey-dias:applicative-flow-graph}. 
%%  \john{Presumably this sentence disappears if the italic text is made permanent.}

These monads are private to the implementation of the library;
what is exposed in the interface is a combined monad called @DFMonad@, and a function
@runDFIO@ of type @IORef Fuel -> DFMonad a -> IO a@.
\remark{Unfortunately, this is the first mention of @Fuel@}

The upper layer of the dataflow engine comprises four functions:
a forward solver, a forward rewriter,
a backward solver, and a backward rewriter.
We show excerpts of the forward solver and rewriter in
\secreftwo{forward-solver}{forward-rewriter}. 
More complete descriptions are available as a supplement to this
paper. 


%%  Note that the dataflow engine is the only part of the system that is
%%  hard to get right---this is where all the hair is.
%%  Prime benefit of our system is that once this is right, everything is
%%  easy (and indeed is just logic, strongest postcondition, or weakest
%%  precondition). 
%%  

\subsection{Throttling the dataflow engine using ``optimization
  fuel''}

\seclabel{vpoiso}

We have extended Lerner, Grove, and Chambers's algorithm with
Whalley's \citeyearpar{whalley:isolation} algorithm for isolating
faults.
Whalley's algorithm is used to test a faulty optimizer by automatically
finding the first rewrite that introduces a fault.
It works by giving the optimizer a finite supply of \emph{optimization
fuel}.
Each time a rewrite function proposes to replace a node, one unit of @Fuel@ is
consumed.
When the optimizer runs out of fuel, further rewrites are suppressed.
Because each rewrite leaves the observable behavior of the
program unchanged, it is safe to suppress rewrites at
any point.
In~normal operation, the optimizer has unlimited fuel, but during
debugging, a fault can be isolated quickly by doing a binary search on
the size of the fuel supply.
To control the fuel supply in a purely functional setting, we use
the fuel monad, which works with
computations of type @Fuel -> (a, Fuel)@.


\subsection{The forward solver}

\seclabel{forward-solver}

\simon{What does the reader gain from here on?}

To avoid duplicating code,
\emph{the dataflow engine implements only composed
analysis and transformation}.
Pure analysis is a special case in which no node is
ever rewritten.


\finalremark{How and where do we say what's new over
\citet{ramsey-dias:applicative-flow-graph}?}
%showed a backward solver and rewriter that kept dataflow facts in
%mutable cells.
%\emph{[New stuff is pure, polymorphic, and shows how to use
%    fuel]}\remark{fix me}


\figref{solver-excerpts} shows excerpts from the forward solver 
@forward_sol@, which is used to implement the solver phase of
@zdfRewriteFwd@ and also to implement
@zdfSolveFwd@.
Because @forward_sol@ is used to solve replacement graphs as well as
full graphs, it
takes a few more parameters than @zdfSolveFwd@
(\figref{solver-excerpts}, \linerangeref{forward.sol.sig}{forward.sol.args}).
\begin{itemize}
\item
An analysis of a subgraph starts with known facts @start_facts@, not
bottom facts (\lineref{forward.sol.args}).
\item
In the solver phase described in \secref{solver-phase},
if a node is rewritten, the replacement graph is used to compute new
dataflow facts, and then the replacement graph is thrown away.
But sometimes we want to prevent a node from being rewritten,
either because
we are implementing a pure analysis, or because the optimizer
is out of fuel. 
We~do so using parameter @squash@.
\item
We track the amount of fuel remaining using
parameter @fuel@.
Parameter @fuel@ comes last because the fuel monad works with
functions of type @Fuel -> (a, Fuel)@.
\end{itemize}
%A~fixed point is computed by initializing the facts using
%@setAllFacts@, which is an operation in the dataflow monad.
%
Function @solve@, on \linerangeref{solve.1}{solve.*} of
\figref{solver-excerpts}, 
takes an input fact, a graph, and a fuel supply; it~returns a pair
containing the exit fact and the 
remaining fuel supply.
It~also has a side effect on the state stored in the inner dataflow monad:
it brings the facts associated with labels up to a fixed point.
 
Function @solve@ works by applying a higher-order @run@ function,
not shown here,
% And don't you dare ask for its type.  The type of run would make you
% vomit.  ---NR
which repeatedly runs
@set_successor_facts@
(\linerangeref{set.successor.facts.1}{set.successor.facts.*})
on each block in the graph, until the facts
stored in the inner dataflow monad stop changing.
Each block comes from  the list of reachable blocks in~@blocks@,
which is computed by reverse postorder depth-first traversal of the graph.
(Although any order will result in a correct fixed point,
reverse postorder depth-first order hastens convergence of forward dataflow
analyses---often just a few iterations suffice.
The @LastNode l@ constraint in the type of @forward_sol@
guarantees that we can
  find the successors of last nodes and so do the traversal.)
%(Computation @getExitFact :: DFM a a@ is an operation in the inner dataflow
%monad.)



To understand @set_successor_facts@, you must understand the
representation of basic blocks.
A~basic block is a sequence beginning with a first node, continuing
with zero or more middle nodes, and ending in a last node.
A~first node contains only a unique identifier of type @BlockId@; the
types of middle and last nodes are parameters.
A~sequence of middle nodes followed by a last node is reresented by a
value of type @ZTail m l@, where @m@~and~@l@
are the types of middle and last nodes:
\begin{verbatim}
data Block m l = Block BlockId (ZTail m l)
data ZTail m l = ZTail m (ZTail m l) 
               | ZLast l | ZNoLast
\end{verbatim}
Case @ZNoLast@ arises when a replacement graph has no last node.


\begin{figure*}
%%  forward_rew
%%          :: forall m l a . 
%%             (DebugNodes m l, LastNode l, Outputable a)
%%          => (forall a . Fuel -> Maybe a -> Maybe a)
%%          -> RewritingDepth
%%          -> BlockEnv a
%%          -> PassName
%%          -> ForwardTransfers m l a
%%          -> ForwardRewrites m l a
%%          -> a
%%          -> Graph m l
%%          -> Fuel
%%          -> DFM a (ForwardFixedPoint m l a (Graph m l), Fuel)
%%  forward_rew squash depth xstart_facts name transfers rewrites in_factx gx fuelx = fixed_pt_and_fuel
%%    where
%%      fixed_pt_and_fuel =
%%          do { (a, g, fuel) <- rewrite xstart_facts getExitFact in_factx gx fuelx
%%             ; facts <- getAllFacts
%%             ; let fp = ... facts ... g ...
%%             ; return (fp, fuel)
%%             }
%%  
%%  
%%      rewrite_blocks (Block id t : bs) rewritten fuel =
%%        ... rew_tail h (ft_first_out transfers id a) t rewritten fuel ...
%%      rewrite_blocks [] rewritten fuel = return (rewritten, fuel)
\setcounter{codeline}{0}
\begin{numberedcode}
    rewrite :: BlockEnv a -> DFM a b -> a -> Graph m l -> Fuel -> DFM a (b, Graph m l, Fuel)
    rewrite start in_fact g fuel =
      let Graph eid blockenv = g
          blocks = postorder_dfs_from blockenv entry
      in do { forward_sol squash depth name start transfers rewrites in_fact g fuel
            ; (rewritten, fuel) <- rewrite_blocks blocks emptyBlockEnv fuel
            ; a <- getExitFact
            ; return (a, Graph eid rewritten, fuel)
            }

!rew.tail.1!    rew_tail head in' (ZTail m t) rewritten fuel =
!rewrite.squash!      case squash fuel $ fr_middle rewrites m in' of
!rewrite.append.m!        Nothing -> rew_tail (ZHead head m) (ft_middle_out transfers m in') t rewritten fuel
!rewrite.rewrite.1!        Just g -> do { (a, g, fuel) <- inner_rew in' g fuel
!rewrite.rewrite.*!                     ; let (blocks, h) = splice_head head g
!rew.tail.*!                     ; rew_tail h a t (blocks `plusBlockEnv` rewritten) fuel }
\end{numberedcode}
\caption{Excerpts from the forward rewriter}
\figlabel{rewriter-excerpts}
\end{figure*}

Function @set_successor_facts@ walks a basic block from first node to
last node, using and updating the state maintained by
the inner dataflow monad.
The initial fact is the fact associated with the block's identifier,
extracted using @getFact@ (\lineref{solver.getFact}).
%% , which has type @BlockId -> DFM a a@.
Function @solve_tail@ walks the block and computes @last_outs@, which
gives the new facts to be propagated to the successors of the blocks,
as defined in \figref{transfers}.
Function @set_or_save@ updates the facts in the inner dataflow monad
(\lineref{solver.set.or.save}), 
and finally @set_successor_facts@ returns the remaining fuel
(\lineref{set.successor.facts.*}). 
\john{If we need space, this paragraph is very heavy for its payload.}

The workhorse of the analysis is @solve_tail@, which handles the three
cases of @ZTail@: a middle node, a last node, and @ZNoLast@.
\figref{solver-excerpts} shows only the code for the middle node
(\linerangeref{solver.tail.1}{solver.tail.*}).
The~first step presents the incoming fact~@in'@ and the middle
node~@m@ to the appropriate rewrite function (\lineref{solver.fr.middle}).
If~that function returns @Nothing@, or if the value is squashed to
@Nothing@,\footnote
{@squash fuel a@ returns @Nothing@ if @a@~is @Nothing@, if @fuel@~is
  exhausted, or if we are running @zdfSolveFwd@, which uses no rewrite
  functions.
%In~the last case, lazy evaluation guarantees that @rewrites@ is not
%evaluated. 
}
@solve_tail@
% simply  % lost to a bad line break
calls the transfer function
@ft_middle_out transfers@ and continues with the next node
(\lineref{solver.no.rewrite}). 



The interesting case occurs on \linerangeref{solver.rewrite.1}{solver.tail.*},
after the rewrite function 
@fr_middle rewrites@
proposes a replacement graph~@g@.\finalremark
{Orphaned text:
and
a list of facts that hold on edges leaving the graph, which is  
 extracted using function @zdfFpLastOuts@.
\iffalse %%% WRONG -- it's the @goto@ that does this...
In the example above, when the subgraph
@z = 7 + y@ is analyzed, @zdfFpLastOuts@ will contain
the pair $(@L2@, @x == 7@ \land @y == 8@)$.
\fi
}
\begin{enumerate}
\item
If we are doing \emph{deep} rewriting, @g@~may be further rewritten,
so we call @solve@ recursively after removing one unit of fuel (\lineref{solver.rewrite.deep}).
\item
If we are doing \emph{shallow} rewriting,  new graph~@g@ must not be
rewritten, but we must still find a fixed point of the transfer
equations.
We compute that fixed point using @anal_f@ (not shown), which  recursively calls
@forward_sol@ using the @squash@ function
@\ _ _ -> Nothing@, 
thereby doing no rewriting and consuming no fuel
(\linepairref{solver.rewrite.shallow.1}{solver.rewrite.*}).

\end{enumerate}
Whether rewriting is shallow or deep, the new graph~@g@ is analyzed as
a ``sub-analysis'' in the inner dataflow monad
(\lineref{solver.rewrite.1}), which means it has fresh 
state for tracking facts and deciding whether analysis has reached a
fixed point.
State changes made in a sub-analysis cannot be observed by the outer
analysis. 
In~fact, after the sub-analysis all the new state \emph{and the new
  graph} are thrown away.
As~\linerangeref{solver.rewrite.1}{solver.tail.*} show, the new
graph~@g@ is used only to compute the new 
dataflow fact~@a@ and the diminished
supply of fuel.
On \lineref{solver.tail.*}, @solve_tail@ continues tail-recursively solving~@t@.
The only difference between rewriting
(\linerangeref{solver.rewrite.1}{solver.tail.*}) and not 
rewriting (\lineref{solver.no.rewrite}) is the
\emph{fact} and the \emph{fuel} passed to the tail call.

The full implementations of @solve@ and @solve_tail@, which are
included in a supplement to this paper, contain similar code to
rewrite first nodes and last nodes and to sub-analyze the replacement
graphs. 


%%\begin{itemize}
%%\item
%%\item
%%The internal @solve@ function is higher-order in the parameter
%%@finish@, which extracts from the inner dataflow monad either the unique
%%exit fact or the set of @LastOuts@, depending on context.
%%\item
%%The function @set_or_save@ calls @setFact@ for @BlockId@s located
%%within graph~@g@ and calls @addLastOutFact@ for @BlockId@s located
%%outside graph~@g@.
%%\end{itemize}



\subsection{The forward rewriter}

\seclabel{forward-rewriter}

The core of the rewriter is shown in \figref{rewriter-excerpts}.
At~this point, because the rewriter produces a new graph, we~explain
how a graph is represented: it is a finite map from  @BlockId@
  to basic block, together with the @BlockId@ of the entry block:
\begin{code}
data Graph m l = Graph BlockId (BlockEnv (Block m l))
\end{code}
Function @rewrite@ first calls @forward_sol@
(\figref{solver-excerpts}) to iterate to a fixed point.
At~this point, the facts stored in the inner dataflow monad can be
used for rewriting.
The work is done by @rewrite_blocks@ (not shown), which simply
rewrites each block in turn and returns a finite map containing the
rewritten blocks, plus the remaining fuel supply.

As~above, we show only the code relevant to rewriting middle nodes;
code for first nodes and last nodes is similar.
The @rew_tail@ function (\linerangeref{rew.tail.1}{rew.tail.*}) uses
exactly the same test as in the solver: on \lineref{rewrite.squash},
it scrutinizes
\begin{verbatim}
   squash fuel $ fr_middle rewrites m in'
\end{verbatim}
If the node~@m@ is not rewritten, the action taken is almost the same
as in the solver, but @rew_tail@ takes two 
 additional accumulating parameters.
Parameter @head@ carries
the leading fragment of the current basic block,
from its first node up to the middle node~@m@, which is
a candidate for rewriting;
parameter @rewritten@ carries the blocks that have already been
rewritten.
%
To~store a leading fragment of a basic block, we use type @ZHead m@:
\begin{verbatim} 
data ZHead m = ZFirst BlockId | ZHead (ZHead m) m
\end{verbatim}
We append~@m@ to the
fragment~@head@ by forming @ZHead head m@ 
(\figref{rewriter-excerpts}, \lineref{rewrite.append.m}). 


The interesting case is where the middle node~@m@ is rewritten into
the replacement graph~@g@ (\linerangeref{rewrite.rewrite.1}{rewrite.rewrite.*}).
%%We~first mark the graph as rewritten, so that whatever called this
%% pass may know that the graph has changed.
%% As~before, we convert~@g@ by supplying it with fresh labels.
On \lineref{rewrite.rewrite.1}, @inner_rew@ (not shown) handles the
distinction between deep and shallow rewriting:
when rewriting deeply, it recursively calls @rewrite@,
and 
when rewriting shallowly, it calls @forward_sol@ from
\figref{solver-excerpts}. 
Deep or shallow, it produces the exit fact from the sub-analysis of
the rewritten graph, 
the rewritten graph itself, 
and the remaining fuel supply.
The rewritten graph is
spliced onto the head of the block, producing a new head~@h@ and a set
of fully rewritten blocks~@blocks@ (\lineref{rewrite.rewrite.*}).
The splicing algorithm is described by
\citet{ramsey-dias:applicative-control-flow}. 
On~\lineref{rew.tail.*}, 
the new head~@h@ replaces @head@; @blocks@ is added to the
accumulating parameter @rewritten@; and
@rew_tail@ continues with the next node by making a tail call
to rewrite~@t@.






%%  \subsection{The inner dataflow monad}
%%  
%%  The primary purpose of the dataflow monad is to keep track of 
%%  dataflow facts as the engine iterates.
%%  Dataflow facts are found in three places:
%%  \begin{itemize}
%%  \item
%%  There is a dataflow fact associated with every labelled basic block in
%%  the current graph;
%%  the dataflow monad maintains this association in a finite map.
%%  The functions @getFact@ and @setFact@ query and update this map.
%%  \item
%%  The current graph may be a subgraph of a larger graph, in which case a
%%  forward dataflow pass may produce dataflow facts that flow to labelled
%%  blocks that are outside the current graph.
%%  These facts must be retained and propagated even if the current graph
%%  is abandoned; such facts are added with @addLastOutFact@ and recovered
%%  with @bareLastOuts@.
%%  \item
%%  Finally, a foraward dataflow pass over a subgraph may propagate a fact forward by
%%  ``falling off the end;'' such a fact is set with @setExitFact@ and
%%  recovered with @getExitFact@.
%%  \end{itemize}
%%  In addition to keeping track of facts, 
%%  the dataflow monad provides a number of other facilities to manage
%%  changes in state as graphs are rewritten and facts climb the dataflow
%%  lattice:
%%  \begin{itemize}
%%  \item
%%  The monad keeps track of whether any fact has changed.
%%  \item
%%  It provides a @subanalysis@ function which makes it possible to
%%   analyze a subgraph using the current set of facts, then discard any
%%   changes in state that may have resulted from the analysis of the
%%   subgraph.
%%  \item
%%  It provides a supply of fresh @BlockId@s, which are available for use
%%  by rewrite functions.
%%  \item
%%  It tracks the supply of \emph{optimization fuel}.
%%  As~shown below, when fuel runs out, the dataflow engine stops
%%  calling rewrite functions, effectively halting optimization.
%%  Binary search on the size of the fuel supply enables the compiler to
%%  identify unsound rewrites quickly \cite{whalley:isolation}.
%%  \end{itemize}











\section{The next 700 dataflow analyses}

\seclabel{logic}
\seclabel{next-700}

%%  \simon{I know that this section is Dear to the Ramsey Heart, but
%%  I still can't figure out what it is trying to say to the reader.
%%
%%  [For the record: one reason this section is dear to my heart is
%%  that after my talk at the Computing Lab in 2007, Alan Mycroft
%%  encouraged me in the strongest possible terms to get this
%%  explanation published. ---NR]
%%  
%%  Let me try one stab at what I think you might be trying to say.
%%  In this paper we have described a re-usable, polymorphic 
%%  framework (concretely, a library)
%%  for the analysis and transformation of control flow graphs.
%%  This is not a specialised tool; on the contrary we claim that
%%  a huge variety of optimisations can be viewed through the lens of
%%  dataflow optimisation.
%%  
%%  If that's the thrust, then the generalities of ths section would
%%  be stronger if they were supported with a list of dragon-book stuff
%%  that are all special cases. 
%%  
%%  But I may have misunderstood.}
%%  
%%  \remark{Here's what I'm trying to say: Gentle Reader,
%%  Bad people may have told you that dataflow analysis is new,
%%  special, difficult, and intimidating.  But it's not really---it's
%%  stuff you knew about all along.  Why, if you already understand the
%%  classic material on program correctness, you know how to write a new
%%  dataflow analysis---just invent an approximation of logic that has no
%%  infinite ascending chains.
%%  
%%  I could indeed go through the dragon book, but there's no time.}
%%  \simon{Well that is certainly better. Why not say that?  It's much 
%%  shorter too!  Evidence of the claim that data flow analysis is presented
%%  as special difficult intimidating might be good.}


Most of this paper is devoted to the implementation of dataflow
analyses and their associated transformations.
But we also promised you ideas.
The~main idea is that dataflow analysis is not unique,
difficult, or intimidating---it's a special case of work you may
already know about:
the seminal work of
\citet{floyd:assigning-meanings},
\citet{hoare:axiomatic-basis},
and
\citet{dijkstra:discipline}
on semantics of programming languages and
 formal methods of program construction.
If~you like this work, you are ready to invent a new
dataflow analysis:
just define an approximation of program logic
using a lattice with no infinite ascending chains.
% just approximate program logic using a lattice with no infinite ascending chains.



%%  \section{Logical view of optimization}
%%  
%%  Connection to Hoare logic:
%%   - facts derived by forward analysis as assertions on state
%%   - facts derived by backward analysis as assertions on continuation.
%%  
%%  Examples on board.

To~illuminate the connection, let's compare the formal methods a 
person uses to 
\emph{construct} a new program with the formal methods a compiler uses
to \emph{analyze} a program.
%The comparison is a bit tricky,
%because 
%People and compilers 
%typically work in opposite directions.
Dijkstra says people should start with a specification (the postcondition a procedure
must establish and the precondition it may assume), then construct the
procedure body from back 
to front, computing weakest preconditions until one is implied by the
procedure's precondition. 
%%  
%%  
%%  
%%  
%%  The most highly developed
%%  methodology for constructing a procedure (Dijkstra's) starts with a
%%  postcondition~$Q$ that the procedure must establish and a
%%  precondition~$P$ that the procedure may assume.
%%  Using Dijkstra's method, a programmer 
%%  works backwards, adding a statement, computing the weakest
%%  precondition of that statement with respect to~$Q$,
%%  and continuing with more statements until
%%  eventually the weakest precondition of the procedure body is implied by~$P$.
A~compiler, by contrast, is given a procedure with no specification,
and most analyses reason forward,  starting with the
weakest possible precondition and 
computing strongest postconditions that hold at various points throughout the program.
% using strongest postconditions to
% compute assertions that hold at various points throughout the program.

Computing strongest postconditions or weakest
preconditions is a simple matter of calculation---except for loops.
In~a loop, such as the one in the example of induction-variable elimination in
\secref{induction-var-elim}, 
%If~a compiler wants to know what @i@~is, it can easily see that on the
%first trip $@i@=0$, on the second trip $@i@=1$, and so on.
it's all too easy for a compiler to get stuck trying to compute an
infinite disjunction like $\bigvee_{k=0}^{\infty} @i@=k$.
Compilers are no good at solving recursion
equations that require limits of infinite sequences.
%
People don't like infinite sequences either, which is why they invent
\emph{loop invariants}. 
%%  
%%  In the classic situation, using weakest preconditions, a loop
%%  invariant~$I$ is an assertion that is \emph{stronger} than the weakest
%%  precondition, is invariant under the loop, and when conjoined with
%%  the loop's termination condition, implies the weakest precondition of
%%  what follows the loop.
%%  %%   and has these properties:
%%  %%  \begin{itemize}
%%  %%  \item
%%  %%  Together with the loop-termination condition, the invariant~$I$ implies the weakest
%%  %%  precondition of the code following the loop.
%%  %%  \item
%%  %%  The invariant~$I$ implies the weakest precondition of the loop body
%%  %%  with respect to~$I$.
%%  %%  In other words, $I$~is strong enought so that having $I$ hold at the
%%  %%  beginning of the loop body is enough to
%%  %%  imply that $I$~also holds at the end of the loop body.
%%  %%  \end{itemize}
%%  To be useful, a loop invariant must be neither too strong nor too
%%  weak.
%Much like finding a useful induction hypothesis for a proof, 
But finding a loop invariant requires art and intuition;
in~all but the simplest cases, it is beyond the capabilities of any
compiler.

%%  Strongest postconditions also require a loop invariant, but with dual
%%  properties:
%%  \begin{itemize}
%%  \item
%%  The invariant~$I$ is weaker than the true strongest postcondition,
%%  but it is implied by the strongest postcondition of the code before
%%  the loop.
%%  \item
%%  Assuming invariant~$I$ holds on entry to the loop, the strongest
%%  postcondition of the loop body must imply~$I$.
%%  \item
%%  The strongest postcondition of the entire loop is deemed to be the
%%  conjunction of invariant~$I$ and the termination condition.
%%  \end{itemize}
%%  


So if we can't find loop invariants automatically, what's a poor
compiler writer to do?
We~want assertions that justify interesting program
transformations. 
Such assertions can often be established by a dataflow analysis.
So to write a new dataflow analysis, we
should \emph{invent a new logic} which is
\emph{expressive enough to justify interesting transformations}
but \emph{inexpressive enough that no fact can increase more than
finitely many times}.
With the logic in place, the transfer functions simply approximate
  weakest preconditions (for a backward analysis) or strongest
  postconditions (for a forward analysis) as best they can within the
  constraints of the logic.
From~this point of view, we can understand the traditional
``optimization zoo'' as arising from a collection of approximate,
inexpressive logics, each with its own implementation of strongest postcondition
or weakest precondition. 
We~even venture to predict that the next 700 dataflow analyses will
arise from 700 more variations on 
program logic, each of which will be inexpressive in a different way.

%%  Let's review the examples of \secref{example:xforms} in that light.
%%  \begin{itemize}
%%  \item
%%  Logic $\bigwedge_i x_i=k_i$.
%%  \begin{itemize}
%%  \item
%%  $i = 7 \join \true \equiv \true$ (no loss of information)
%%  \item
%%  $i = 7 \join i= 7 \equiv  i=7$ (no loss of information)
%%  \item
%%  $i = 7 \join i = 8 \implies \true$ (loss of information)
%%  \end{itemize}
%%  Because there are only finitely many variables, we can say only
%%  finitely many things at each program point, and we're guaranteed to
%%  reach a fixed point.
%%  \item
%%  Example: induction-variable elimination.
%%  \begin{itemize}
%%  \item
%%  What is the proposition?
%%  \item
%%  Key point: be able to express the intermediate state where the
%%  invariant is temporarily violated (first add 1 to @i@, then add 4
%%  to~@p@).
%%  \end{itemize}
%%  \end{itemize}


%\clearpage

\section{Conclusion}

Compiler textbooks make dataflow analysis and optimization appear
difficult and complicated.
We make dataflow simple not by using a single magic
ingredient, but by applying ideas that are well understood by functional
programmers. %% and others who reason formally about programs.
These ideas
make it possible to think simple thoughts about classical code improvements.
\begin{itemize}
\item
We acknowledge only one program-analysis technique: the solution of
recursion equations.
Like our colleagues working in imperative languages, we solve
recursion equations by iterating to a fixed point.
Many equations relate
properties of program states; some relate properties of paths through
programs. 
\item
We consider only two
relations: weakest liberal precondition and strongest 
postcondition.
These relations correspond 
respectively
to
``backward dataflow problems'' and ``forward dataflow problems.''
\finalremark{Can we give an exmple of a property of program states which is
neither, just by way of contrast; ie this we cannot do.}
%%  \item
%%  In a language that admits loops, iterating weakest preconditions or
%%  strongest postconditions typically does not reach a fixed point in
%%  finitely many steps; hence the need for loop invariants
%%  \cite{hoare:axiomatic-basis,dijkstra:discipline,gries:science-programming}.
%%  In \secref{logic-reconciled}, we show that we can guarantee to reach a
%%  fixed point by limiting what we can express in the logic.\remark{needs
%%  a fix}
%%  We~show that many classical analyses can be explained this way;
%%  the great diversity of classical analyses corresponds to a great
%%  diversity of inexpressive logics.
%%  This view leads us to a unifying principle:
%%  \emph{To implement a code-improving transformation, find the least
%%    expressive logic that can justify the transformation, then use that
%%    logic to compute strongest postconditions, which justify the
%%    transformation locally.}
\item
Although our implementation allows graph nodes to be rewritten in any
way that preserves semantics, we concentrate on
only three program-transformation techniques:
substitution of equals for equals, 
insertion of assignments to unobserved variables, 
and removal of assignments to unobserved variables. 
Substitution of equals for equals is often justified by properties of program
states; for example, if variable~$x$
is always~7, we may substitute~7 for~$x$.\finalremark
{We can also justify substitution of \emph{labels} in goto
  statements by reasoning about continuations.  This is
  probably not the place to mention this fact.}
Insertion and removal of assignments are often justified by properties
of paths through programs;
for example, if an assignment's continuation does not use the variable
assigned~to, that assignment may be removed.

%%  Some compiler texts treat the removal of unreachable code as a
%%  code-improving transformation in its own right.
%%  In~our framework, unreachable code becomes unreachable in the
%%  garbage-collection sense, so no special effort is required to remove
%%  it.
\item
Complex program tranformations should be composed from simple
transformations. 
For example, both ``code motion'' and ``induction-variable
elimination'' should be implemented in three stages: insert new assignments;
substitute equals for equals; remove unneeded assignments
(\secref{induction-var-elim}). 

\item 
Because each rewrite leaves the semantics
of the program unchanged, 
we can use 
``optimization fuel'' to limit the number of rewrites.
 When we isolate a fault in the optimizer
(\secref{vpoiso}), we therefore have the luxury of debugging a single
 rewrite, not a complex transformation.
\end{itemize}

We also blend proven implementation techniques
in a way that
makes it easy
to implement classical code improvements.
\begin{itemize}
\item
We use the algorithm of \citet{lerner-grove-chambers:2002} to 
compose analyses and transformations.
This~algorithm makes it easy to compose complex transformations
from simple ones.
\item
Inspired by Huet's~\citeyearpar{huet:zipper} zipper,
we use an applicative representation of
control-flow graphs
\cite{ramsey-dias:applicative-flow-graph}. 
We~also improve on our own work by making the state of analysis
explicit in the inner dataflow monad;
these two functional data structures make it easy to implement the hard
part of Lerner, Grove, and Chamber's algorithm: discarding replacement
graphs and their associated state at need.
%
% important, but no longer mentioned in this paper:
%
%%  \item
%%  To \emph{construct} programs, we use a different representation of
%%  flow graphs, one which hides the complexity of the zipper and which
%%  provides a constant-time operation for joining flow graphs in
%%  sequence.
%%  It is inspired in part by Hughes's \citeyearpar{hughes:novel-lists}
%%  representation of lists, which supports a constant-time append operation.
\item
We also make 
the interface to our dataflow library polymorphic in the
representations of 
assignments and control-flow operations.
%%  Although our polymorphic representations have been instantiated only
%%  with the low-level intermediate code used by the Glasgow Haskell
%%  Compiler, they are intended eventually to be instantiated with
%%  machine-dependent representations of target-machine instructions, as
%%  part of a larger project of refactoring GHC's back ends.
%
This design seems obvious in retrospect,
but we underestimated the degree to which polymorphism would force us to
separate concerns.
Introducing polymorphism has made the code simpler, easier
to understand, and easier to maintain.\finalremark
{SLPJ: Is it possible to substantiate this claim by [more] examples?}
In particular, it forced us to make explicit \emph{exactly} what the
dataflow engine must know about flow-graph nodes.
(It must be able to enumerate the successors of a last
node, and it must be able to create an unconditional branch to the
label of its choice.)
\item
The code is pure.
State is managed explicitly using the dataflow monad, rather than
implicitly using ML's built-in state monad.
This explicit management makes it especially easy to implement such
operations as independent sub-analysis of a replacement graph.
\end{itemize}
%
% gen and kill are history
%
%%\item
%%Judicious use of Haskell type classes makes is possible to write
%%weakest precondition or strongest postcondition using the ``transfer
%%equations'' that are familiar from compiler textbooks.
%%If you like, you can even write overloaded @gen@ and @kill@ functions.
%%The benefit is that it is easy to compare the actual code with the
%%abstract treatments found in textbooks\ifgenkill \ (\secref{gen-kill})\fi.
Using our tools,
you can create a new code improvement in three steps:
create a lattice representation for the assertions that can be
expressed in the logic;
create transfer functions that approximate weakest preconditions or
strongest postconditions;
and 
create rewrite functions that use the assertions to justify
interesting program transformations.  
You can get quickly to the real 
intellectual work of code improvement: identifying interesting
transformations and the assertions that can justify them.

\makeatother

\providecommand\includeftpref{\relax} %% total bafflement -- workaround
\IfFileExists{nrbib.tex}{\bibliography{cs,ramsey}}{\bibliography{cs,ramsey,simon,jd}}
\bibliographystyle{plainnatx}

\subsubsection*{Relationship to previously published work}

\citet{ramsey-dias:applicative-flow-graph} describe
an applicative flow graph which is
an ancestor of the one used in this paper. 
That paper also presents an implementation of dataflow analysis
similar to the one described in \secref{engine} above.
The~interface to that implementation has evolved significantly, as
described above, and the analyses of
\secreftwo{create-analysis}{example-analyses}, the transformations of
\secref{rewrites}, and the 
explanation in \secref{next-700} are entirely new.
(If~the paper is accepted, we will find a way to work this information
into the text.)


\clearpage

\appendix

\section{Dataflow-engine functions}


\begin{figure*}
%%  forward_sol
%%          :: forall m l a . 
%%             (DebugNodes m l, LastNode l, Outputable a)
%%          => (forall a . Fuel -> Maybe a -> Maybe a)
%%  		-- Squashes proposed rewrites if there is
%%  		-- no more fuel; OR if we are doing a pure
%%  		-- analysis, so totally ignore the rewrite
%%  		-- ie. For pure-analysis the fn is (\_ _ -> Nothing)
%%          -> RewritingDepth	-- Shallow/deep
%%          -> PassName
%%          -> BlockEnv a		-- Initial set of facts
%%          -> ForwardTransfers m l a
%%          -> ForwardRewrites m l a
%%          -> a			-- Entry fact
%%          -> Graph m l
%%          -> Fuel
%%          -> DFM a (ForwardFixedPoint m l a (), Fuel)
\setcounter{codeline}{0}
\begin{numberedcode}
forward_sol squash rewrite name start_facts transfers rewrites = fixed_point
 where 
   fixed_point in_fact g fuel =
     do { setAllFacts start_facts
        ; (a, fuel) <- solve getExitFact in_fact g fuel
        ; facts <- getAllFacts
        ; let fp = ... facts ...
        ; return (fp, fuel)
        }

   solve :: DFM a b -> a -> Graph m l -> Fuel -> DFM a (b, Fuel)
   solve finish in_fact (Graph entry blockenv) fuel =
     let blocks = postorder_dfs_from blockenv entry
         set_or_save = mk_set_or_save (isJust . lookupBlockEnv blockenv)
         set_successor_facts (Block id _ tail) fuel =
           do { idfact <- getFact id
              ; (last_outs, fuel) <-
                  case squash fuel $ fr_first rewrites id idfact of
                    Nothing -> solve_tail (ft_first_out transfers id idfact) tail fuel
                    Just g -> do { (a, fuel) <- subAnalysis' $
                                     case rewrite of
                                       RewriteDeep -> solve getExitFact idfact g (oneLessFuel fuel)
                                       RewriteShallow ->
                                         do { a <- anal_f getExitFact idfact g
                                            ; return (a, oneLessFuel fuel) }
                                  ; solve_tail a tail fuel }
              ; set_or_save last_outs
              ; return fuel }

     in do { (last_outs, fuel) <- solve_tail in_fact entry fuel
           ; set_or_save last_outs                                    
           ; fuel <- run "forward" name set_successor_facts blocks fuel
           ; b <- finish
           ; return (b, fuel)
           }

   solve_tail in' (ZTail m t) fuel =
     case squash fuel $ fr_middle rewrites m in' of
       Nothing -> solve_tail (ft_middle_out transfers m in') t fuel
       Just g -> do { (a, fuel) <- subAnalysis' $                                     
                         case rewrite of                                              
                           RewriteDeep -> solve getExitFact in' g (oneLessFuel fuel)  
                           RewriteShallow -> do { a <- anal_f getExitFact in' g       
                                                ; return (a, oneLessFuel fuel) }      
                    ; solve_tail a t fuel                                             
                    }

   solve_tail in' (ZLast l) fuel = 
     case squash fuel $ either_last rewrites in' l of
       Nothing -> case l of LastOther l -> return (ft_last_outs transfers l in', fuel)
                            LastExit -> do { setExitFact in' ; return (LastOuts [], fuel) }
       Just g -> do { (last_outs :: LastOuts a, fuel) <- subAnalysis' $           
                        case rewrite of                                               
                          RewriteDeep -> solve lastOutFacts in' g (oneLessFuel fuel)  
                          RewriteShallow -> do { los <- anal_f lastOutFacts in' g     
                                               ; return (los, fuel) }                 
                    ; return (last_outs, fuel)                                        
                    }

   anal_f :: DFM a b -> a -> Graph m l -> DFM a b
   anal_f finish in' g = do { fwd_pure_anal name emptyBlockEnv transfers in' g; finish }
   either_last rewrites in' (LastExit) = in'
   either_last rewrites in' (LastOther l) = fr_last rewrites l in'
\end{numberedcode}
\caption{The forward solver}
\end{figure*}

\begin{figure*}
%%  forward_rew
%%          :: forall m l a . 
%%             (DebugNodes m l, LastNode l, Outputable a)
%%          => (forall a . Fuel -> Maybe a -> Maybe a)
%%          -> RewritingDepth
%%          -> BlockEnv a
%%          -> PassName
%%          -> ForwardTransfers m l a
%%          -> ForwardRewrites m l a
%%          -> a
%%          -> Graph m l
%%          -> Fuel
%%          -> DFM a (ForwardFixedPoint m l a (Graph m l), Fuel)
\setcounter{codeline}{0}
\begin{numberedcode}
forward_rew squash depth xstart_facts name transfers rewrites in_factx gx fuelx = fixed_pt_and_fuel
  where
    fixed_pt_and_fuel =
        do { (a, g, fuel) <- rewrite xstart_facts getExitFact in_factx gx fuelx
           ; facts <- getAllFacts
           ; let fp = ... facts ... g ...
           ; return (fp, fuel)
           }
    solve = forward_sol squash
    rewrite :: BlockEnv a -> DFM a b -> a -> Graph m l -> Fuel -> DFM a (b, Graph m l, Fuel)
    rewrite start finish in_fact g fuel =
      let Graph entry blockenv = g
          blocks = postorder_dfs_from blockenv entry
      in do { solve depth name start transfers rewrites in_fact g fuel
            ; eid <- freshBlockId "temporary entry id"
            ; (rewritten, fuel) <- rew_tail (ZFirst eid emptyStackInfo) in_fact entry emptyBlockEnv fuel
            ; (rewritten, fuel) <- rewrite_blocks blocks rewritten fuel
            ; a <- finish
            ; return (a, lgraphToGraph (LGraph eid 0 rewritten), fuel)
            }
    don't_rewrite facts finish in_fact g fuel =
        do  { solve depth name facts transfers rewrites in_fact g fuel
            ; a <- finish
            ; return (a, g, fuel)
            }
    inner_rew :: DFM a f -> a -> Graph m l -> Fuel -> DFM a (f, Graph m l, Fuel)
    inner_rew f i g fu = getAllFacts >>= \facts -> inner_rew' facts f i g fu
        where inner_rew' = case depth of RewriteShallow -> don't_rewrite
                                         RewriteDeep    -> rewrite
    rewrite_blocks :: [Block m l] -> (BlockEnv (Block m l)) -> Fuel -> DFM a (BlockEnv (Block m l), Fuel)
    rewrite_blocks [] rewritten fuel = return (rewritten, fuel)
    rewrite_blocks (Block id off t : bs) rewritten fuel =
      do { let h = ZFirst id off
         ; a <- getFact id
         ; case squash fuel $ fr_first rewrites id a of
             Nothing -> do { (rewritten, fuel) <- rew_tail h (ft_first_out transfers id a) t rewritten fuel
                           ; rewrite_blocks bs rewritten fuel }
             Just g  -> do { markGraphRewritten
                           ; (outfact, g, fuel) <- inner_rew getExitFact a g fuel
                           ; let (blocks, h) = splice_head h g
                           ; (rewritten, fuel) <- rew_tail h outfact t (blocks `plusBlockEnv` rewritten) fuel
                           ; rewrite_blocks bs rewritten fuel }
         }

    rew_tail head in' (ZTail m t) rewritten fuel =
      case squash fuel $ fr_middle rewrites m in' of
        Nothing -> rew_tail (ZHead head m) (ft_middle_out transfers m in') t rewritten fuel
        Just g -> do { markGraphRewritten
                     ; (a, g, fuel) <- inner_rew getExitFact in' g fuel
                     ; let (blocks, h) = splice_head head g
                     ; rew_tail h a t (blocks `plusBlockEnv` rewritten) fuel }
    rew_tail h in' (ZLast l) rewritten fuel = 
      case squash fuel $ either_last rewrites in' l of
        Nothing -> return (insertBlock (zipht h (ZLast l)) rewritten, fuel)
        Just g -> do { markGraphRewritten
                     ; ((), g, fuel) <- inner_rew (return ()) in' g fuel
                     ; let g' = splice_head_only h g
                     ; return (lg_blocks g' `plusBlockEnv` rewritten, fuel) }
    either_last rewrites in' (LastExit) = in'
    either_last rewrites in' (LastOther l) = fr_last rewrites l in'
\end{numberedcode}
\caption{The forward rewriter}
\end{figure*}






\end{document}
