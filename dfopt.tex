%%  advances over prior work
%%  
%%  (if open/closed) -- detect bad splices at compile time
%%  constant-time access to exit sequence
%%  (indeed exit  node)
%%  
%%  directly splicable in constant amortized time (vs Hughes
%%  technique)
%%  
%%  polymorphic in node type (type classes)
%%  
%%  

\iffalse % submission abstract

We present Hoopl, a Haskell library that makes it easy for compiler
writers to implement program transformations based on dataflow
analyses. The compiler writer must identify (a) logical assertions on
which the transformation will be based; (b) a representation of such
assertions, which should form a lattice of finite height; (c) transfer
functions that approximate weakest preconditions or strongest
postconditions over the assertions; and (d) rewrite functions whose
soundness is justified by the assertions. Hoopl uses the algorithm of
Lerner, Grove, and Chambers (2002), which can compose very simple
analyses and transformations in a way that achieves the same precision
as complex, handwritten "superanalyses." Hoopl will be the workhorse
of a new back end for the Glasgow Haskell Compiler.

Because the major claim in the paper is that Hoopl makes it easy to
implement program transformations, the paper is filled with examples,
which are written in Haskell.  The paper also sketches the
implementation of Hoopl, including some excerpts from the
implementation. 

\fi


\input{dfoptdu.tex}

\newif\ifpagetuning \pagetuningfalse  % adjust page breaks

\newif\ifnoauthornotes \noauthornotesfalse

\newif\iftimestamp\timestamptrue  % show MD5 stamp of paper

\timestamptrue % it's submission time

\IfFileExists{timestamp.tex}{}{\timestampfalse}

\newif\ifcutting \cuttingfalse % cutting down to submission size


\newif\ifgenkill\genkillfalse  % have a section on gen and kill
\genkillfalse


\newif\ifnotesinmargin \notesinmarginfalse
\IfFileExists{notesinmargin.tex}{\notesinmargintrue}{\relax}

\documentclass[blockstyle,preprint,natbib,nocopyrightspace]{sigplanconf}

\newcommand\ourlib{Hoopl}
   % higher-order optimization library
   % ('Hoople' was taken -- see hoople.org)
\let\hoopl\ourlib

\newcommand\ag{\ensuremath{\mathit{ag}}}
\renewcommand\ag{\ensuremath{g}}  % not really seeing that 'ag' is helpful here ---NR
\newcommand\rw{\ensuremath{\mathit{rw}}}

% l2h substitution ourlib Hoopl
% l2h substitution hoopl Hoopl

\newcommand\fs{\ensuremath{\mathit{fs}}} % dataflow facts, possibly plural

\newcommand\vfilbreak[1][\baselineskip]{%
  \vskip 0pt plus #1 \penalty -200 \vskip 0pt plus -#1 }

\usepackage{alltt}
\usepackage{array}
\usepackage{afterpage}
\newcommand\lbr{\char`\{}
\newcommand\rbr{\char`\}}
 
\clubpenalty=10000
\widowpenalty=10000

\usepackage{verbatim} % allows to define \begin{smallcode}
\newenvironment{smallcode}{\par\unskip\small\verbatim}{\endverbatim}

\newcommand\lineref[1]{line~\ref{line:#1}}
\newcommand\linepairref[2]{lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\linerangeref[2]{\mbox{lines~\ref{line:#1}--\ref{line:#2}}}
\newcommand\Lineref[1]{Line~\ref{line:#1}}
\newcommand\Linepairref[2]{Lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\Linerangeref[2]{\mbox{Lines~\ref{line:#1}--\ref{line:#2}}}

\makeatletter

\let\c@table=
           \c@figure % one counter for tables and figures, please

\newcommand\setlabel[1]{%
  \setlabel@#1!!\@endsetlabel
}
\def\setlabel@#1!#2!#3\@endsetlabel{%
  \ifx*#1*% line begins with label or is empty
     \ifx*#2*% line is empty
        \verbatim@line{}%
     \else
       \@stripbangs#3\@endsetlabel%
       \label{line:#2}%
     \fi
  \else
     \@stripbangs#1!#2!#3\@endsetlabel%
  \fi
}
\def\@stripbangs#1!!\@endsetlabel{%
  \verbatim@line{#1}%
}


\verbatim@line{hello mama}

\newcommand{\numberedcodebackspace}{0.5\baselineskip}

\newcounter{codeline}
\newenvironment{numberedcode}
  {\endgraf
     \def\verbatim@processline{%
        \noindent
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
               %{\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\phantom{: \,}}}%
            \else
               \refstepcounter{codeline}%
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\llap{\arabic{codeline}}: \,}}%
            \fi
        \expandafter\setlabel\expandafter{\the\verbatim@line}%
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
          \vspace*{-\numberedcodebackspace}\par%
        \else
          \the\verbatim@line\par
        \fi}%
   \verbatim
   }
   {\endverbatim}

\makeatother

\newcommand\arrow{\rightarrow}

\newcommand\join{\sqcup}
\newcommand\slotof[1]{\ensuremath{s_{#1}}}
\newcommand\tempof[1]{\ensuremath{t_{#1}}}
\let\tempOf=\tempof
\let\slotOf=\slotof

\makeatletter
\newcommand{\nrmono}[1]{%
  {\@tempdima = \fontdimen2\font\relax
   \texttt{\spaceskip = 1.1\@tempdima #1}}}
\makeatother

\usepackage{times}  % denser fonts
\renewcommand{\ttdefault}{aett} % \texttt that goes better with times fonts
\usepackage{enumerate}
\usepackage{url}
\usepackage{graphicx}
\usepackage{natbib}  % redundant for Simon
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar

\usepackage[ps2pdf,bookmarksopen,breaklinks,pdftitle=dataflow-made-simple]{hyperref}

\newcommand\naive{na\"\i ve}
\newcommand\naively{na\"\i vely}
\newcommand\Naive{Na\"\i ve}

\usepackage{amsfonts}
\newcommand\naturals{\ensuremath{\mathbb{N}}}
\newcommand\true{\ensuremath{\mathbf{true}}}
\newcommand\implies{\supseteq}  % could use \Rightarrow?

\newcommand\PAL{\mbox{C{\texttt{-{}-}}}}
\newcommand\high[1]{\mbox{\fboxsep=1pt \smash{\fbox{\vrule height 6pt
   depth 0pt width 0pt \leavevmode \kern 1pt #1}}}}

\usepackage{tabularx}

%%
%% 2009/05/10: removed 'float' package because it breaks multiple
%% \caption's per {figure} environment.   ---NR
%%
%%  % Put figures in boxes --- WHY??? --NR
%%  \usepackage{float}
%%  \floatstyle{boxed}
%%  \restylefloat{figure}
%%  \restylefloat{table}



% ON LINE THREE, set \noauthornotestrue to suppress notes (or not)

%\newcommand{\qed}{QED}
\ifnotesinmargin
  \long\def\authornote#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}
\else
  % Simon: please set \notesinmarginfalse on the first line
  \long\def\authornote#1{{\em #1\/}}
\fi
\ifnoauthornotes
  \def\authornote#1{\unskip\relax}
\fi

\newcommand{\simon}[1]{\authornote{SLPJ: #1}}
\newcommand{\norman}[1]{\authornote{NR: #1}}
\let\remark\norman
\def\finalremark#1{\relax}
% \let \finalremark \remark % uncomment after submission
\newcommand{\john}[1]{\authornote{JD: #1}}
\newcommand{\todo}[1]{\textbf{To~do:} \emph{#1}}
\newcommand\delendum[1]{\relax\ifvmode\else\unskip\fi\relax}

\newcommand\secref[1]{Section~\ref{sec:#1}}
\newcommand\secreftwo[2]{Sections \ref{sec:#1}~and~\ref{sec:#2}}
\newcommand\seclabel[1]{\label{sec:#1}}

\newcommand\figref[1]{Figure~\ref{fig:#1}}
\newcommand\figreftwo[2]{Figures \ref{fig:#1}~and~\ref{fig:#2}}
\newcommand\figlabel[1]{\label{fig:#1}}

\newcommand\tabref[1]{Table~\ref{tab:#1}}
\newcommand\tablabel[1]{\label{tab:#1}}


\newcommand{\CPS}{\textbf{StkMan}}    % Not sure what to call it.


\usepackage{code}   % At-sign notation

\iftimestamp
\input{timestamp}
\preprintfooter{\mdfivestamp}
\fi

\hyphenation{there-by}

\renewcommand{\floatpagefraction}{0.9} % must be less than \topfraction
\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{0.05}

\begin{document}
%\title{\ourlib: Dataflow Optimization Made Simple}
\title{\ourlib: A Modular, Reusable Library\\ for Dataflow Analysis and Transformation}
%\title{Implementing Dataflow Analysis and Optimization by Lifting Node Functions to Basic Blocks and Control-Flow Graphs}
\subtitle{Programming pearl}

%\titlebanner{\textsf{\mdseries\itshape Submitted to the 2010 ACM Symposium on Principles
%    of Programming Languages (POPL)}}

\ifnoauthornotes
\makeatletter
\let\HyPsd@Warning=
                \@gobble
\makeatother
\fi

% João


\authorinfo{Norman Ramsey}{Tufts University}{nr@cs.tufts.edu}
\authorinfo{Jo\~ao Dias}{Tufts University}{dias@cs.tufts.edu}
\authorinfo{Simon Peyton Jones}{Microsoft Research}{simonpj@microsoft.com}


\maketitle
 
\begin{abstract}
\iffalse % A vote for Simon's abstract
\remark{I have replaced a good abstract of the POPL submission with a
bad abstract of \emph{this} submission.}
We present \ourlib, a Haskell library that makes it easy for a
compiler writer
to implement program transformations based on dataflow analyses.
%%  The client defines
%%   (a)~logical assertions
%%  on which the transformation will be based;
%%  (b)~a~representation of such assertions, which
%%  \ifcutting
%%  should form a lattice of finite height;
%%  \else
%%  must have a lattice structure
%%   such that every assertion can be increased at
%%  most finitely many times;
%%  \fi
%%  (c)~transfer functions that approximate weakest preconditions or
%%  strongest postconditions over the assertions; and
%%  (d)~rewrite functions whose soundness is justified by the assertions.
%%  %%  To~guide compiler writers,
%%  %%  we show how dataflow analyses are related to
%%  %%  seminal work on program 
%%  %%  correctness. \simon{The ``next 700'' section sort of does so, but I'm not 
%%  %%  sure it deserves mention in the abstract.}
A~client of \ourlib\ defines a representation of 
logical assertions,
a transfer function that computes outgoing assertions from incoming
assertions, 
and a rewrite function that improves code when improvements are
justified by the assertions.
\ourlib\ does the actual analysis and transformation.

\ourlib\ implements state-of-the art algorithms:
Lerner, Grove, and Chambers's 
\citeyearpar{lerner-grove-chambers:2002}
composition of simple analyses and transformations, which achieves
the same precision as complex, handwritten
``super-analyses;''
and Whalley's \citeyearpar{whalley:isolation} dynamic technique for
isolating bugs in a client's code.
\ourlib's implementation is unique in that unlike previous
implementations,
it carefully separates the tricky
elements of each of these algorithms, so that they can be examined and
understood independently.


\simon{Here is an alternative abstract based on the four-sentence model.}
\remark{Four-sentence model?  You must teach me\ldots}
\fi
Dataflow analysis and transformation of control-flow graphs is
pervasive in optimizing compilers, but it is typically tightly
interwoven with the details of a \emph{particular} compiler.  
We~describe \ourlib{}, a reusable Haskell library that makes it
unusually easy for compiler writers to define new analyses and
transformations.
\ourlib's interface is modular and polymorphic,
and it offers unusually strong static guarantees.
The implementation
is also far from routine: it encapsulates 
state-of-the-art algorithms (interleaved analysis and rewriting,
dynamic error isolation), and it cleanly separates their tricky elements
so that they can be understood independently.
%
%\ourlib\ will be the workhorse of a new
%back end for the Glasgow Haskell Compiler (version~6.14, forthcoming).

%% \emph{Reviewers:} code examples are indexed at {\small\url{http://bit.ly/jkr3K}}
%%% Source: http://www.cs.tufts.edu/~nr/drop/popl-index.pdf
\end{abstract}

\makeatactive   %  Enable @foo@ notation

\section{Introduction}

A mature optimizing compiler for an imperative language includes many
analyses, the results of which justify the optimizer's
code-improving transformations.
Many of the most important analyses and transformations---constant
propagation, live-variable analysis, inlining, sinking of loads, 
and so on---should be regarded as particular cases of
a single general problem: \emph{dataflow analysis and optimization}.
%% \remark{I do not feel compelled to cite Muchnick (or anyone else) here}
Dataflow analysis is over thirty years old,
but a recent, seminal paper by \citet{lerner-grove-chambers:2002} goes further, 
describing a powerful but subtle way to
\emph{interleave} analysis and transformation so that each 
piggybacks on the other.

Because optimizations based on dataflow analysis 
share a common intellectual framework, and because that framework is
subtle, it it tempting to
try to build a single reusable library that embodies the 
subtle ideas, while
making it easy for clients to instantiate the library for different
situations.
Tempting, but difficult.
Although some such frameworks exist, as we discuss 
in \secref{related}, they have complex APIs and implementations,
and none implements the Lerner/Grove/Chambers technique.

In this paper we present \ourlib{} (short for ``higher-order
optimization library''), a new Haskell library for dataflow analysis and
optimization.  It has the following distinctive characteristics:

\begin{itemize}
\item
\ourlib\ is purely functional.  
Perhaps surprisingly, code that
manipulates control-flow graphs is easier to write, and far easier
to write correctly, when written in a purely functional style
\cite{ramsey-dias:applicative-flow-graph}.
When analysis and rewriting
are interleaved, so that rewriting must be done \emph{speculatively},
without knowing whether
the result of the rewrite will be retained or discarded,
the value of a purely functional style is intensified
(Sections \ref{sec:overview} and \ref{sec:fixpoints}).

\item
\ourlib\ is polymorphic. Just as a list library is
polymorphic in the list elements, so is \ourlib{} polymorphic, both in
the nodes that inhabit graphs, and in the dataflow facts that 
analyses compute over these graphs (\secref{using-hoopl}).

\item The paper by Lerner, Grove, and Chambers is inspiring but abstract.
We articulate their ideas in a concrete but simple API that hides 
a subtle implementation (Sections \ref{sec:graph-rep} and \ref{sec:using-hoopl}).  
You provide a representation for assertions, 
a transfer function that transforms assertions across a node, 
and a rewrite function that uses a computed assertion to 
justify rewriting a node.
\ourlib\ ``lifts'' these node-level functions to work over
control-flow graphs, sets up and solves recursion equations,
and interleaves rewriting with analysis.
Designing good abstractions (data types, APIs) is surprisingly
hard; we have been through over a dozen significantly different
iterations, and we offer our API as a contribution.
% ``in its own right''  -- there's an echo in here...

\item Analyses and transformations built on \ourlib\ 
are small, simple, and easy to get right because the client
only has to perform local reasoning (``@y@ is live before
@x:=y+2@'').%
\footnote
{Using \hoopl, it is not necessary to have the more complex rule
``if @x@~is live after @x:=y+2@ then @y@ is live before~it,''
because if @x@~is \emph{not} live after @x:=y+2@, the assignment
@x:=y+2@ will be eliminated.}
Moreover, \ourlib\ helps you write correct optimizations:
it~statically rules out transformations that violate invariants
of the control-flow graph (Sections \ref{sec:graph-rep} and \ref{sec:rewrites}),
and dynamically it can help find the first transformation that introduces a fault
in a test program (\secref{fuel}). 
\finalremark{SLPJ: I wanted to write more about open/closed,
but I like this sentence with its claim to both static and dynamic assistance,
and maybe the open/closed story is hard to understand here.}

% \item \ourlib{} makes use of GADTS and type functions to offer unusually
% strong static guarantees. In particular, nodes, basic blocks, and
% graphs are all statically typed by their open or closedness on entry, and
% their open or closedness on exit (\secref{graph-rep}). For example, an add instruction is
% open on entry and exit, while a branch is open on entry and closed on exit.
% Using these types we can statically guarantee that, say, an add instruction
% is rewritten to a graph that is also open on both entry and exit; and 
% that the user cannot construct a block where an add instruction follows an
% unconditional branch.  We know of no other system that offers 
% static guarantees this strong.

\item \ourlib{} implements subtle algorithms, including at least
(a)~interleaved analysis and rewriting, (b)~speculative rewriting,
(c)~computing fixed points, and (d)~dynamic fault isolation.
Previous implementations of these algorithms---including three of our
own---are complicated and hard to understand, because the tricky pieces
are implemented all together, inseparably. 
A~significant contribution of this paper is 
a new way to structure the implementation so that each tricky piece
is handled in just 
one place, separate from all the others (\secref{engine}). 
%\remark{This is a very important claim---is
%it substantiated in \secref{engine}? And shouldn't the word
%\textbf{contribution} appear in this~\P?}
% \simon{Better?} % yes thanks ---NR
The result is sufficiently elegant that 
we emphasize the implementation as an object of interest in
its own right.
\end{itemize}
A working prototype of \ourlib{} is available for download 
at \unskip\break \url{http://ghc.cs.tufts.edu/hoopl}.
It is no toy: an ancestor of this library is
embodied in the Glasgow Haskell Compiler, where it optimizes the
imperative {\PAL} code in GHC's back end.  The new design is far
nicer, and will be in GHC shortly.

The API for \ourlib{} seems quite natural, but it requires
relatively sophisticated aspects of Haskell's type system, such
as higher-rank polymorphism, GADTs, and type functions.
As such, \ourlib{} offers a compelling case study in the utility
of these features.


\section{Dataflow analysis {\&} transformation by \rlap{example}}
\seclabel{overview}
\seclabel{constant-propagation}
\seclabel{example:transforms}
\seclabel{example:xforms}

We begin by setting the scene, introducing some vocabulary, and
showing a small motivating example.
A control-flow graph, perhaps representing the body of a procedure,
is a collection of \emph{basic blocks}---or just ``blocks''.
Each block is a sequence of instructions,
beginning with a label and ending with a
control-transfer instruction that branches to other blocks.
% Each block has a label at the beginning,
% a sequence of 
%  -- each of which has a label at the 
% beginning.  Each block may branch to other blocks with arbitrarily
% complex control flow.
The goal of dataflow optimization is to compute valid
\emph{assertions} (or \emph{dataflow facts}), 
then use those assertions to justify code-improving
transformations (or \emph{rewrites}) on a \emph{control-flow graph}.  

Consider a concrete example: constant propagation with constant folding.
On the left we have a basic block; in the middle we have
assertions that hold between statements (or \emph{nodes}) 
in the block; and at
the right we have the result of transforming the block 
based on the assertions:
\begin{verbatim}
      Before        Facts        After
          ------------{}-------------
      x := 3+4                   x := 7
          ----------{x=7}------------
      z := x>5                   z := True
          -------{x=7, z=True}-------
      if z                       goto L1
       then goto L1
       else goto L2
\end{verbatim}
constant propagation works
from top to bottom.  We start with the empty assertion.  
Given the empty assertion and the node @x:=3+4@ can we make a (constant-folding)
transformation?
Yes!  We can replace the node with @x:=7@.
Now, given this transformed node,
and the original assertion, what assertion flows out of the bottom of
the transformed node?  
The assertion \{@x=7@\}.  
Given the assertion \{@x=7@\} and the node @z:=x>5@, can we make a
transformation?  Yes: constant propagation can replace the node with @z:=7>5@.
Now, can we do another transformation?  Yes: constant folding can 
replace the node with @z:=True@.
And so the process continues to the end of the block, where we
can replace the conditional branch with an unconditional one, @goto L1@.

\seclabel{simple-tx}
\paragraph{Interleaved transformation and analysis.}
Our example \emph{interleaves} transformation and analysis.
Interleaving makes it far easier to write effective analyses.
If, instead, we \emph{first} analyzed the block
and \emph{then} transformed it, the analysis would have to ``predict''
the transformations.
For example, given the incoming fact \{@x=7@\}
and the instruction @z:=x>5@,
a pure analysis could produce the outgoing fact
\{@x=7@, @z=True@\} by simplifying @x>5@ to @True@.
But the subsequent transformation must perform
\emph{exactly the same simplification} when it transforms the instruction to @z:=True@!
If instead we \emph{first} rewrite the node to @z:=True@, 
and \emph{then} apply the transfer function to the new node, 
the transfer function becomes laughably simple: it merely has to see if the
right hand side is a constant (you can see actual code in \secref{const-prop-client}).
The gain is even more compelling if there are a number of interacting 
analyses and/or transformations; for more substantial
examples, consult \citet{lerner-grove-chambers:2002}.

\paragraph{Forwards and backwards.}
Constant propagation works \emph{forwards}, and a fact is typically an
assertion about the program state (such as ``variable~@x@ holds value~@7@'').  
Some useful analyses work \emph{backwards}.
A~prime example is live-variable analysis, where a fact takes the form
``variable @x@ is dead'' and is an assertion about the
\emph{continuation} of a program point.  For example, the fact ``@x@~is
dead'' at a program point P is an assertion that @x@ is unused on any program
path starting at \mbox{P}.  % TeXbook, exercise 12.6
The accompanying transformation is called dead-code elimination;
if @x@~is dead, this transformation 
replaces the node @x:=e@ with a no-op.

% ----------------------------------------
\section{Representing control-flow graphs} \seclabel{graph-rep}

\ourlib{} is a library that makes it easy to define dataflow analyses,
and transformations driven by these analyses, on control-flow graphs.
Graphs are composed from smaller units, which we discuss from the
bottom up:\finalremark{something about replacement graphs?}
\begin{itemize}
\item A \emph{node} is defined by \ourlib's client;
\ourlib{} knows nothing about the representation of nodes (\secref{nodes}).
\item A basic \emph{block} is a sequence of nodes (\secref{blocks}).
\item A \emph{graph} is an arbitrarily complicated control-flow graph,
composed from basic blocks (\secref{graphs}).
\end{itemize}

\subsection{Shapes: Open and closed}

Nodes, blocks, and graphs share important properties in common.
In particular, each can be \emph{open or closed at entry}
and \emph{open or closed at exit}.  
An \emph{open} point is one at which control may implicitly ``fall through;''
to transfer control at a \emph{closed} point requires an explicit
control-transfer instruction.
For example,
\begin{itemize}
\item A shift-left instruction is open on entry (because control can fall into it
from the preceding instruction), and open on exit (because control falls through
to the next instruction).
\item An unconditional branch is open on entry, but closed on exit (because 
control cannot fall through to the next instruction).
\item A label is closed on entry (because in \ourlib{} we do not allow
control to fall through into a branch target), but open on exit.
\end{itemize}
% This taxonomy enables \ourlib{} to enforce invariants:
% only nodes closed at entry can be the targets of branches, and only nodes closed
% at exits can transfer control (see also \secref{edges}).
% As~a consequence, all control transfers originate at control-transfer
% instructions and terminated at labels; this invariant dramatically
% simplifies analysis and transformation. 
These examples concern nodes, but the same classification applies
to blocks and graphs.  For example the block
\begin{code}
   x:=7; y:=x+2; goto L
\end{code}
is open on entry and closed on exit.  
This is the block's \emph{shape}, which we often abbreviate
``open/closed;''
we may refer to an ``open/closed block.''

The shape of a thing determines that thing's control-flow properties.
In particular, whenever E is a node, block, or graph,
% : \simon{Removed the claim about a unique entry point.}
\begin{itemize}
\item
If E is open at the entry, it has a unique predecessor; 
if it is closed, it may have arbitrarily many predecessors---or none.
\item
If E is open at the exit, it has a unique successor; 
if it is closed, it may have arbitrarily many successors---or none.
\end{itemize}
%%%%    
%%%%    
%%%%    % \item Regardless of whether E is open or closed, 
%%%%    % it has a unique entry point where execution begins.
%%%%    \item If E is closed at exit, control leaves \emph{only}
%%%%    by explicit branches from closed-on-exit nodes.
%%%%    \item If E is open at exit, control \emph{may} leave E
%%%%    by ``falling through'' from a distinguished exit point.
%%%%    \remark{If E is a node or block, control \emph{only} leaves E by
%%%%    falling through, but this isn't so for a graph.  Example: a body of a
%%%%    loop contains a \texttt{break} statement} \simon{I don't understand.
%%%%    A break statement would have to be translated as a branch, no?
%%%%    Can you give a an example? I claim that control only leaves an 
%%%%    open graph by falling through.}
%%%%    \end{itemize}


\subsection{Nodes} \seclabel{nodes}

The primitive constituents of a \ourlib{} control-flow graph are
\emph{nodes}, which are defined by the client.
Typically, a node might represent a machine instruction, such as an
assignment, a call, or a conditional branch.  
But \ourlib{}'s graph representation is polymorphic in the node type,
so each client can define nodes as it likes.
Because they contain nodes defined by the client,
graphs can include arbitrary client-specified data, including
(say) C~statements, method calls in an object-oriented language, or
whatever.


\begin{figure}
\begin{code}
data `Node e x where
  `LabelNode  :: Label -> Node C O
  `Assign     :: Reg   -> Expr -> Node O O
  `Store      :: Expr  -> Expr -> Node O O
  `Branch     :: Label -> Node O C
  `CondBranch :: Expr  -> Label -> Label -> Node O C
    -- ... more constructors ...
\end{code}
\caption{A typical node type, as it might be defined by a client} 
\figlabel{cmm-node}
\end{figure}

\ourlib{} knows \emph{at compile time} whether a node is open or
  closed at entry and exit:
the type of a node has kind @*->*->*@, where the two type parameters
are type-level flags, one for entry and one for exit.
Such a type parameter may be instantiated only with type @O@~(for
open) or type~@C@ (for closed).
As an example,
\figref{cmm-node} shows a typical node type as it might be written by
one of \ourlib's {clients}.
The type parameters are written @e@ and @x@, for
entry and exit respectively.  
The type is a generalized algebraic data type, and
it is defined in using syntax
that gives the type of each constructor.  
\cite{peyton-jones:unification-based-gadts}.
For example, constructor @LabelNode@
takes a @Label@ and returns a node of type @Node C O@, where
the~``@C@'' says ``closed at entry'' and the~``@O@'' says ``open at exit''.  
The types @Label@, @O@, and~@C@ are 
defined by \ourlib{} (\figref{graph}).  

Similarly, an @Assign@ node takes a register and an expression, and
returns a @Node@ open at both entry and exit; the @Store@ node is
similar.  The types @Reg@ and @Expr@ are private to the client, and
\ourlib{} knows nothing of them.  
Finally, the control-transfer nodes @Branch@ and @CondBranch@ are open at entry
and closed at exit.  

Nodes closed on entry are the only targets of control transfers;
nodes open on entry and exit never perform control transfers;
and nodes closed on exit always perform control transfers\footnote{%
To obey these invariants,
a node for
a conditional-branch instruction, which typically either transfers control
\emph{or} falls through, must be represented as a two-target
conditional branch, with the fall-through path in a separate block.  
This representation is standard \cite{appel:modern},
and it costs nothing in practice:
such code is easily sequentialized without superfluous branches.
%a late-stage code-layout pass can readily reconstruct efficient code.
}.
Because of the position each type of node occupies in a
basic block,
we~often call them \emph{first}, \emph{middle}, and \emph{last} nodes
respectively.

\subsection{Blocks} \seclabel{blocks}

\begin{figure}
\begin{code}
data `O   -- Open
data `C   -- Closed

data `Block n e x where
 `BUnit :: n e x       -> Block n e x
 `BCat  :: Block n e O -> Block n O x -> Block n e x

data `Graph n e x where
  `GNil  :: Graph n O O
  `GUnit :: Block n O O -> Graph n O O
  `GMany :: MaybeO e (Block n O C) 
        -> Body n
        -> MaybeO x (Block n C O)
        -> Graph n e x

data `Body n where
  `BodyEmpty :: Body n
  `BodyUnit  :: Block n C C -> Body n
  `BodyCat   :: Body n -> Body n -> Body n

data `MaybeO ex t where
  `JustO    :: t -> MaybeO O t
  `NothingO ::      MaybeO C t

newtype `Label = Label Int

class Edges n where
  entryLabel :: n C x -> Label
  successors :: n e C -> [Label]
\end{code}
\caption{The block and graph types defined by \ourlib} 
\figlabel{graph} \figlabel{edges}
\end{figure}

\ourlib\ combines the client's nodes into
blocks and graphs, which, unlike the nodes, are defined by \ourlib{}
 (\figref{graph}).
A~@Block@ is parameterized over the node type~@n@
as well as over the same flag types that make it open or closed at
entry and exit.

The @BUnit@ constructor lifts a node to become a block; @BCat@
concatenates blocks in sequence. 
It~makes sense to concatenate blocks only when control can fall
through from the first to the second; therefore, 
two blocks may be concatenated {only} if each block is open at
the point of concatenation.
This restriction is enforced by the type of @BCat@, whose first 
argument must be open on exit, and whose second argument must be open on entry.
It~is statically impossible, for example, to concatenate a @Branch@
immediately before an
@Assign@.  
Indeed, the @Block@ type statically guarantees that any
closed/closed @Block@---which compiler writers normally 
call a ``basic block''---consists of exactly one closed/open node
(such as @Label@ in \figref{cmm-node}), 
followed by zero or more open/open nodes (@Assign@ or @Store@), 
and terminated with exactly one 
open/closed node (@Branch@ or @CondBranch@).  
Using GADTs to enforce these invariants is one of 
\ourlib{}'s innovations.
% Also notice that the definition of @Block@ guarantees that every @Block@ 
% has at least one node.
% SPJ: I think we've agreed to stick with PNil.
% \remark{So what? Who cares?  Why is it important
% that no block is empty?  (And if we had an empty block, we could ditch
% @PNil@ from @PGraph@, which woudl be an improvement.)}

\subsection{Graphs} \seclabel{graphs}

\ourlib{} composes blocks into graphs, which are also defined 
in  \figref{graph}.
Like, @Block@ the data type @Graph@ is parameterized over
both nodes @n@ and its open/closed shape (@e@ and @x@).
It has three constructors.  The first two
deal with the base cases of open/open graphs:
an empty graph is represented by @GNil@ while a single-block graph
is represented by @GUnit@.

More general graphs are represented by @GMany@, which has three
fields: an optional entry sequence, a body, and an optional exit
sequence.
\begin{itemize}
\item 
If the graph is open at the entry, it contains an entry sequence of type 
@Block n O C@.
We could represent this sequence as a value of type
@Maybe (Block n O C)@, but we can do better: 
a~value of @Maybe@ type requires a \emph{dynamic} test,
but we know \emph{statically}, at compile time, that the sequence is present if and only
if the graph is open at the entry.
We~express our compile-time knowledge by using the type
@MaybeO e (Block n O C)@, a type-indexed version of @Maybe@
which is also defined in \figref{graph}:
the type @MaybeO O a@ is isomorphic to~@a@, while 
the type @MaybeO C a@ is isomorphic to~@()@.
\item 
The body of the graph is a collection of  closed/closed blocks.  
To be able to concatenate bodies in constant time,
we introduce the representation 
@Body n@.
\item 
The exit sequence is dual to the entry sequence, and like the entry
sequence, its presence or absence is deducible from the static type of the graph.
\end{itemize}
% 
% \ourlib{} defines two distinct types of control-flow
% graphs.\remark{Why?  Wouldn't it be simpler to have just one type?
% (Not only do I think a reader will have this question, but I myself am
% not liking this direction\ldots)
% }
% The first,
% @Graph@, is entirely straightforward (see \figref{graph}): it is a
% collection of basic blocks, each with its own label.
% To~ensure that each label refers to a unique basic block, we represent
% the graph as a finite map from @Label@ to \mbox{@Block@ @n@ @C@ @C@}.  
% A~procedure might be represented by a @Graph@ together with the @Label@
% at which execution starts.
% 
% As illustrated in \secref{constant-propagation}, a client of \ourlib{} 
% may rewrite a node.
% But what does it rewrite the node \emph{to}?  
% In~\secref{constant-propagation}, it is enough to rewrite a node into
% a new node.
% But in general, a node might be rewritten into a graph, possibly with
% internal control flow:
% \begin{itemize}
% \item
% A redundant assignment might be eliminated by rewriting it into an
% empty graph.
% \item
% A~single node representing a ``safe'' divide instruction (that detects
% overflow) might be rewritten into a small graph of machine
% instructions that requires a test and conditional branch to detect
% overflow.
% \item
% A~single node that copies a large object might be rewritten into an
% internal loop that copies the words of the object individually.
% \end{itemize}
% None of these graphs can be represented as a @Graph@, because a
% @Graph@ is closed at both ends; 
% indeed a @Graph@ does not even have shape parameters @e@ and @x@.
% Rather, the rewrite function should return a ``partial graph'', 
% or @PGraph@, which can be open or closed at either
% end just like a node or block.  
% \remark{Not only don't I like this story, but I think it vitiates one
% of the beautiful parts of the paper: nodes are like blocks are like
% graphs.
% Instead of introducing @Graph@ as standard and treating @PGraph@ as
% special, I'd like to do it exactly the other way round.}
% 
% The definition of @PGraph@ is given in \figref{graph}.
% It includes a @PNil@ constructor for the empty graph,
% and a @PUnit@ constructor to lift a single @Block@ to be a @PGraph@.
% But in general, a graph has many blocks, and is built by
% \finalremark{Tabling the question of the closed/closed empty graph}%
% the @PMany@ constructor:
% \begin{itemize}
% \item
% A~graph that is open at the entry has a ``head'' block that is open at
% the entry and closed at the exit.\remark{Why isn't this called the
% ``entry sequence?''  Seems better than ``head''\ldots}
% A~graph that is closed at the entry doesn't have a ``head'' block.
% This choice is represented by a field of type @Head n O@ or @Head n e@:
% If the field's type is @Head n O@, it is created with the @Head@ value
% constructor, and it contains an entry sequence of type @Block n O C@.
% If the field's type is @Head n C@, it is created with the @NoHead@ value
% constructor, and it contains only a @Label@.\remark{Why a @Label@
% instead of nothing?}
% %\item The first field is called the ``head'' of the @PGraph@.
% %If it is $@(Head@\;b@)@$, the @PGraph@ is open on entry, with initial block $b$.
% %If it is $@(NoHead@\;l@)@$, the @PGraph@ is closed on entry, and execution begins
% %at block $l$. 
% We need the @Label@ $l$, because the @PGraph@ 
% may replace a node, so just as a node closed at entry has a unique entry point,
% so does a @PGraph@.\remark{This argument just seems like special
% pleading to me.   If you want the node analogy, \emph{start} from the
% node analogy.  And the actual reasoning goes like this: if the
% @PGraph@ replaces a node closed at entry, the replacement graph must
% begin with the \emph{same} label as the node it replaces.
% Now this reader starts to wonder why one would store  that label twice.}
% \item
% A~general graph, no matter how it is shaped at the entry or exit,
% contains a collection of labelled closed/closed (basic) blocks.
% This collection is represented as a @Graph@ of blocks,
% as discussed above.
% \item
% A~graph that is open at the exit has a ``tail'' block that is closed at
% the entry and open at the exit.\remark{Why isn't this called the
% ``exit sequence?''  Seems better than ``tail\ldots}
% A~graph that is closed at the exit doesn't have a ``tail'' block.
% The exit sequence (or lack thereof) is represented by a value of type
% @Tail n x@, where @x@~describes the shape of the exit.
% \remark{And if we could get rid of the beastly block IDs, we could
% have the \emph{same} construct here as at the entry sequence, and
%  enjoy a little parallel structure.}
% If the @Graph@ is closed on exit the tail is just @NoTail@.
% If it is open on exit, the tail is a (labelled) @Block@, open on
%  exit.\remark{Why is it labelled?  None of the other blocks closed on
%  entry is labelled?  What's special about this one?}
% \end{itemize}

Graphs concatenate nicely, in constant time.
Unlike blocks, two graphs may be concatenated 
not only when they are both open at the point of concatenation but also
when they are both closed---and not in the other two cases:
\finalremark{We've lost John's point about concatenation, which
I~found convincing.  If it's too late to do something by Friday, we
should remember to revisit this question later.
John's point is that in order to force programmers to think carefully
about what they are doing we should give concatenation different
\emph{names} depending on the \emph{types} of the graphs being
concatenated:
\begin{tabular}{>{\ttfamily}l}
O/O `cat` O/O\\
O/C `addEntrySequence` C/x\\
e/C `addExitSequence` C/O\\
e/C `addBlocks` C/C\\
\end{tabular}
}

\par{\small
\begin{code}
`gCat :: Graph n e a -> Graph n a x -> Graph n e x
gCat GNil g2 = g2
gCat g1 GNil = g1

gCat (GUnit ^b1) (GUnit ^b2) = GUnit (b1 `BCat` b2)

gCat (GUnit b) (GMany (JustO e) bs x) 
  = GMany (JustO (b `BCat` e)) bs x

gCat (GMany e bs (JustO x)) (GUnit b2) 
  = GMany e bs (JustO (x `BCat` b2))

gCat (GMany e1 bs1 (JustO x1)) (GMany (JustO e2) bs2 x2)
  = GMany e1 (bs1 `BodyCat` b `BodyCat` bs2) x2
  where b = BodyUnit (x1 `BCat` e2)

gCat (GMany e1 bs1 NothingO) (GMany NothingO bs2 x2)
  = GMany e1 (bs1 `BodyCat` bs2) x2
\end{code}}
This definition is a nice example of the power of GADTs: the
pattern matching is exhaustive, and all the open/closed invariants are
statically checked.  For example, consider the second-last equation for @gCat@.
Since the exit link of the first argument is @JustO x1@,
we know that type parameter~@a@ is~@C@, and hence the entry link of the second 
argument must be @JustO e2@.
Moreover, block~@x1@ must be
closed/open, and block~@e2@ must be open/closed.  
We can therefore concatenate them
with @BCat@ to produce a closed/closed block, which is
added to the @Body@ of the result.

We~have carefully crafted the types so that if @BCat@ and @BodyCat@
are considered as associative operators, 
every graph has a unique representation.\remark{So what? (Doumont)
Need some words about how this is part of making the API simple for
the client---I've added something to the end of the paragraph, but I'm
not particularly thrilled.}
%%  \simon{Well, you were the one who was so keen on a unique representation!
%%  And since we have one, I think tis useful to say so. Lastly, the representation of
%%  singleton blocks is not entirely obvious.}
%%%%    
%%%%    An empty open/open graph is represented
%%%%    by @GNil@, while a closed/closed one is @gNilCC@:
%%%%    \par {\small
%%%%    \begin{code}
%%%%      gNilCC :: Graph C C
%%%%      gNilCC = GMany NothingO BodyEmpty NothingO
%%%%    \end{code}}
%%%%    The representation of a @Graph@ consisting of a single block~@b@ 
%%%%    depends on the shape of~@b@:\remark{Does anyone care?}
%%%%    \par{\small
%%%%    \begin{code}
%%%%      gUnitOO :: Block O O -> Graph O O
%%%%      gUnitOC :: Block O C -> Graph O C
%%%%      gUnitCO :: Block O C -> Graph C O
%%%%      gUnitCC :: Block O C -> Graph C C
%%%%      gUnitOO b = GUnit b
%%%%      gUnitOC b = GMany (JustO b) BodyEmpty   NothingO
%%%%      gUnitCO b = GMany NothingO  BodyEmpty   (JustO b)
%%%%      gUnitCC b = GMany NothingO  (BodyUnit b) NothingO
%%%%    \end{code}}
%%%%    Multi-block graphs are similar.
%%%%    From these definitions
To guarantee uniqueness, @GUnit@ is restricted to open/open
blocks.
If~@GUnit@ were more accommodating, there would be 
more than one way to represent some graphs, and it wouldn't be obvious
to a client which representation to choose---or if the choice made a difference.

\subsection{Labels and successors} \seclabel{edges}

If \ourlib{} knows nothing about nodes, how can it know where
a control transfer goes, or what is the @Label@ at the start of a block?
To~answer such questions,
the standard Haskell idiom is to define a type class whose methods
provide exactly the operations needed;
\ourlib's type class, called @Edges@, is given in \figref{edges}.
The @entryLabel@ method takes a first node (one closed on entry, \secref{nodes})
and returns its @Label@;
the @successors@ method takes a last node (closed on exit) and returns
the @Label@s to 
which it can transfer control.  
A~middle node, which is open at both entry and exit, cannot refer to
any @Label@s, 
so no corresponding interrogation function is needed.

A node type defined by a client must be an instance of @Edges@.
In~\figref{cmm-node},
the client's instance declaration for @Node@ would be
\begin{code}
instance Edges Node where
  entryLabel (LabelNode l) = l
  successors (Branch b) = [b]
  successors (CondBranch e b1 b2) = [b1,b2]
\end{code}
Again, the pattern matching for both functions is exhaustive, and
the compiler statically checks this fact.  Here, @entryLabel@ 
cannot be applied to an @Assign@ or @Branch@ node,
and any attempt to define a case for @Assign@ or @Branch@ would result
in a type error.

While it is required for the client to provide this information about
nodes, it is very convenient for \ourlib\ to get the same information
about blocks.
For its own internal use,
\ourlib{} provides this instance declaration for the @Block@ type:
\begin{code}
instance Edges n => Edges (Block n) where
  entryLabel (BUnit n)  = entryLabel n
  entryLabel (BCat b _) = entryLabel b
  successors (BUnit n)  = successors n
  successors (BCat _ b) = successors b
\end{code}
Because the functions @entryLabel@ and @successors@ are used to track control
flow \emph{within} a graph, \ourlib\ does not need to ask for the
entry label or successors of a @Graph@ itself.
Indeed, @Graph@ \emph{cannot} be an instance of @Edges@, because even
if a @Graph@ is closed at the entry, it does not have a unique entry label.
%
% A slight infelicity is that we cannot make @Graph@ an instance of @Edges@,
% because a graph closed on entry has no unique label.  Fortunately, we never
% need @Graph@s to be in @Edges@.
%
% ``Never apologize.  Never confess to infelicity.'' ---SLPJ

\section {Using \ourlib{} to analyze and transform graphs} \seclabel{using-hoopl}
\seclabel{making-simple}
\seclabel{create-analysis}

\begin{figure}
\begin{code}
data `ForwardPass n f
  = `FwdPass { `fp_lattice  :: DataflowLattice f
            , `fp_transfer :: FwdTransfer n f
            , `fp_rewrite  :: FwdRewrite n f }

------- Lattice ----------
data `DataflowLattice a = DataflowLattice
 {`fact_bot    :: a,
  `fact_extend :: a -> a -> (ChangeFlag, a) }

data `ChangeFlag = `NoChange | `SomeChange

------- Transfers ----------
type `FwdTransfer n f 
  = forall e x. n e x -> Fact e f -> Fact x f 

------- Rewrites ----------
type `FwdRewrite n f 
  = forall e x. n e x -> Fact e f
             -> Maybe (FwdRes n f e x)

data `FwdRes n f e x 
  = FwdRes (AGraph n e x) (FwdRewrite n f)

------- Fact-like things -------
type family   `Fact x f :: *
type instance Fact C f = FactBase f
type instance Fact O f = f

------- FactBase -------
type `FactBase f = LabelMap f
 -- A finite mapping from Labels to facts f
\end{code}
\caption{\ourlib{} API data types}
  \figlabel{api-types}
  \figlabel{lattice-type} \figlabel{lattice}
  \figlabel{transfers}  \figlabel{rewrites}
\end{figure}
% omit mkFactBase :: [(Label, f)] -> FactBase f

Now that we have graphs, how do we optimize them?
\ourlib{} makes it  easy for a
client to build a new dataflow analysis and optimization.  
The client must supply the following pieces:
\begin{itemize}
\item \emph{A node type} (\secref{graph-rep}).  
\ourlib{} supplies the @Block@ and @Graph@ types
that let the client build control-flow graphs out of nodes.
\item \emph{A data type of facts} and some operations over 
those facts (\secref{facts}).
Each analysis uses facts that are specific to that particular analysis,
which \ourlib{} accommodates by being polymorphic in 
the fact type.   
\item \emph{A transfer function} that takes a node and returns a
\emph{fact transformer}, which takes a fact flowing into the node and
returns the transformed fact that flows out of the node (\secref{transfers}).  
\item \emph{A rewrite function} that takes a node and an input fact,
and which returns either @Nothing@
or @(Just g)@ where @g@~is a graph that should
replace the node.  
The ability to replace a \emph{node} by a \emph{graph} that may include
internal control flow is
crucial for many code-improving transformations.
We discuss the rewrite function
in Sections \ref{sec:rewrites} and \ref{sec:shallow-vs-deep}.
\end{itemize}
Facts, transfer functions, and rewrite functions work closely together,
so we represent their combination
as a single record of type @ForwardPass@ (\figref{api-types}).
The elements of @ForwardPass@ are, and must be,
polymorphic functions---\ourlib{} makes essential use of higher-rank types.


Given a node type~@n@ and a @ForwardPass@,
a client can ask \ourlib{}\ to analyze and rewrite a closed/closed
graph represented as @Body n@:
\begin{code}
analyzeAndRewriteFwd
  :: Edges n           -- Access to flow edges
  => ForwardPass n f   -- Lattice, transfer, 
                       -- and rewrite functions
  -> Body n                 -- Input body
  -> FactBase f             -- Input fact(s)
  -> FuelMonad (Body n,     -- Result body
                FactBase f) -- ...and its facts
\end{code}
Given a @ForwardPass@, the 
analyze-and-rewrite function transforms a @Body@ into an
optimized @Body@.
As its type shows, this function
is polymorphic in the types of nodes~@n@ and facts~@f@;
these types are determined entirely by the client.


As well as taking and returning a @Body@, the function also takes
some input facts (the @FactBase@) and produces output facts. 
A~@FactBase@ is simply a finite mapping from @Label@ to facts.
The
output @FactBase@ maps each @Label@ in the @Body@ to its fact; if
the @Label@ is not in the domain of the @FactBase@, its fact is the
bottom of the lattice.
Similarly the input @FactBase@ supplies any
facts that hold on entry to the @Body@.
For example, in our constant-propagation example,\remark{Where is this
vaunted example?  Where is @Top@ defined??}
if the @Body@
represents the body of a procedure 
with parameters $x,y,z$, we would map the entry @Label@ to a fact
$@[@(x,@Top@), (y,@Top@), (z,@Top@)@]@$, to specify that the procedure's
parameters have unknown values.

The client's model of how @analyzeAndRewriteFwd@ works is as follows:
\ourlib\ walks forward over each block in the graph.
At each node, \ourlib\ applies the
rewrite function to the node and the incoming fact.  If the rewrite
function returns @Nothing@, the node is retained as part of the output
graph, the transfer function is used to compute the downstream fact,
and \ourlib\ moves on to the next node.
But if the rewrite function returns @(Just g)@,
indicating that it wants to rewrite the node to the replacement graph~@g@, then
\ourlib\ recursively analyzes and rewrites~@g@ before moving on the the next node.
A~node following a rewritten node sees 
\emph{up-to-date} facts; that is, its input fact is computed by
analyzing the replacement graph.

In this section we flesh out the
\emph{interface} to @analyzeAndRewriteFwd@, leaving the implementation for
\secref{engine}.  

\subsection{Dataflow lattices} \seclabel{lattices} \seclabel{facts}

For each analysis or transformation, the client must define a type
of dataflow facts.
A~dataflow fact often represents an assertion
about a program point,\footnote
{In \ourlib, a program point is simply an edge in a
control-flow graph.}
but in general, dataflow analysis establishes properties of \emph{paths}:
\begin{itemize}
\item An assertion about all paths \emph{to} a program point is established
by a \emph{forwards analysis}. For example the assertion ``$@x@=@3@$'' at point P 
claims that variable @x@ holds value @3@ at P, regardless of the
path by which P is reached.
\item An assertion about all paths \emph{from} a program point is 
established by a \emph{backwards analysis}. For example, the 
assertion ``@x@ is dead'' at point P claims that no path from P uses 
variable @x@.
\end{itemize}

A~set of dataflow facts must form a lattice, and \ourlib{} must know
(a)~the bottom element of the lattice and (b)~how to take 
the least upper bound of (join) two elements.
To ensure that analysis
terminates, it is enough if no fact has more than finitely many
distinct facts above it, so that every infinite sequence of joins
eventually reaches a fixed point.

In our constant-propagation example,\remark{I'm worried this example
has been lost}
a fact is a finite conjunction of
sub-facts, at most one for each variable.  Each sub-fact has the form ``@x=@$k$'',
for some constant $k$, or ``@x=@$\top$'' (meaning that there is 
no single value held by $x$ at this program point).  For example, 
consider this graph,
where we assume @L1@ is the entry point:
\begin{verbatim}
  L1: x=3; y=4; if z then goto L2 else goto L3
  L2: x=7; goto L3
  L3: ...
\end{verbatim}
Since control flows to @L3@ from two places,
we must join the facts coming from those two places.  The fact at the
end of block @L1@ is $@x=3@ \land @y=4@ \land @z=False@$, while at the end of
@L2@ the fact is $@x=7@ \land @y=4@ \land @z=True@$.  
Both blocks branch to @L3@
where they must be joined, giving
the fact $@x=@\top \land @y=4@ \land @z=@\top$ at @L3@, where the $\top$'s indicate
that the analysis cannot
nail down @x@ and @z@ to hold a single value at @L3@\footnote{
In this example @x@ and @z@ really do vary at @L3@, but in general
the analysis might be conservative.}.

In practice, joins are computed at labels.
If~$f_{\mathit{id}}$ is the fact currently associated with the
label~$\mathit{id}$, 
and if a transfer function propagates a new fact~$f_{\mathit{new}}$
into the label~$\mathit{id}$, 
the dataflow engine replaces $f_{\mathit{id}}$ with
the join  $f_{\mathit{new}} \join f_{\mathit{id}}$.
Furthermore, the dataflow engine wants to know if
  $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$,
because if not, the analysis has not reached a fixed point.

Concretely, @analyzeAndRewriteFwd@ is polymorphic in the fact
type~@f@, and it requires an argument of type @ForwardPass n f@,
which in turn contains a value of type @DataflowLattice f@.
The lattice
type is shown in \figref{lattice}; it gives \ourlib{} the lattice's
bottom element and
join operation.
As noted in the previous paragraph, 
\ourlib{} needs to know when the result of a join is equal
to one of the arguments joined.  
Because this information is often available very cheaply at the time
when the join is computed, \ourlib\ does not
 require a separate equality test on facts (which might be
expensive).
Instead, \ourlib\ requires that @fact_extend@ return a @ChangeFlag@ as
well as the least upper bound.
The @ChangeFlag@ should be @NoChange@ if
the result is the same as the second argument, and @SomeChange@ if the
result differs.  (Function @fact_extend@ is \emph{not} symmetric in
its arguments.)

A~possible representation of the facts needed to implement
constant propagation is shown in \figref{const-prop}.
A~fact
is represented as a finite map from a variable to a value of type
@Maybe HasConst@.%
\remark{Why does the figure have @Reg@ instead of @Var@?}
A~variable $x$ maps to @Nothing@ iff $x=\bot$;
$x$~maps to @Just Top@ iff $x=\top$;
and $x$~maps to $@Just@\, (@I@\, k)$ iff $x=k$ (and similarly for
Boolean constants).\remark{Ugh}
Any one procedure has only
finitely many variables; only finitely many facts are computed at any
program point; and in this lattice any one fact can increase at most
twice.  These properties ensure that the dataflow engine will reach a
fixed point.


\subsection{The transfer function} \seclabel{transfers}

A~forward transfer function is presented with the dataflow fact(s) on
the edge(s) coming 
into a node, and it computes dataflow fact(s) on the outgoing edge(s).
In a forward analysis, the dataflow engine starts with the fact at the
beginning of a block and applies the transfer function to successive 
nodes in that block until eventually the transfer function for the last node
computes the facts that are propagated to the block's successors.
For example, consider this graph, with entry at @L1@:
\begin{code}
  L1: x=3; goto L2
  L2: y=x+4; x=x-1; 
      if x>0 then goto L2 else return
\end{code}
A forward analysis starts with the bottom fact \{\} at every label.
Analyzing @L1@ propagates this fact forward, by applying the transfer 
function successively to the nodes
of @L1@, emerging with the fact \{@x=3@\} for @L2@.
This new fact is joined with the existing (bottom) fact for @L2@.
Now the analysis propagates @L2@'s fact forward, again using the transfer
function, this time emerging with a new fact \{@x=2@, @y=7@\} for @L2@.
Again, the new fact is joined with the existing fact for @L2@, and the process
is iterated until the facts for each label reach a fixed point.

But wait!  What is the \emph{type} of the transfer function?
If the node is open at exit, the transfer function produces a single fact.
But what if the node is \emph{closed} on exit?
In that case the transfer function must
produce a list of (@Label@,fact) pairs, one for each outgoing edge.  
%
\emph{So the type of the transfer function's result 
depends on the shape of the node's exit.}
Fortunately, this dependency can be expressed precisely, at compile
time, by Haskell's (recently added) 
\emph{indexed type families}.
The relevant \ourlib{} definitions are given in \figref{transfers}.
A~forward transfer function, of type (@FwdTransfer@ @n@ @f@), 
is a function polymorphic in @e@ and @x@.  It takes a 
node of type \mbox{(@n@ @e@ @x@)} and a fact of type~@f@, and it produces an
outgoing ``fact-like thing'' of type (@Fact@ @x@ @f@).  The 
type constructor @Fact@
should be thought of as a type-level function; its signature is given in the
@type family@ declaration, while its definition is given by two @type instance@
declarations.  The first declaration says that the fact-like thing
coming out of a node
\emph{open} at the exit is just a fact~@f@. The second declaration says that 
the fact-like thing
coming out of a node
\emph{close} at the exit is a mapping from @Label@ to facts.

We have ordered the arguments such that if
\begin{code}
  transfer_fn :: FwdTransfer n f
  node        :: n e x
\end{code}
then @(transfer_fn n)@ is a predicate transformer:
\begin{code}
  transfer_fn n :: Fact e f -> Fact x f
\end{code}


\subsection{The rewrite function} 
 \seclabel{rewrites} 
 \seclabel{example-rewrites}

\begin{figure}
\hfuzz=5.7pt % cheating again!
\begin{code}
type `AGraph n e x 
  = [Label] -> (Graph n e x, [Label])

`withLabels :: Int -> ([Label] -> AGraph n e x)
           -> AGraph n e x
withLabels n fn = \ls -> fn (take n ls) (drop n ls)

`mkIfThenElse :: Expr -> AGraph Node O O
             -> AGraph Node O O -> AGraph Node O O
mkIfThenElse p t e
  = withLabels 3 $ \[l1,l2,l3] ->
    gUnitOC (BUnit (CondBranch p l1 l2))   `gCat` 
    mkLabel l1 `gCat` t `gCat` mkBranch l3 `gCat`
    mkLabel l2 `gCat` e `gCat` mkBranch l3 `gCat`
    mkLabel l3

mkLabel  l = gUnitCO (BUnit (LabelNode l))          
mkBranch l = gUnitOC (BUnit (Branch l))
\end{code}
\caption{The \texttt{AGraph} type and example constructions} \figlabel{agraph}
\end{figure}

We compute dataflow facts in order to enable code-improving
transformations.
In our constant-propagation example,\remark{It's a myth!}
the dataflow facts may enable us
to simplify an expression by performing constant folding, or to 
turn a conditional branch into an unconditional one.
Similarly, a liveness analysis may allow us to 
replace a dead assignment with a no-op; knowing that a register is
live at a call site might provoke us to insert a spill before the call; and so on.

A @ForwardPass@ therefore includes a \emph{rewriting function}, whose
type, @FwdRewrite@, is given in \figref{api-types}.
A rewriting function takes a node and a fact, and optionally returns\ldots what?
At first one might
expect that rewriting should return a new node, but that is not enough:
We~might want to remove a node by rewriting it to the empty graph,
or more ambitiously, we might want to replace a high-level operation
with a tree of conditional branches or a loop, which would entail
introducing new blocks with internal control flow.
In general, a rewrite function must be able to return an
\emph{arbitrary graph}. 

Concretely, a @FwdRewrite@ takes a node and a suitably shaped
fact, and returns either @Nothing@, indicating that the node should
not be replaces,
or $@(Just@\;@(FwdRes@\;\ag\;\rw@))@$, indicating that the node should
be replaced with~\ag: the replacement graph.  
You may have been expecting \ag\ to have type  @Graph n e x@, 
but it actually has type @AGraph n e x@.
The reason is that 
if the rewriter makes graphs containing blocks,
it may need fresh @Labels@.
An~@AGraph@ provides easy access to fresh lables,
using @withLabels@ (\figref{agraph}).
The figure also shows an implementation of @AGraph@
and a few simple functions typically used to
build @AGraph@s.


The type of @FwdRewrite@ in \figref{api-types} guarantees
\emph{at compile time} that
the replacement graph $\ag$ has
the \emph{same} open/closed shape as the node being rewritten.
For example, a branch instruction can be replaced only by a graph 
closed at the exit.  Moreover, because only an open/open graph can be 
empty---look at the type of @GNil@ in \figref{graph}---the type 
of @FwdRewrite@ 
guarantees, at compile time, that no head of a block (closed/open)
or tail of a block (open/closed) can ever be deleted by being
rewritten to an empty graph.

\subsection{Shallow vs deep rewriting}
 \seclabel{shallow-vs-deep}

Once the rewrite has been performed, what then?  
Since the rewrite returns a graph, the 
replacement graph must itself be analyzed, and its nodes may be rewritten.
So we must call @analyzeAndRewriteFwd@ to process the replacement
graph---but what @ForwardPass@ should we use?
There are two common situations:
\begin{itemize}
\item Sometimes we want to analyze and transform the replacement graph
with an unmodified @ForwardPass@, further rewriting the replacement graph.
This procedure is
called \emph{deep rewriting}. 
When deep rewriting is used, the client's rewrite function must
ensure that the graphs it produces are not rewritten indefinitely
(\secref{correctness}). 
\item Sometimes we want to analyze \emph{but not further rewrite} the
replacement graph.  This procedure is called \emph{shallow rewriting}.
It~is easily implemented by using a modified @ForwardPass@
whose rewriting function always returns @Nothing@.
\end{itemize}
Deep rewriting is essential to achieve the full benefits of
interleaved analysis and transformation
\citep{lerner-grove-chambers:2002}.
But shallow rewriting can be vital as well; 
for example, a forward dataflow pass that inserts
a spill before a call must not rewrite the call again, lest it attempt
to insert infinitely many spills.

An~innovation of \hoopl\ is to build the choice of shallow or deep
rewriting into each rewrite function, 
an idea that is elegantly captured by the
@FwdRes@ type returned by a @FwdRewrite@ (\figref{api-types}).
The first component of the @FwdRes@ is the replacement graph, as discussed earlier.
The second component, $\rw$, is a 
\emph{new rewriting function} to use when recursively processing
the replacement graph. 
For shallow rewriting this new function is
the constant @Nothing@ function; for deep rewriting it is the original
rewriting function.


\subsection{Composing rewrite functions and dataflow passes} \seclabel{combinators}

By requiring each rewrite to return a new rewrite function,
\hoopl\ enables a variety of combinators over
rewrite functions. %% \vadjust{\penalty-10000}
For example, here is a function
that combines two rewriting functions in sequence:
\par{\small
\begin{code}
  thenFwdRw :: FwdRewrite n f 
            -> FwdRewrite n f 
            -> FwdRewrite n f
  `thenFwdRw rw1 rw2 n f
    = case rw1 n f of
        Nothing              -> rw2 n f
        Just (FwdRes g rw1a) -> Just $ FwdRes g $
                                rw1a `thenFwdRw` rw2

  noFwdRw :: FwdRewrite n f
  noFwdRw n f = Nothing
\end{code}}
What a beautiful type @thenFwdRw@ has! It tries @rw1@, and if @rw1@
declines to rewrite, it behaves like @rw2@.  But if
@rw1@ rewrites, returning a new rewriter @rw1a@, the the overall call also
succeeds, returning a new rewrite function obtained by combining @rw1a@
with @rw2@.  (We cannot apply @rw1a@ or @rw2@ 
directly to the replacment graph~@g@, 
because @r1@~returns a graph and @rw2@~expects a node.)
The rewriter @noFwdRw@ is the identity of @thenFwdRw@.
Finally, @thenFwdRw@ can 
combine a deep-rewriting function and a shallow-rewriting function,
to produce a rewriting function that is a combination of deep and shallow.
%%This is something that the Lerner/Grove/Chambers framework could not do,
%%because there was a global shallow/deep flag.
%% Our shortsightedness; Lerner/Grove/Chambers is deep only ---NR

A shallow rewriting function can easily be made deep:
\begin{code}
  deepFwdRw :: FwdRewrite n f -> FwdRewrite n f
  deepFwdRw rw = rw `thenFwdRw` deepFwdRw rw
\end{code}
If we have shallow rewrites $A$~and~$B$ then we can build $AB$,
$A^*B$, $(AB)^*$, 
and so on: sequential composition is @thenFwdRw@ and the Kleene star
is @deepFwdRw@. 

The combinators above operate on rewrite functions that share a common
fact type and transfer function.
It~can also be useful to combine entire dataflow passes that use
different facts.
We~invite you to write one such combinator, with type
\begin{code}
  thenFwd :: ForwardPass n f1 
          -> ForwardPass n f2
          -> ForwardPass n (f1,f2)
\end{code}
The two passes run interleaved, not sequentially, and each may help
the other,
yielding better results than running $A$~and then~$B$ or $B$~and then~$A$
\citep{lerner-grove-chambers:2002}.
%%  call these passes. ``super-analyses;''
%%  in \hoopl, construction of super-analyses is
%%  particularly concrete.

\subsection{Example: Constant propagation and constant folding} 
  \seclabel{const-prop-client}

\begin{figure}
{\small
\begin{code}
-- Types and definition of the lattice
data `HasConst = `Top | `B Bool | `I Integer
type `ConstFact = Map.Map Reg HasConst
`constLattice = DataflowLattice
  { fact_bot    = Map.empty
  , fact_extend = stdMapJoin constFactAdd }
  where
    `constFactAdd new old = (c, j)
      where j = if new == old then new else Top
            c = if j == old then NoChange else SomeChange

-------------------------------------------------------
-- Analysis: register has constant value
`regHasConst :: FwdTransfer Node ConstFact
regHasConst (LabelNode l)       f = lookupFact f l
regHasConst (Store _ _)         f = f
regHasConst (Assign x (Bool b)) f = Map.insert x (B b) f
regHasConst (Assign x (Int  i)) f = Map.insert x (I i) f
regHasConst (Assign x _)        f = Map.insert x Top   f
regHasConst (Branch l)          f = mkFactBase [(l, f)]
regHasConst (CondBranch (Reg x) tid fid) f
  = mkFactBase [(tid, Map.insert x (B True)  f),
                (fid, Map.insert x (B False) f)]
regHasConst (CondBranch _ tid fid) f 
  = mkFactBase [(tid, f), (fid, f)]

-------------------------------------------------------
-- Constant propagation
`constProp :: FwdRewrite Node ConstFact
constProp node facts
  = fmap toAGraph (mapE rewriteE node)
  where
    rewriteE e(Reg r)
      = case M.lookup r facts of
          Just (B b) -> Just $ Bool b
          Just (I i) -> Just $ Int  i
          _          -> Nothing
    rewriteE e = Nothing

-------------------------------------------------------
-- Simplification ("constant folding")
`simplify :: FwdRewrite Node f
simplify (CondBranch (Bool b) t f) _
  = Just $ toAGraph $ Branch (if b then t else f)
simplify node _ = fmap toAGraph (mapE s_exp node)
  where
    s_exp (Binop Add (Int i1) (Int i2))
       = Just $ Int $ i1 + i2
    ...  -- more cases for constant folding

-- Rewriting expressions
`mapE :: (Expr    -> Maybe Expr) 
      -> Node e x -> Maybe (Node e x)
mapE f (LabelNode _) = Nothing
mapE f (Assign x e)  = fmap (Assign x) $ f e
 ...  -- more cases for rewriting expressions

-------------------------------------------------------
-- Defining the forward dataflow pass
`constPropPass = ForwardPass
   { fp_lattice = constLattice
   , fp_transfer = regHasConst
   , fp_rewrite = constProp `thenFwdRw` simplify } 
\end{code}}
\caption{The client for constant propagation and constant folding} \figlabel{const-prop}
\end{figure}
\figref{const-prop} shows client code for
constant propagation and constant folding.
For each register at each point in a graph, the analysis concludes one of three facts:
the register holds a constant value (Boolean or integer),
the register might hold a non-constant value,
or nothing is known about what the register holds (bottom).
We~represent these facts using a finite map from registers to dataflow facts
(@HasConst@).
A register with a constant value maps to @Just v@, where @v@ is the constant value;
a register with a non-constant value maps to @Just Top@;
and a register with an unknown value maps to @Nothing@ (i.e., it is not
in the domain of the finite map).

% \afterpage{\clearpage}

The definition of the lattice (@constLattice@) is straightforward.
The bottom element is an empty map (nothing is known about the contents of any register).
We use the @stdMapJoin@ function to lift the join operation
for a single register (@constFactAdd@) up to the map containing facts
for all registers.

For the transfer function, @regHasConst@, 
there are two interesting kinds of nodes:
assignment and conditional branch.
In the first two cases for assignment, a register gets a constant value,
so we produce a dataflow fact mapping the register to its value.
In the third case for assignment, the register gets a non-constant value,
so we produce a dataflow fact mapping the register to @Top@.
The last interesting case is a conditional branch where the condition
is a register.
If the conditional branch flows to the true successor,
the register holds @True@, and similarly for the false successor.
We update the fact flowing to each successor accordingly.

We do not need to consider complicated cases such as
an assignment @x:=y@ where @y@ holds a constant value @k@.
Instead, we rely on the interleaving of transformation
and analysis to first transform the assignment to @x:=k@,
which is exactly what our simple transfer function expects.
As we mention in \secref{simple-tx},
interleaving makes it possible to write
the simplest imaginable transfer functions, without missing
opportunities to improve the code.

The rewrite function for constant propagation, @constProp@,
simply rewrites each use of a register to its constant value.
We use the auxiliary function @mapE@
to apply @rewriteE@ to each use of a register in each kind of node;
in turn, @rewriteE@ function checks if the register has a constant
value and makes the substitution.  We assume an auxiliary function
\begin{code}
  toAGraph :: Node e x -> AGraph e x
\end{code}

\figref{const-prop} also gives a completely separate rewrite function 
to perform constant
folding, called @simplify@.  It rewrites a conditional branch on a
boolean constant to an unconditional branch, and
to find constant subexpressions, 
it runs @s_exp@
on every subexpression.
Function @simplify@ does not need to check whether a register holds a
constant value; it relies on @constProp@ to have replaced the
register by the constant.
Indeed, @simplify@ does not consult the
incoming fact at all, and hence is polymorphic in~@f@.



We have written two @FwdRewrite@ functions 
because they are independently useful.  But in this case we
want to apply \emph{both} of them,
so we compose them with @thenFwdRw@.
The composed rewrite functions, along with the lattice and the
transfer function,
go into @constPropPass@ (bottom of \figref{const-prop}).
To improve a particular graph, we pass @constPropPass@ and the graph
to
@analyzeAndRewriteFwd@.


\subsection{Throttling the dataflow engine using ``optimization fuel''}
\seclabel{vpoiso}
\seclabel{fuel}

Debugging an optimization can be tricky:
an optimization may rewrite hundreds of nodes,
and any of those rewrites could be incorrect.
To debug dataflow optimizations, we use Whalley's
\citeyearpar{whalley:isolation} powerful technique
to identify the first rewrite that 
transforms a program from working code to faulty code.

The key idea is to~limit the number of rewrites that
are performed while optimizing a graph.
In \hoopl, the limit is called
\emph{optimization fuel}:
each rewrite costs one unit of fuel,
and when the fuel is exhausted, no more rewrites are permitted.
Because each rewrite leaves the observable behavior of the
program unchanged, it is safe to stop rewriting at any point.
Given a program that fails when compiled with optimization,
a test infrastructure uses binary search on the amount of
optimization fuel, until
it finds that the program works correctly after $n-1$ rewrites but fails
after $n$~rewrites.
The $n$th rewrite is faulty.


You may have noticed that @analyzeAndRewriteFwd@
returns a value in the @FuelMonad@ (\secref{using-hoopl}).
The @FuelMonad@ is a simple state monad maintaining the supply of unused
fuel.  It also holds a supply of fresh labels, which are used by the rewriter
for making new blocks; more precisely, 
\hoopl\ uses these labels to
take the @AGraph@ (\figref{rewrites}) returned by a pass's rewrite
function and converted it to a~@Graph@.


\subsection{Fixed points and speculative rewrites} \seclabel{fixpoints}

Are rewrites sound, especially when there are loops?
Many analyses compute a fixed point starting from unsound
``facts''; for example, a live-variable analysis starts from the
assumption that all variables are dead.  This means \emph{rewrites
performed before a fixed point is reached may be unsound, and their results
must be discarded}.  Each iteration of the fixed-point computation must
start afresh with the original graph.  


Although the rewrites may be unsound, \emph{they must be performed}
(speculatively, and possibly recursively), 
so that the facts downstream of the replacement graphs are as accurate
as possible.
For~example, consider this graph, with entry at @L1@:
\par{\small
\begin{code}
  L1: x=0; goto L2
  L2: x=x+1; if x==10 then goto L3 else goto L2
\end{code}}
The first traversal of block @L2@ starts with the unsound ``fact'' \{x=0\};
but analysis of the block propagates the new fact \{x=1\} to @L2@, which joins the
existing fact to get \{x=$\top$\}.
What if the predicate in the conditional branch were @x<10@ instead
of @x==10@?
Again the first iteration would begin with the tentative fact \{x=0\}.
Using that fact, we would rewrite the conditional branch to an unconditional
branch @goto L3@.  No new fact would propagated to @L2@, and we would
have successfully (and soundly) eliminated the loop.
This example is contrived, but it illustrates that 
for best results we should
\begin{itemize}
\item Perform the rewrites on every iteration.
\item Begin each new iteration with the orignal, virgin graph.
\end{itemize}
This sort of algorithm is hard to implement in an imperative setting, where rewrites
mutate a graph in place.
But  with an immutable graph, implementing the algorithm
is trivially easy: we simply revert to the original graph at the start
of each fixed-point iteration.

\subsection{Correctness} \seclabel{correctness}

Facts computed by @analyzeAndRewriteFwd@ depend on graphs produced by the rewrite
function, which in turn depend on facts computed by the transfer function.
How~do we know this algorithm is sound, or if it terminates?
A~proof requires a POPL paper
\cite{lerner-grove-chambers:2002}, but we can give some
intuition.

\hoopl\ requires that a client's functions meet
 these preconditions:
\begin{itemize}
\item 
The lattice must have no \emph{infinite ascending chains}; that is,
every sequence of calls to @fact_extend@ must eventually return @NoChange@.
\item 
The transfer function must be 
\emph{monotonic}: given a more informative fact in,
it should produce a more informative fact out.
\item 
The rewrite function must be \emph{sound}:
if it replaces a node @n@ by a replacement graph~@g@, then @g@~must be
observationally equivalent to~@n@ under the  
assumptions expressed by the incoming dataflow fact~@f@.
%%\footnote{We do not permit a transformation to change
%%  the @Label@ of a node. We have not found any optimizations
%%  that are prevented (or even affected) by this restriction.}
\item 
The rewrite function must be \emph{consistent} with the transfer function;
that is, \mbox{@transfer n f@ $\sqsubseteq$ @transfer g f@}.
For example, if the analysis says that @x@ is dead before the node~@n@,
then it had better still be dead if @n@ is replaced by @g@.
\item 
To ensure termination, a transformation that uses deep rewriting
must not return replacement graphs which 
contain nodes that could be rewritten indefinitely.
\end{itemize}
Without the conditions on montonicity and consistency,
our algorithm will terminate, 
but there is no guarantee that it will compute
a fixed point of the analysis.  And that in turn threatens the
soundness of rewrites based on possibly bogus ``facts''.

However, when the preconditions above are met,
\begin{itemize} 
\item
The algorithm terminates.  The fixed-point loop must terminate because the 
lattice has no infinite ascending chains. And the client is responsible
for avoiding infinite recursion when deep rewriting is used.
\item 
The algorithm is sound.  Why? Because if each rewrite is sound (in the sense given above), 
then applying a succession of rewrites is also sound.
Moreover, a~sound analysis of the replacement graph
may generate only dataflow facts that could have been
generated by a more complicated analysis of the original graph.
\end{itemize}
% \john{If we haven't sold the audience by now, they're not going
%   to be convinced by this quote.}
%
% Why use such a complex algorithm?
% \ifcutting Because interleaving \else
% \citet{lerner-grove-chambers:2002} write
% \begin{quote}
% \emph{Previous efforts to exploit [the mutually beneficial
% interactions of dataflow analyses] either (1)~iteratively performed
% each individual analysis until no further improvements are discovered
% or (2)~developed [handwritten] ``super-analyses'' that manually
% combine conceptually separate analyses. We have devised a new approach
% that allows analyses to be defined independently while still enabling
% them to be combined automatically and profitably. Our approach avoids
% the loss of precision associated with iterating individual analyses
% and the implementation difficulties of manually writing a
% super-analysis.}
% \end{quote}



%%  \simon{But do we apply rewrites even before the analysis reaches a fixed point?
%%  If so, what property do the rewrites have to satisfy to ensure soundness?
%%  If not, even a single rewrite might destroy the fixed-point property of the
%%  current facts.  Or perhaps we iterate the analysis to a fixed point, and only \emph{then}
%%  do rewriting? If so, do we need the transfer functions at that stage?
%%  
%%  Also the fixed-point of the analysis relies on upward chains. What if
%%  the rewrite pushed it downward?  Or is it the case that a rewrite must
%%  change a node $n$ into a graph $g$ so 
%%  that $\mathit{fwdtrans}(n) \leq \mathit{fwdtrans}(g)$?
%%  
%%  Also the fixed point calculation requires multiple passses; do the 
%%  rewrites then apply multiple times?
%%  
%%  I'm deliberately playing the role of the reader here, and not peeking at
%%  the code.  I don't think it's enough to say ``go look at Chambers paper''; 
%%  I suggest we say enough (half a column would do it) to address the obvious
%%  questions and point to Chambers for details.
%%  
%%  
%%  \textbf{NR}: Good questions, but let's have a forward reference to \secref{dfengine}}

% Interleaving
% \fi
%  analysis with transformation makes it
% possible to implement useful  transformations using startlingly simple
% client code.
% \john{And maybe this is the place to brag that we now have a startlingly simple
% implementation?}

\finalremark{Doesn't the rewrite have to be have the following property:
for a forward analysis/transform, if (rewrite P s) = Just s',
then (transfer P s $\sqsubseteq$ transfer P s').
For backward: if (rewrite Q s) = Just s', then (transfer Q s' $\sqsubseteq$ transfer Q s).
Works for liveness.
``It works for liveness, so it must be true'' (NR).
If this is true, it's worth a QuickCheck property!
}%
\finalremark{Version 2, after further rumination.  Let's define
$\scriptstyle \mathit{rt}(f,s) = \mathit{transform}(f, \mathit{rewrite}(f,s))$.
 Then $\mathit{rt}$ should
be monotonic in~$f$.  We think this is true of liveness, but we are not sure
whether it's just a generally good idea, or whether it's actually a 
precondition for some (as yet unarticulated) property of \ourlib{} to hold.}%

%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixed point. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }




\section{\ourlib's implementation}
\seclabel{engine}
\seclabel{dfengine}

\secref{making-simple}
gives a client's-eye view of \hoopl, showing how to use 
it to create analyses and transformations.
\hoopl's interface is simple, but 
the \emph{implementation} of interleaved analysis and rewriting is 
quite complicated.  \citet{lerner-grove-chambers:2002} 
do not describe their implementation.  We have written
at least three previous implementations, all of which
were long and hard to understand, and only one of which
provided compile-time guarantees about open and closed shapes.
We are not confident that any of these implementations are correct.

In this paper we describe our new implementation.  It is short
(about a third of the size of our last attempt), elegant, and offers
strong static shape guarantees.  The whole thing is about 300~lines of
code, excluding comments; this count includes both forward and backward
dataflow analysis and transformation.

We describe the implementation of \emph{forward} 
analysis and transformation.
The implementations of backward analysis and transformation are
exactly \remark{``exactly''?} analogous and are part of \hoopl.



\subsection{Overview}

We concentrate on implementing @analyzeAndRewriteFwd@, whose
type is in \secref{using-hoopl}.
Its implementation is built on the hierarchy of nodes, blocks, and graphs
described in \secref{graph-rep}.  For each thing in the hierarchy,
we develop a function of this type:
\begin{code}
type `ARF thing n
 = forall f e x. ForwardPass n f
              -> thing e x -> Fact e f 
              -> FuelMonad (RG n e x, Fact x f)
\end{code}
An @ARF@ (short for ``analyze and rewrite forward'') is a combination of
a rewrite and transfer function.
An @ARF@ takes a @ForwardPass@, a @thing@ (a node, block, or graph),
and an input fact,
and it returns a rewritten graph of type @(RG n e x)@ of the same shape
as the @thing@, plus a suitably shaped output fact.
%
%Regardless of whether @thing@ is a node, block, or graph, the result
%is always a graph.
% 
% point is made adequately in the client section ---NR
The type~@RG@ is internal to \hoopl; it is not seen by any client.
We use it, not @Graph@, for two reasons:
\begin{itemize}
\item The client is often interested not only in the facts flowing
out of the graph (which are returned in the @Fact x f@), 
but also in the facts on the \emph{internal} blocks
of the graph. A~replacement graph of type @(RG n e x)@ is decorated with
these internal facts.
\remark{SO IF THE TYPE IS INTERNAL, HOW DOES THE CLIENT GET ITS GRUBBY
PAWS ON THESE FACTS???}
\item A @Graph@ has deliberately restrictive invariants; for example,
a @GMany@ with a @JustO@ is always open at exit (\figref{graph}).  It turns
out to be awkward to maintain these invariants \emph{during} rewriting,
but easy to restore them \emph{after} rewriting by ``normalizing'' an @RG@.
\end{itemize}
The information in an @RG@ is returned to the client by
the normalization function @normalizeBody@, which
splits an @RG@ into a @Body@ and its corresponding @FactBase@:
\begin{code}
`normalizeBody :: Edges n => RG n f C C 
              -> (Body n, FactBase f)
\end{code}
\begin{figure}
\begin{code}
data `RG n f e x where
  `RGNil   :: RG n f a a
  `RGCatO  :: RG n f e O -> RG n f O x -> RG n f e x
  `RGCatC  :: RG n f e C -> RG n f C x -> RG n f e x
  `RGUnit  :: Fact e f  -> Block n e x -> RG n f e x
\end{code}
\caption{The data type \texttt{RG} of rewritten graphs} \figlabel{rg}
\end{figure}
The constructors of @RG@ are given in \figref{rg}.
The essential points are that constructor @RGUnit@ is polymorphic in
the shape of a block, @RGUnit@ carries a fact as
well as a block, and the concatenation constructors record the shapes
of the graphs at the point of concatenation.\remark{Why record the shapes?}


We exploit the type distinctions of nodes, @Block@, @Body@,
and @Graph@ to structure the code into several small pieces, each of which
can be understood independently.  Specifically, we define a layered set of
functions, each of which calls the previous one:
\begin{code}
 arfNode  :: Edges n => ARF n n
 arfBlock :: Edges n => ARF (Block n) n
 arfBody  :: Edges n
          => ForwardPass n f -> Body n -> FactBase f
          -> FuelMonad (RG n f C C, FactBase f)
 arfGraph :: Edges n => ARF (Graph n) n
\end{code} 
\begin{itemize} 
\item 
The @arfNode@ function processes nodes (\secref{arf-node}).
It handles the subtleties of interleaved analysis and rewriting,
and it deals with fuel consumption.  It calls @arfGraph@ to analyze
and transform rewritten graphs.
\item 
Based on @arfNode@ it is extremely easy to write @arfBlock@, which liftes
the analysis and rewriting from nodes to blocks (\secref{arf-block}).
\item
Using @arfBlock@ we define @arfBody@, which analyses and rewrites a
@Body@: that is, a group of closed/closed blocks linked by arbitrary
control flow.
Because a @Body@ is
always closed/closed and does not take shape parameters, function
@arfBody@ is less polymorphic than the others, but its type is what
would be obtained by expanding and specializing the definition of
@ARF@ for a @thing@ which is always closed/closed and is equivalent to
a @Body@.

Function @arfBody@ takes care of fixed points (\secref{arf-body}).
\item 
Based on @arfBody@ it is easy to write @arfGraph@ (\secref{arf-graph}).
\end{itemize}
Given these functions, writing the main analyzer is a simple
matter of matching the external API to the internal functions:
\begin{code}
  analyzeAndRewriteFwd
     :: forall n f. Edges n
     => ForwardPass n f -> Body n -> FactBase f
     -> FuelMonad (Body n, FactBase f)

  analyzeAndRewriteFwd pass body facts
    = do { (rg, _) <- arfBody pass body facts
         ; return (normaliseBody rg) }
\end{code}
 
\subsection{From nodes to blocks} \seclabel{arf-block}
\seclabel{arf-graph}

We begin our explanation with the second task:
writing @arfBlock@, which analyzes and transforms blocks.
\begin{code}
`arfBlock :: Edges n => ARF (Block n) n
arfBlock pass (BUnit node) f 
  = arfNode pass node f
arfBlock pass (BCat b1 b2) f 
  = do { (g1,f1) <- arfBlock pass b1 f  
       ; (g2,f2) <- arfBlock pass b2 f1 
       ; return (g1 `RGCatO` g2, f2) }
\end{code}
The code is delightfully simple.
The @BUnit@ case is implemented by @arfNode@.
The @BCat@ case is implemented by recursively applying @arfBlock@ to the two
sub-blocks, threading the output fact from the first as the 
input to the second.  
Each recursive call produces a rewritten graph;
we concatenate them with @RGCatO@. 

Function @arfGraph@ is equally straightforward:
\begin{code}
`arfGraph :: Edges n => ARF (Graph n) n
arfGraph _    GNil        f = return (RGNil, f)
arfGraph pass (GUnit blk) f = arfBlock pass blk f
arfGraph pass (GMany NothingO body NothingO) f
  = do { (body', fb) <- arfBody pass body f
       ; return (body', fb) }
arfGraph pass (GMany NothingO body (JustO exit)) f
  = do { (body', fb) <- arfBody  pass body f
       ; (exit', fx) <- arfBlock pass exit fb
       ; return (body' `RGCatC` exit', fx) }
 --  ... two more equations for GMany ...
\end{code}
The pattern is the same as for @arfBlock@: thread
facts through the sequence, and concatenate the results.
Because the constructors of type~@RG@ are more polymorphic than those
of @Graph@, type @RG@ can represent
graphs more simply than @Graph@; for example, each element of a
@GMany@ becomes a single @RG@ object, and these @RG@ objects are then 
concatenated to form a single result of type~@RG@.

\subsection{Analyzing and rewriting nodes} \seclabel{arf-node}

Although interleaving analysis with transformation is tricky, we have
succeeded in isolating the algorithm in just two functions,  \remark{Sure hope
this is true!}
@arfNode@ and its backward analog, @arbNode@:
\begin{code}
`arfNode :: Edges n => ARF n n
arfNode pass n f
 = do { mb_g <- withFuel (fp_rewrite pass n f)
      ; case mb_g of
          Nothing -> return (RGUnit f (BUnit n),
                             fp_transfer pass n f)
          Just (FwdRes ag rw) ->
            do { g <- graphOfAGraph ag
               ; let pass' = pass { fp_rewrite = rw }
               ; arfGraph pass' g f } }
\end{code}
The code here is more complicated, 
but still admirably brief.  
Using the @fp_rewrite@ record selector (\figref{api-types}),
we~begin by extracting the
rewriting function from the @ForwardPass@,
and we apply it function to the node~@n@ 
and the  incoming fact~@f@.  

The resulting @Maybe@ is passed to @withFuel@, which 
deals with fuel accounting:
\begin{code}
  `withFuel :: Maybe a -> FuelMonad (Maybe a)
\end{code}
If @withFuel@'s argument is @Nothing@, \emph{or} if we have run out of
optimization fuel (\secref{fuel}), @withFuel@ returns @Nothing@.
Otherwise, @withFuel@ consumes one unit of fuel and returns its
argument (which will be a @Just@).  That is all we need say about fuel.

In the @Nothing@ case, no rewrite takes place---either because the rewrite function
didn't want one or because fuel is exhausted.
We~return a single-node
graph @(RGUnit f (BUnit n))@, decorated with its incoming fact.
We~also apply the transfer function @(fp_transfer pass)@ 
to the incoming fact to produce the outgoing fact.
(Like @fp_rewrite@, @fp_transfer@ is a record selector of @ForwardPass@.)

In the @Just@ case, we receive a replacement
@AGraph@ @ag@ and a new rewrite function~@rw@.
We~convert @ag@ to a @Graph@, using
\par{\small
\begin{code}
`graphOfAGraph :: AGraph n e x -> FuelMonad (Graph n e x)
\end{code}}
and we analyze the resulting @Graph@ with @arfGraph@.  
This analysis uses @pass'@, which contains the original lattice and transfer
function from @pass@, together with the new rewrite function~@rg@.

And that's it!  If~the client wanted deep rewriting, it is
implemented by the call to @arfGraph@;
if the client wanted
shallow rewriting, the rewrite function will have returned
@noFwdRw@ as~@rw@, which is implanted in @pass'@
(\secref{shallow-vs-deep}).

\subsection{Fixed points} \seclabel{arf-body}

Lastly, @arfBody@ deals with the fixed-point calculation.
This part of the implementation is the only really tricky part, and it is
cleanly separated from everything else:
\par{\small
\begin{code}
arfBody  :: Edges n
         => ForwardPass n f -> Body n -> FactBase f
         -> FuelMonad (RG n f C C, FactBase f)
`arfBody pass body fbase
  = fixpoint (fp_lattice pass) (arfBlock pass) fbase $
    forwardBlockList (factBaseLabels fbase) body
\end{code}}
Function @forwardBlockList@ takes a list of possible entry points and @Body@,
 and it
 returns a linear list of
blocks, sorted into an order that makes forward dataflow efficient:
\begin{code}
`forwardBlockList 
  :: Edges n => [Label]
  -> Body n -> [(Label,Block n C C)]
\end{code}
For
example, if the @Body@ starts at block~@L2@, and @L2@ 
branches to~@L1@, but not vice versa, then \hoopl\ will reach a fixed point
more quickly faster if we process @L2@ before~@L1@.  
To~find an efficient order, @forwardBlockList@ uses
the methods of the @Edges@ class---@entryLabel@ and @successors@---to
perform a reverse depth-first traversal of the control-flow graph.
%%
%%The @Edges@ type-class constraint on~@n@ propagates to all the
%%@`arfThing@ functions.
%%  paragraph carrying too much freight
%%
The order of the blocks does not affect the fixed point or any other
part of the answer; it affects only the number of iterations needed to
reach the fixed point.

How do we know what entry points to pass to @forwardBlockList@? 
We treat
any block with an entry in the in-flowing @FactBase@ as an entry point.
\remark{Why does this work?}

The rest of the work is done by @fixpoint@, which is shared by
both forward and backward analyses:
\begin{code}
`fixpoint :: forall n f.
     Edges n
  => Bool     -- going Forward?
  -> DataflowLattice f
  -> (Block n C C -> FactBase f -> 
                FuelMonad (RG n f C C, FactBase f))
  -> FactBase f 
  -> [(Label, Block n C C)]
  -> FuelMonad (RG n f C C, FactBase f)
\end{code}
Except for the mysterious @Bool@ passed as the first argument,
the type signature tells the story.
The third argument is
a function that analyzes and rewrites a single block; 
@fixpoint@ applies that function successively to all the blocks,
which are passed as the fifth argument.\finalremark{For consistency with the transfer
functions, blocks should come before @FactBase@, even though this change will
ugly up the call site some.}
The @fixpoint@ function maintains a
 ``Current @FactBase@''
which grows monotonically:
the initial value of the Current @FactBase@ is the fourth argument to
@fixpoint@,
and the Current @FactBase@ is augmented with the new facts that flow
out of each @Block@ as it is analyze.
The @fixpoint@ functions
keeps analyzing blocks until the Current @FactBase@ reaches a fixed
point.  

The code for @fixpoint@ is a massive 70 lines long;
for completeness, it
appears in Appendix~\ref{app:fixpoint}.  
We highlight
some interesting points here:
\remark{Perhaps we should refrain from saying anything interesting and
simply explain briefly how @fixpoint@ works?}
\begin{itemize}
\item
 Suppose that @forwardBlockList@ has put block @L2@ first, and
that @L2@ branches to @L1@.
After processing @L2@ we will propagate a new fact to @L1@, which might change
the fact currently recorded for @L1@.  However, since we have not yet processed @L1@ in
this iteration, we do not need to record that the @FactBase@ has changed.
We need only run a new iteration if the fact for an \emph{already-processed} block
is changed.
\remark{I~fear a reader may find this point baffling rather than
interesting.  Delete?}
\item
Consider a forward analysis of this @Body@, where execution starts at~@L1@:
\begin{code}
  L1: x:=3; goto L4
  L2: x:=4; goto L4
  L4: if x>3 goto L2 else goto L5
\end{code}
Block @L2@ is unreachable. 
But if we \naively\ process all the blocks (say in 
order @L1@, @L4@, @L2@), then we will start with the bottom fact for @L2@, propagate
\{@x=4@\} to @L4@, where it will join with \{@x=3@\} to yield \{@x=@$\top$\}.  Now the
conditional in @L4@ cannot be rewritten, and @L2@~seems reachable.  We have
lost a good optimization.

It is easy for the fixed-point algorithm to take account of unreachability.
Recall
that the Current @FactBase@ is a finite mapping from @Label@s to
facts.  
The @fixpoint@ function treats any label
in the domain of the Current @FactBase@ as ``currently reachable'',
even if the @FactBase@ maps that
label to $\bot$. Whenever a fact is propagated to a label, the Current
@FactBase@ is augumented, perhaps extending the domain of the @FactBase@,
and implicitly making the block ``reachable''.
To~avoid propagating facts forward from unreachable blocks,
@fixpoint@ 
simply \emph{refrains from analyzing any block~$B$ that is not in the domain of 
the Current @FactBase@}.  If a later block makes $B$ reachable, @fixpoint@ notes
that the Current @FactBase@ has changed, and it will run a new
iteration that will analyze~$B$.
This trick is safe only for a forward analysis, and that is why
@fixpoint@ takes a @Bool@ as its first argument:
if the analysis is \emph{not} forward, @fixpoint@ analyzes every
block, whether it is in the domain of the Current @FactBase@ or not.

The interplay between analysis and reachability is quite subtle, so it is precisely
the sort of thing that one might hope to implement once in a library and
then forget---and that is the case in \hoopl.  Moreover,
the implementation takes only a couple
of lines of code, and better still, unreachable blocks do not
appear in the rewritten graph.
\end{itemize}

\section {Related work} \seclabel{related}

While dataflow analysis and optimization are covered
by a vast literature, 
\emph{design} of optimizers, the topic of this paper, is covered
relatively sparsely.
We therefore focus on the foundations of dataflow analysis,
and on the implementations of comparable dataflow frameworks.

\paragraph{Foundations}

When transfer functions are monotone and lattices are finite in height,
iterative dataflow analysis converges to a fixed point
\cite{kam-ullman:global-iterative-analysis}. 
If~the lattice's join operation distributes over transfer
functions,
this fixed point is equivalent to a join-over-all-paths solution to
the recursive dataflow equations
\cite{kildall:unified-optimization}.\footnote
{Kildall uses meets, not joins.  
Lattice orientation is conventional, and conventions have changed.
We use Dana Scott's
orientation, in which higher elements carry more information.}
\citet{kam-ullman:monotone-flow-analysis} generalize to some
monotone functions.
Each~client of \hoopl\ must guarantee monotonicity,
but for transfer functions that
approximate weakest preconditions or strongest postconditions,
monotonicity falls out naturally.

\ifcutting
\citet{cousot:abstract-interpretation:1977}
\else
\citet{cousot:abstract-interpretation:1977,cousot:systematic-analysis-frameworks}
\fi
introduce abstract interpretation as a technique for developing
lattices for program analysis.
\citet{schmidt:data-flow-analysis-model-checking} shows that
an all-paths dataflow problem can be viewed as model checking an
abstract interpretation.

The soundness of interleaving analysis and transformation,
even when some speculative transformations are not performed on later
iterations, was shown by
\citet{lerner-grove-chambers:2002}.
\ifcutting\else
Muchnick \citeyearpar{muchnick:compiler-implementation} 
presents many examples of both particular analyses and related
algorithms.
\fi

\paragraph{Frameworks}
The Soot framework is designed for analysis and optimization of Java programs
\cite{hendren:soot:2000}.
\john{This citation is probably the best for Soot in general, but there doesn't appear
 to be any formal publication that actually details the dataflow framework part.}
Soot provides a dataflow library that is abstracted over the representation of
the control-flow graph and the representation of instructions.
The interface for defining lattice and analysis functions is similar to our own,
with additional functions required to copy lattice elements (Soot is implemented
in an imperative style) and to invoke the analysis.
But unlike \ourlib, Soot cannot interleave analyses and transformations.

The CIL toolkit \cite{necula:cil:2002}\john{Again, no good citation for same reason}
provides a dataflow framework for optimizing C~programs.
The framework is specialized to a specific control-flow graph
and a specific definition of instructions.
The interface for the client is fairly complicated,
with the evident goal of allowing the client to affect which instructions
the analysis will iterate over.
Like Soot, CIL cannot interleave analyses and transformations.

\john{FYI, LLVM has Pass Managers that try to control the order of passes,
  but I'll be darned if I can find anything that might be termed a dataflow framework.}

The Whirlwind compiler contains the dataflow framework implemented
by \citet{lerner-grove-chambers:2002}.
Their implementation is much like our early efforts:
it is a complicated mix of code that manages the interleaving,
the deep rewriting, and the fixed-point computation.
Our implementation simplifies the problem dramatically
by~separating each of these tasks.

Because speculative transformation is difficult with an imperative implementation,
the implementation in Whirlwind is split into two separate phases.
The first phase runs the interleaved analyses and transformations
to compute the final dataflow facts and a representation of the transformations
that should be applied to the input graph.
Then, a subsequent phase executes the transformations.
With our immutable representation, tentative transformations
can be applied aggressively, and there is no need
for a phase distinction.

\ourlib\ also improves upon Whirlwind's dataflow framework by providing
new support for the optimization-writer:
\begin{itemize}
\item Using static type guarantees, we~rule out a whole
  class of possible bugs: transformations that produced malformed
  control-flow graphs.
\item Using dynamic testing,
  we can isolate the rewrite that transforms a working program
  into a faulty program,
  using Whalley's fault-isolation technique \cite{whalley:isolation}.
\end{itemize}

In our previous work we described a zipper-based representation of control-flow
graphs \cite{ramsey-dias:applicative-flow-graph}, stressing the advantages
of immutability for writing dataflow optimizers.
Our new representation, described in \secref{graph-rep}, is a major improvement:
\begin{itemize}
\item
We can concatenate nodes, blocks, and graphs, in constant time.
Previously, we had to resort to Hughes's
\citeyearpar{hughes:lists-representation:article} technique, representing
a graph as a function.
\item
Most important, errors in concatenation are ruled out at
compile-compile time by Haskell's static
type system.
In~earlier implementations, such errors were not detected until
the compiler~ran, at which point \ourlib\ tried to compensate
for the errors
\ifcutting---but
\else
by inserting branch instructions and then issued a warning message.
But
\fi
the compensation code harbored subtle faults%
\ifcutting\else, which were discovered while developing a new back end
for~GHC\fi. 
\hfuzz=1.6pt % don't want to reword ``errors in concatenation''
\item
There is only one type of node in a control-flow graph,
and we use GADTs to express the differences between
first, middle, and last nodes.
The main benefit is that higher-order functions for manipulating
graphs (e.g. transfer and rewrite functions)
now require only one function argument instead of three.
\end{itemize}

The implementation of \ourlib\ is also a dramatic improvement
over our earlier implementations.
Not only is the code conceptually simpler,
but it is also shorter:
our new implementation is about half as long
as the previous version currently available in the Glasgow Haskell Compiler.

\section{What we have learned}

In conclusion we offer the following lessons from the experience of designing
and implementing \ourlib{}.
\begin{itemize}
\item 
Although we have not stressed this point, there is a close connection
between dataflow analyses and program logic:
\begin{itemize}
\item
A forward dataflow analysis is represented by a predicate transformer
that is related to \emph{strongest postconditions}
\cite{floyd:meaning}.\footnote
{In Floyd's paper the output of the predicate transformer is called
the \emph{strongest verifiable consequent}, not the ``strongest
postcondition.''} 
\item
A backward dataflow analysis is represented by a predicate transformer
that is related to \emph{weakest preconditions} \cite{dijkstra:discipline}.
\end{itemize}
Logicians write down the predicate transformers for the primitive
program fragments, and then use compositional rules to ``lift'' them 
to a logic for whole programs.  In the same way \ourlib{} lets the client
write simple predicate transformers,
and local rewrites based on those assertions, and ``lifts'' them to entire
function bodies with arbitrary control flow.

\item Designing good abstractions (data type and functions over those data types,
which is what \ourlib{} is) is very challenging. Even in our exposition we 
have devoted nearly seven pages to the library API, and only a third of that
to the implementation.  Moreover we have been through many, many variants
of the API.  That said, Haskell's type system (polymorphism, GADTs, higher 
order functions, type classes, and so on) is a remarkably effective design
language.  It took us ages to get the types right, but once we did the
code wrote itself.  

\item Here are some examples of how the types helped us:
\begin{itemize}
\item Making \ourlib{} polymorphic in the nodes 
made the code simpler, easier to understand, and easier to maintain.
In particular, it forced us to make explicit \emph{exactly} what
\ourlib\ must know about nodes, and embody that knowledge in the @Edges@ class (\seclabel{edges}).

\item We are particularly proud of our use of GADTs to statically
track the open/closed status of nodes, blocks, and graphs.  It may
seem like a small thing, but it helped tremendously when building
the Hoopl implementation, and we expect it to help clients. 
Moreover, the implementation is faster than it would otherwise be,
because, say, a @(Fact O f)e@ is known to be just an @f@ rather than
being a sum type that must be tested (with a statically known outcome!).

\item The similar shapes of nodes, blocks, and graphs consistently helped our
thinking, and helped to structure the implementation.
\end{itemize}

\item Writing a dataflow optimizer for an imperative language
in a functional language is a huge win.  We absolutely could not
have conceived \ourlib{} in C++.  Partly that is to do with 
confidence that a data structure will not be unexpectedly mutated,
but it's also that functional programming forces you to be 
explicit about every input and output.  Indeed, we have invented
several improvements to the already-tricky Lerner/Grove/Chambers idea,
including:
optimization fuel in \secref{fuel}; 
the per-function control of shallow vs deep rewriting 
\secref{shallow-vs-deep};
the combinators of \secref{combinators}; and the fixed-point improvements 
in \secref{arf-body}). These improvements are implemented in separate
parts of the code, and do not interact with each other.
\end{itemize}
In our earlier designs, graphs were parameterised over three node
types, first, middle, and last nodes.  That had the knock-on effect of
three transfer functions, three rewrite functions, and so on.  It was
a major breakthrough when we found we could use just one node type,
one transfer function, and one rewrite function.  This step in turn
depended crucially on the availablity of the type-level function for
@Fact@ (\figref{api-types}), so that we could express dependency
of the facts on the open/closed-ness of the node.

An ancestor of \ourlib{} is in the Glasgow Haskell comiler already.
We are now engaged in pushing this vastly improved version into GHC.

% 
% \simon{I think this section has interesting material, but it's
% incomprehensible at the beginning, because our readers won't have the 
% mental scaffolding to grok it.  Norman, have at it!}
% 
% \john{Is it just me, or have we lost all the text claiming that analyses should
%   just be strongest postconds/ weakest preconds? Is this by design?}
% \simon{I did remove some material of that sort.  I think I did so because it didn't
% seem (to me) to contribute much at the place it then appeared. But I am entirely open
% to resurrecting it if you think that would help.  Perhaps in the section on 
% correctness, which follows next?  Can you do that?}
% 
% One story of this paper is that we can think similar thoughts about
% assembly code, IR nodes, basic blocks, and control-flow graphs.
% Morever these thoughts are reflected in the types.
% Observations:
% \begin{itemize}
% \item
% In compilation and optimization, things are always easiest to analyze
% when there is a single entry and single exit.  (Lesson of structured
% programming.)
% \item
% In assembly code, we introduce labels (not present in the hardware) to
% be able to code what we mean by branch targets.
% \item
% In a control-flow graph, unlike in assembly code, \emph{every} edge
% out of a control-transfer instruction is associated with a label.
% This invariant simplifies the representation and gives us freedom to
% lay out assembly code in an order that, e.g., is good for the I-cache,
% or that minimizes branch instructions on hot paths.  (Foreshadow that
% blocks in our @Graph@ body are unordered.
% \item
% We distinguish labels, computational instructions, control transfers.
% They are distinguished by the number of predecessors and successors.
% We~express this distinction in Haskell by attaching to each
% entry and exit point one of two types: \emph{open} means there is exactly
% one predecessor or successor, and control can ``fall through;''
% \emph{closed} means that control cannot fall through, but can be
% transferred only by an explicit branch---but the \emph{number} of
% successors or predecessors is not constrained \emph{a~priori}.
% \item
% The open/closed concepts lifts to sequences of straight-line code and
% to full control-flow graphs.
% As~a special case, a sequence of straight-line code that is closed at
% both ends---beginning with a label and ending with a control
% transfer---is the familiar \emph{basic block}.
% \item
% By making open/closed part of the types, we can ensure that a client
% of \ourlib{} builds only sequences and graphs that respect our
% invariants. 
% Moreover, the types provide guidance (really???)
% \end{itemize}
% 
% 
% A second story of this paper is that we can think of dataflow analysis
% as a specialization of program logic.
% \begin{itemize}
% A~client of \ourlib{} defines a type of node, a type of predicate
% (aka dataflow fact), and a function which, given a node, returns a
% predicate transformer.
% The beauty of our approach is that we can \emph{lift} a client's
% predicate transformer from nodes into sequences and control-flow
% graphs.
% Moreover, with help from the client, we can iterate the transformer
% over an entire function body
% until it reaches a fixed point, the result of which is a dataflow
% analysis.
% 
% In addition, we can do Lerner/Grove/Chambers.
% 
% Finally, our implementation has some especially wonderful Haskell
% (Simon please flesh out these bits):
% \begin{itemize}
% \item
% Our open/closed tags aren't just phantom types.  By virtue of GADTs,
% they are real. 
% And we get exhaustiveness checking, etc --- the works.
% \item
% There's more!  By using higher-rank polymorphism, we can define a
% \emph{single} kind of widget, then instantiate it at nodes, blocks,
% graphs.
% SO WHAT?  This is wonderful because\ldots
% \item
% More wonderfulness?
% \end{itemize}
% 
% 
% 
% \section{Conclusions}
% 
% \john{Lots of obsolete claims here: continuations, monad, separate node types,
% HavingSuccessors.}
% Compiler textbooks make dataflow optimization appear
% difficult and complicated.
% In~this paper, we show how to engineer a library, \ourlib, which makes
% it easy to build analyses and transformations based on dataflow.
% \ourlib\ makes dataflow simple not by using a single magic
% ingredient, but by applying ideas that are well understood by 
% the programming-language community.
% \begin{itemize}
% \item
% We acknowledge only one program-analysis technique: the solution of
% recursion equations over assertions.
% %Like our colleagues working in imperative languages, 
% We solve the equations by iterating to a fixed point.
% % Many equations relate
% % properties of program states; some relate properties of paths through
% % programs. 
% \item
% We consider only two
% ways of relating assertions: weakest liberal precondition and strongest 
% postcondition, which
%  correspond 
% %\ifpagetuning\else
% %respectively
% %\fi
% to
% \ifcutting
% ``backward'' and ``forward'' dataflow
% problems.
% \else
% ``backward dataflow problems'' and ``forward dataflow
% problems.''
% \fi
% \finalremark
% {Can we give an example of a property of program states which is
% neither, just by way of contrast; ie this we cannot do.}
% %%  \item
% %%  In a language that admits loops, iterating weakest preconditions or
% %%  strongest postconditions typically does not reach a fixed point in
% %%  finitely many steps; hence the need for loop invariants
% %%  \cite{hoare:axiomatic-basis,dijkstra:discipline,gries:science-programming}.
% %%  In \secref{logic-reconciled}, we show that we can guarantee to reach a
% %%  fixed point by limiting what we can express in the logic.\remark{needs
% %%  a fix}
% %%  We~show that many classic analyses can be explained this way;
% %%  the great diversity of classic analyses corresponds to a great
% %%  diversity of inexpressive logics.
% %%  This view leads us to a unifying principle:
% %%  \emph{To implement a code-improving transformation, find the least
% %%    expressive logic that can justify the transformation, then use that
% %%    logic to compute strongest postconditions, which justify the
% %%    transformation locally.}
% \item
% Although our implementation allows graph nodes to be rewritten in any
% way that preserves semantics, we describe
% three program-transformation techniques:
% substitution of equals for equals, 
% insertion of assignments to unobserved variables, 
% and removal of assignments to unobserved variables
% (\secref{example:transforms}).
% Substitution of equals for equals is often justified by properties of program
% states; for example, if variable~$x$
% is always~7, we may substitute~7 for~$x$.\finalremark
% {We can also justify substitution of \emph{labels} in goto
%   statements by reasoning about continuations.  This is
%   probably not the place to mention this fact.}
% Insertion and removal of assignments are often justified by properties
% of paths through programs;
% for example, if an assignment's continuation does not use the variable
% assigned~to, that assignment may be removed.
% 
% %%  Some compiler texts treat the removal of unreachable code as a
% %%  code-improving transformation in its own right.
% %%  In~our framework, unreachable code becomes unreachable in the
% %%  garbage-collection sense, so no special effort is required to remove
% %%  it.
% \item
% Complex program transformations should be composed from simple
% transformations. 
% For example, both ``code motion'' and ``induction-variable
% elimination'' can be implemented in three stages: insert new assignments;
% substitute equals for equals; remove unneeded assignments
% (\secref{induction-var-elim}). 
% 
% \item 
% Because each rewrite leaves the semantics
% of the program unchanged, 
% we can use 
% ``optimization fuel'' to limit the number of rewrites.
%  When we isolate a fault 
% \ifcutting\else in the optimizer \fi
% (\secref{vpoiso}), we 
% \ifcutting have to debug just \else therefore have the luxury of debugging \fi
%  a single
%  rewrite, not a complex transformation.
% \end{itemize}
% 
% We also build on proven implementation techniques
% in a way that
% makes it easy
% to implement classic code improvements.
% \begin{itemize}
% \item
% We use the algorithm of \citet{lerner-grove-chambers:2002} to 
% compose analyses and transformations.
% This~algorithm makes it easy to compose complex transformations
% from simple ones.
% 
% Using continuation-passing style and generalized algebraic data types,
% we have created a new implementation%
% \ifcutting\else\ of the algorithm\fi, 
% which works by 
% composing three relatively simple functions
% (\secref{forward-iterator}). 
% The functions are simple
% because the static type of a node constrains the number of predecessors
% and successors it may have.
% And because we can
% compare our code with a standard continuation semantics, we have more
% confidence in this new implementation than in 
% any previous implementation. 
% \item
% Our code is pure.
% Inspired by Huet's~\citeyearpar{huet:zipper} zipper,
% we use an applicative representation of
% control-flow graphs
% \cite{ramsey-dias:applicative-flow-graph}. 
% We~improve on our prior work by storing changing dataflow facts
% in an explicit dataflow monad,
% which
% makes it especially easy to implement such
% operations as sub-analysis of a replacement graph
% (\secref{dataflow-monad});
% by using static types to guarantee that each replacement graph can be
% spliced in place of the node it replaces
% (\secreftwo{subgraphs}{rewrite-functions});
% and by simplifying our implementation using continuation-passing style
% (\secreftwo{forward-iterator}{forward-actualizer}). 
% %
% % important, but no longer mentioned in this paper:
% %
% %%  \item
% %%  To \emph{construct} programs, we use a different representation of
% %%  flow graphs, one which hides the complexity of the zipper and which
% %%  provides a constant-time operation for joining flow graphs in
% %%  sequence.
% %%  It is inspired in part by Hughes's \citeyearpar{hughes:novel-lists}
% %%  representation of lists, which supports a constant-time append operation.
% \item
% \ourlib\ is polymorphic in the
% representations of 
% assignments and control-flow operations.
% %%  Although our polymorphic representations have been instantiated only
% %%  with the low-level intermediate code used by the Glasgow Haskell
% %%  Compiler, they are intended eventually to be instantiated with
% %%  machine-dependent representations of target-machine instructions, as
% %%  part of a larger project of refactoring GHC's back ends.
% %
% %This design seems obvious in retrospect,
% %but we underestimated the degree to which polymorphism would force us to
% %separate concerns.
% %Introducing polymorphism has made the code simpler, easier
% %to understand, and easier to maintain.
% By forcing us to separate concerns, introducing polymorphism
% made the code simpler, easier to understand, and easier to maintain.
% \finalremark
% {SLPJ: Is it possible to substantiate this claim by [more] examples?}
% In particular, it forced us to make explicit \emph{exactly} what
% \ourlib\ 
%  must know about flow-graph nodes:
% it must be able to find
% targets of control-flow operations (constraint
% @HavingSuccessors l@, \secref{zdfSolveFwd}).
% \end{itemize}
% %
% % gen and kill are history
% %
% %%\item
% %%Judicious use of Haskell type classes makes is possible to write
% %%weakest precondition or strongest postcondition using the ``transfer
% %%equations'' that are familiar from compiler textbooks.
% %%If you like, you can even write overloaded @gen@ and @kill@ functions.
% %%The benefit is that it is easy to compare the actual code with the
% %%abstract treatments found in textbooks\ifgenkill \ (\secref{gen-kill})\fi.
% Using \ourlib,
% you can create a new code improvement in three steps:
% create a lattice representation for the assertions you want to
% express;
% create transfer functions that approximate weakest preconditions or
% strongest postconditions;
% and 
% create rewrite functions that use your assertions to justify
% program transformations.  
% You can get quickly to the real 
% intellectual work of code improvement: identifying interesting
% transformations and the assertions that justify them.
% 
% \finalremark{Don't forget acknowledgements!!!
% Microsoft, Intel, NSF


\makeatother

\providecommand\includeftpref{\relax} %% total bafflement -- workaround
\IfFileExists{nrbib.tex}{\bibliography{cs,ramsey}}{\bibliography{cs,ramsey,simon,jd}}
\bibliographystyle{plainnatx}


\clearpage

\appendix

\section{Code for \texttt{fixpoint}}  \label{app:fixpoint}

\begin{smallcode}
data TxFactBase n f
  = TxFB { tfb_fbase :: FactBase f
         , tfb_rg  :: RG n f C C -- Transformed blocks
         , tfb_cha   :: ChangeFlag
         , tfb_lbls  :: LabelSet }
 -- Note [TxFactBase change flag]
 -- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 -- Set the tfb_cha flag iff 
 --   (a) the fact in tfb_fbase for or a block L changes
 --   (b) L is in tfb_lbls.
 -- The tfb_lbls are all Labels of the *original* 
 -- (not transformed) blocks

updateFact :: DataflowLattice f -> LabelSet -> (Label, f)
           -> (ChangeFlag, FactBase f) 
           -> (ChangeFlag, FactBase f)
-- See Note [TxFactBase change flag]
updateFact lat lbls (lbl, new_fact) (cha, fbase)
  | NoChange <- cha2        = (cha,        fbase)
  | lbl `elemLabelSet` lbls = (SomeChange, new_fbase)
  | otherwise               = (cha,        new_fbase)
  where
    (cha2, res_fact) 
       = case lookupFact fbase lbl of
           Nothing -> (SomeChange, new_fact)  -- Note [Unreachable blocks]
           Just old_fact -> fact_extend lat old_fact new_fact
    new_fbase = extendFactBase fbase lbl res_fact

fixpoint :: forall n f. Edges n
         => Bool	-- Going forwards?
         -> DataflowLattice f
         -> (Block n C C -> FactBase f
              -> FuelMonad (RG n f C C, FactBase f))
         -> FactBase f -> [(Label, Block n C C)]
         -> FuelMonad (RG n f C C, FactBase f)
fixpoint is_fwd lat do_block init_fbase blocks
  = do { fuel <- getFuel  
       ; tx_fb <- loop fuel init_fbase
       ; return (tfb_rg tx_fb, 
                 tfb_fbase tx_fb `delFromFactBase` blocks) }
	     -- The successors of the Graph are the the Labels for which
	     -- we have facts, that are *not* in the blocks of the graph
  where
    tx_blocks :: [(Label, Block n C C)] 
              -> TxFactBase n f -> FuelMonad (TxFactBase n f)
    tx_blocks []             tx_fb = return tx_fb
    tx_blocks ((lbl,blk):bs) tx_fb = do { tx_fb1 <- tx_block lbl blk tx_fb
                                        ; tx_blocks bs tx_fb1 }

    tx_block :: Label -> Block n C C 
             -> TxFactBase n f -> FuelMonad (TxFactBase n f)
    tx_block lbl blk tx_fb@(TxFB { tfb_fbase = fbase, tfb_lbls = lbls
                                 , tfb_rg = blks, tfb_cha = cha })
      | is_fwd && lbl `elemFactBase` fbase 
      = return tx_fb	-- Note [Unreachable blocks]
      | otherwise
      = do { (rg, out_facts) <- do_block blk fbase
           ; let (cha',fbase') 
                   = foldr (updateFact lat lbls) (cha,fbase) 
                           (factBaseList out_facts)
           ; return (TxFB { tfb_lbls  = extendLabelSet lbls lbl
                          , tfb_rg  = rg `RGCatC` blks
                          , tfb_fbase = fbase', tfb_cha = cha' }) }

    loop :: Fuel -> FactBase f -> FuelMonad (TxFactBase n f)
    loop fuel fbase 
      = do { let init_tx_fb = TxFB { tfb_fbase = fbase
                                   , tfb_cha   = NoChange
                                   , tfb_rg  = RGNil
                                   , tfb_lbls  = emptyLabelSet }
           ; tx_fb <- tx_blocks blocks init_tx_fb
           ; case tfb_cha tx_fb of
               NoChange   -> return tx_fb
               SomeChange -> do { setFuel fuel
                                ; loop fuel (tfb_fbase tx_fb) } }
\end{smallcode}

\section{Index of defined identifiers}

This appendix lists every nontrivial identifier used in the body of
the paper.  
For each identifier, we list the page on which that identifier is
defined or discussed---or when appropriate, the figure (with line
number where possible).
For those few identifiers not defined or discussed in text, we give
the type signature and the page on which the identifier is first
referred to.

Some identifiers used in the text are defined in the Haskell Prelude;
for those readers less familiar with Haskell, these identifiers are
listed in Appendix~\ref{sec:prelude}.

\newcommand\dropit[3][]{}

\newcommand\hsprelude[2]{\noindent
  \texttt{#1} defined in the Haskell Prelude\\}
\let\hsprelude\dropit

\newcommand\hspagedef[3][]{\noindent
  \texttt{#2} defined on page~\pageref{#3}.\\}
\newcommand\omithspagedef[3][]{\noindent
  \texttt{#2} not shown (but see page~\pageref{#3}).\\}
\newcommand\omithsfigdef[3][]{\noindent
  \texttt{#2} not shown (but see Figure~\ref{#3} on page~\pageref{#3}).\\}
\newcommand\hsfigdef[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} defined in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hstabdef[3][]{%
  \noindent
  \ifx!#1!
    \texttt{#2} defined in Table~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Table~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hspagedefll[3][]{\noindent
  \texttt{#2} {let}- or $\lambda$-bound on page~\pageref{#3}.\\}
\newcommand\hsfigdefll[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} {let}- or $\lambda$-bound in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} {let}- or $\lambda$-bound on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    

\newcommand\nothspagedef[3][]{\notdefd\ndpage{#1}{#2}{#3}}
\newcommand\nothsfigdef[3][]{\notdefd\ndfig{#1}{#2}{#3}}
\newcommand\nothslinedef[3][]{\notdefd\ndline{#1}{#2}{#3}}

\newcommand\ndpage[3]{\texttt{#2}~(p\pageref{#3})}
\newcommand\ndfig[3]{\texttt{#2}~(Fig~\ref{#3},~p\pageref{#3})}
\newcommand\ndline[3]{%
  \ifx!#1!%
      \ndfig{#1}{#2}{#3}%
  \else
      \texttt{#2}~(Fig~\ref{#3}, line~\lineref{#1}, p\pageref{#3})%
  \fi
}

\newif\ifundefinedsection\undefinedsectionfalse

\newcommand\notdefd[4]{%
  \ifundefinedsection
    , #1{#2}{#3}{#4}%
  \else
    \undefinedsectiontrue
    \par
    \section{Undefined identifiers}
    #1{#2}{#3}{#4}%
  \fi
}

\begingroup
\raggedright

\input{defuse}%
\ifundefinedsection.\fi

\undefinedsectionfalse


\renewcommand\hsprelude[2]{\noindent
  \ifundefinedsection
    , \texttt{#1}%
  \else
    \undefinedsectiontrue
    \par
    \section{Identifiers defined in Haskell Prelude}\label{sec:prelude}
    \texttt{#1}%
  \fi
}
\let\hspagedef\dropit
\let\omithspagedef\dropit
\let\omithsfigdef\dropit
\let\hsfigdef\dropit
\let\hstabdef\dropit
\let\hspagedefll\dropit
\let\hsfigdefll\dropit
\let\nothspagedef\dropit
\let\nothsfigdef\dropit
\let\nothslinedef\dropit

\input{defuse}
\ifundefinedsection.\fi



\endgroup


\iffalse

\section{Dataflow-engine functions}


\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward iterator}
\end{figure*}

\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward actualizer}
\end{figure*}


\fi



\end{document}



% Old captions' text:
% The dataflow fact for the available-reload analysis describes
%   the set of registers for which a reload is available.
%   We list the types of the functions that manipuate sets of available registers,
%   as well as the definition of the lattice.
% The standard gen and kill functions for available expressions
% The transfer functions for the available-reloads analysis.
% Running the available-reloads analysis and extracting the results with \texttt{zdfFpFacts}
% The rewrite functions to insert redundant reloads immediately before uses

% Probably no space for the implementations:
% interAvail (UniverseMinus s) (UniverseMinus s') =
%   UniverseMinus (s `plusVarSet`  s')
% interAvail (AvailVars     s) (AvailVars     s') =
%   AvailVars (s `timesVarSet` s')
% interAvail (AvailVars     s) (UniverseMinus s') =
%   AvailVars (s  `minusVarSet` s')
% interAvail (UniverseMinus s) (AvailVars     s') =
%   AvailVars (s' `minusVarSet` s )
% 
% smallerAvail (AvailVars _) (UniverseMinus _) = True
% smallerAvail (UniverseMinus _) (AvailVars _) = False
% smallerAvail (AvailVars     s) (AvailVars    s')  =
%   sizeVarSet s < sizeVarSet s'
% smallerAvail (UniverseMinus s) (UniverseMinus s') =
%   sizeVarSet s > sizeVarSet s'
% 
% extendAvail (UniverseMinus s) r =
%   UniverseMinus (deleteFromVarSet s r)
% extendAvail (AvailVars     s) r =
%   AvailVars (extendVarSet s r)
% 
% delFromAvail (UniverseMinus s) r =
%   UniverseMinus (extendVarSet s r)
% delFromAvail (AvailVars     s) r =
%   AvailVars (deleteFromVarSet s r)
% 
% elemAvail (UniverseMinus s) r =
%   not $ elemVarSet r s
% elemAvail (AvailVars     s) r =
%   elemVarSet r s



THE FUEL PROBLEM:


Here is the problem:

  A graph has an entry sequence, a body, and an exit sequence.
  Correctly computing facts on and flowing out of the body requires
  iteration; computation on the entry and exit sequences do not, since
  each is connected to the body by exactly one flow edge.

  The problem is to provide the correct fuel supply to the combined
  analysis/rewrite (iterator) functions, so that speculative rewriting
  is limited by the fuel supply.

  I will number iterations from 1 and name the fuel supplies as
  follows:

     f_pre      fuel remaining before analysis/rewriting starts
     f_0        fuel remaining after analysis/rewriting of the entry sequence
     f_i, i>0   fuel remaining after iteration i of the body
     f_post     fuel remaining after analysis/rewriting of the exit sequence

  The issue here is that only the last iteration of the body 'counts'.
  To formalize, I will name fuel consumed:

     C_pre      fuel consumed by speculative rewrites in entry sequence
     C_i        fuel consumed by speculative rewrites in iteration i of body
     C_post     fuel consumed by speculative rewrites in exit sequence

  These quantities should be related as follows:

     f_0    = f_pre - C_pref
     f_i    = f_0 - C_i            where i > 0
     f_post = f_n - C_post         where iteration converges after n steps

When the fuel supply is passed explicitly as parameter and result, it
is fairly easy to see how to keep reusing f_0 at every iteration, then
extract f_n for use before the exit sequence.  It is not obvious to me
how to do it cleanly using the fuel monad.


Norman
