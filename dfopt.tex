%%  advances over prior work
%%  
%%  (if open/closed) -- detect bad splices at compile time
%%  constant-time access to exit sequence
%%  (indeed exit  node)
%%  
%%  directly splicable in constant amortized time (vs Hughes
%%  technique)
%%  
%%  polymorphic in node type (type classes)
%%  
%%  



\iffalse % submission abstract

We present Hoopl, a Haskell library that makes it easy for compiler
writers to implement program transformations based on dataflow
analyses. The compiler writer must identify (a) logical assertions on
which the transformation will be based; (b) a representation of such
assertions, which should form a lattice of finite height; (c) transfer
functions that approximate weakest preconditions or strongest
postconditions over the assertions; and (d) rewrite functions whose
soundness is justified by the assertions. Hoopl uses the algorithm of
Lerner, Grove, and Chambers (2002), which can compose very simple
analyses and transformations in a way that achieves the same precision
as complex, handwritten "superanalyses." Hoopl will be the workhorse
of a new back end for the Glasgow Haskell Compiler.

Because the major claim in the paper is that Hoopl makes it easy to
implement program transformations, the paper is filled with examples,
which are written in Haskell.  The paper also sketches the
implementation of Hoopl, including some excerpts from the
implementation. 

\fi


\IfFileExists{timestamp.tex}{\input{dfoptdu.tex}}{}
     % If you are Simon, run un-preprocessed code
     % It not Simon, use version with def-use information

\newif\ifpagetuning \pagetuningtrue  % adjust page breaks

\newif\ifnoauthornotes \noauthornotesfalse
\newif\iftimestamp\timestamptrue  % show MD5 stamp of paper

\timestamptrue % it's submission time

\IfFileExists{timestamp.tex}{}{\timestampfalse}

\newif\ifcutting \cuttingfalse % cutting down to submission size


\newif\ifgenkill\genkillfalse  % have a section on gen and kill
\genkillfalse


\newif\ifnotesinmargin \notesinmarginfalse
\IfFileExists{notesinmargin.tex}{\notesinmargintrue}{\relax}

\documentclass[blockstyle,preprint,natbib,nocopyrightspace]{sigplanconf}

\newcommand\ourlib{Hoopl}
   % higher-order optimization library
   % ('Hoople' was taken -- see hoople.org)
\let\hoopl\ourlib

% l2h substitution ourlib Hoopl
% l2h substitution hoopl Hoopl

\newcommand\fs{\ensuremath{\mathit{fs}}} % dataflow facts, possibly plural

\newcommand\vfilbreak[1][\baselineskip]{%
  \vskip 0pt plus #1 \penalty -200 \vskip 0pt plus -#1 }

\usepackage{alltt}
\usepackage{array}
\newcommand\lbr{\char`\{}
\newcommand\rbr{\char`\}}
 
\clubpenalty=10000
\widowpenalty=10000

\usepackage{verbatim} % allows to define \begin{smallcode}
\newenvironment{smallcode}{\par\unskip\small\verbatim}{\endverbatim}

\newcommand\lineref[1]{line~\ref{line:#1}}
\newcommand\linepairref[2]{lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\linerangeref[2]{\mbox{lines~\ref{line:#1}--\ref{line:#2}}}
\newcommand\Lineref[1]{Line~\ref{line:#1}}
\newcommand\Linepairref[2]{Lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\Linerangeref[2]{\mbox{Lines~\ref{line:#1}--\ref{line:#2}}}

\makeatletter

\let\c@table=
           \c@figure % one counter for tables and figures, please

\newcommand\setlabel[1]{%
  \setlabel@#1!!\@endsetlabel
}
\def\setlabel@#1!#2!#3\@endsetlabel{%
  \ifx*#1*% line begins with label or is empty
     \ifx*#2*% line is empty
        \verbatim@line{}%
     \else
       \@stripbangs#3\@endsetlabel%
       \label{line:#2}%
     \fi
  \else
     \@stripbangs#1!#2!#3\@endsetlabel%
  \fi
}
\def\@stripbangs#1!!\@endsetlabel{%
  \verbatim@line{#1}%
}


\verbatim@line{hello mama}

\newcommand{\numberedcodebackspace}{0.5\baselineskip}

\newcounter{codeline}
\newenvironment{numberedcode}
  {\endgraf
     \def\verbatim@processline{%
        \noindent
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
               %{\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\phantom{: \,}}}%
            \else
               \refstepcounter{codeline}%
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\llap{\arabic{codeline}}: \,}}%
            \fi
        \expandafter\setlabel\expandafter{\the\verbatim@line}%
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
          \vspace*{-\numberedcodebackspace}\par%
        \else
          \the\verbatim@line\par
        \fi}%
   \verbatim
   }
   {\endverbatim}

\makeatother

\newcommand\arrow{\rightarrow}

\newcommand\join{\sqcup}
\newcommand\slotof[1]{\ensuremath{s_{#1}}}
\newcommand\tempof[1]{\ensuremath{t_{#1}}}
\let\tempOf=\tempof
\let\slotOf=\slotof

\makeatletter
\newcommand{\nrmono}[1]{%
  {\@tempdima = \fontdimen2\font\relax
   \texttt{\spaceskip = 1.1\@tempdima #1}}}
\makeatother

\usepackage{times}  % denser fonts
\renewcommand{\ttdefault}{aett} % \texttt that goes better with times fonts
\usepackage{enumerate}
\usepackage{url}
\usepackage{graphicx}
\usepackage{natbib}  % redundant for Simon
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar

\usepackage[ps2pdf,bookmarksopen,breaklinks,pdftitle=dataflow-made-simple]{hyperref}

\newcommand\naive{na\"\i ve}
\newcommand\Naive{Na\"\i ve}

\usepackage{amsfonts}
\newcommand\naturals{\ensuremath{\mathbb{N}}}
\newcommand\true{\ensuremath{\mathbf{true}}}
\newcommand\implies{\supseteq}  % could use \Rightarrow?

\newcommand\PAL{\mbox{C{\texttt{-{}-}}}}
\newcommand\high[1]{\mbox{\fboxsep=1pt \smash{\fbox{\vrule height 6pt
   depth 0pt width 0pt \leavevmode \kern 1pt #1}}}}

\usepackage{tabularx}

%%
%% 2009/05/10: removed 'float' package because it breaks multiple
%% \caption's per {figure} environment.   ---NR
%%
%%  % Put figures in boxes --- WHY??? --NR
%%  \usepackage{float}
%%  \floatstyle{boxed}
%%  \restylefloat{figure}
%%  \restylefloat{table}



% ON LINE THREE, set \noauthornotestrue to suppress notes (or not)

%\newcommand{\qed}{QED}
\ifnotesinmargin
  \long\def\authornote#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}
\else
  % Simon: please set \notesinmarginfalse on the first line
  \long\def\authornote#1{{\em #1\/}}
\fi
\ifnoauthornotes
  \def\authornote#1{\unskip\relax}
\fi

\newcommand{\simon}[1]{\authornote{SLPJ: #1}}
\newcommand{\norman}[1]{\authornote{NR: #1}}
\let\remark\norman
\def\finalremark#1{\relax}
% \let \finalremark \remark % uncomment after submission
\newcommand{\john}[1]{\authornote{JD: #1}}
\newcommand{\todo}[1]{\textbf{To~do:} \emph{#1}}
\newcommand\delendum[1]{\relax\ifvmode\else\unskip\fi\relax}

\newcommand\secref[1]{Section~\ref{sec:#1}}
\newcommand\secreftwo[2]{Sections \ref{sec:#1}~and~\ref{sec:#2}}
\newcommand\seclabel[1]{\label{sec:#1}}

\newcommand\figref[1]{Figure~\ref{fig:#1}}
\newcommand\figreftwo[2]{Figures \ref{fig:#1}~and~\ref{fig:#2}}
\newcommand\figlabel[1]{\label{fig:#1}}

\newcommand\tabref[1]{Table~\ref{tab:#1}}
\newcommand\tablabel[1]{\label{tab:#1}}


\newcommand{\CPS}{\textbf{StkMan}}    % Not sure what to call it.


\usepackage{code}   % At-sign notation

\iftimestamp
\input{timestamp}
\preprintfooter{\mdfivestamp}
\fi

\hyphenation{there-by}

\begin{document}
%\title{\ourlib: Dataflow Optimization Made Simple}
\title{Implementing Dataflow Analysis and Optimization by Lifting Node Functions to Basic Blocks and Control-Flow Graphs}
%\subtitle{\today}

%\titlebanner{\textsf{\mdseries\itshape Submitted to the 2010 ACM Symposium on Principles
%    of Programming Languages (POPL)}}

\ifnoauthornotes
\makeatletter
\let\HyPsd@Warning=
                \@gobble
\makeatother
\fi

% João


\authorinfo{Norman Ramsey}{Tufts University}{nr@cs.tufts.edu}
\authorinfo{Jo\~ao Dias}{Tufts University}{dias@cs.tufts.edu}
\authorinfo{Simon Peyton Jones}{Microsoft Research}{simonpj@microsoft.com}


\maketitle
 
\begin{abstract}
\remark{Needs a complete rewrite}\simon{It's not so bad, is
it?}\remark{It's a fine abstract, just for the wrong paper :-)}
We present \ourlib, a Haskell library that makes it easy for compiler writers
to implement program transformations based on dataflow analyses.
The compiler writer must identify (a)~logical assertions
on which the transformation will be based;
(b)~a~representation of such assertions, which
\ifcutting
should form a lattice of finite height;
\else
must have a lattice structure
 such that every assertion can be increased at
most finitely many times;
\fi
(c)~transfer functions that approximate weakest preconditions or
strongest postconditions over the assertions; and
(d)~rewrite functions whose soundness is justified by the assertions.
%%  To~guide compiler writers,
%%  we show how dataflow analyses are related to
%%  seminal work on program 
%%  correctness. \simon{The ``next 700'' section sort of does so, but I'm not 
%%  sure it deserves mention in the abstract.}
\ourlib\ uses the algorithm of 
\citet{lerner-grove-chambers:2002}, which 
% enables compiler writers to
can
compose very simple analyses and transformations in a way that achieves
the same precision as complex, handwritten
``super-analyses.''
\ourlib\ will be the workhorse of a new
back end for the Glasgow Haskell Compiler (version~6.14, forthcoming).

\emph{Reviewers:} code examples are indexed at {\small\url{http://bit.ly/jkr3K}}
%%% Source: http://www.cs.tufts.edu/~nr/drop/popl-index.pdf
\end{abstract}

\makeatactive   %  Enable @foo@ notation

\section{Introduction}

A mature optimizing compiler for an imperative language includes many
analyses, the results of which are used to justify the optimizer's
code-improving transformations.
Many of the most important analyses and transformations---constant
propagation, live-variable analysis, inlining, sinking of loads, 
and so on---should be regarded as particular cases of
a single general problem: \emph{dataflow analysis and optimization} 
\cite{this-and-that}. \remark{Should we cite Muchnick here?}
Dataflow analysis is over thirty years old,
but a more recent, seminal paper by \citet{lerner-grove-chambers:2002} goes further, 
describing a powerful but subtle way to
\emph{interleave} analysis and transformation so that each 
piggybacks on the other.

Because optimizations based on dataflow analysis 
share a common intellectual framework, and because that framework is
subtle, it it tempting to
try to build a single re-usable library that embodies the 
subtle ideas, while
making it easy for clients to instantiate the library for different
situations.
Tempting, but difficult.
Although some such frameworks exist, as we discuss 
in \secref{related}, they have complex APIs and implementations,
and none implements the Lerner/Grove/Chambers technique.

In this paper we present \ourlib{} (short for ``higher-order
optimization library''), a new Haskell library for dataflow analysis and
optimization.  It has the following distinctive characteristics:

\begin{itemize}
\item
\ourlib\ is purely functional.  
Perhaps surprisingly, code that
manipulates control-flow graphs is easier to write, and far easier
to write correctly, when written in a purely functional style
\cite{ramsey-dias:applicative-flow-graph}.
When analysis and rewriting
are interleaved, so that rewriting must be done \emph{speculatively},
without knowing whether
the result of the rewrite will be retained or discarded,
the value of a purely functional style is intensified
(Sections \ref{sec:overview} and \ref{sec:fixpoints}).

\item
\ourlib\ is polymorphic. Just as a list library is
polymorphic in the list elements, so is \ourlib{} polymorphic, both in
the nodes that inhabit graphs, and in the dataflow facts that 
analyses compute over these graphs (\secref{using-hoopl}).

\item The Lerner/Grove/Chambers paper is persuasive but abstract.
We articulate their ideas in a concrete but simple API that hides 
a subtle implementation (Sections \ref{sec:graph-rep} and \ref{sec:using-hoopl}).  
You provide a representation for assertions, 
a transfer function that transforms assertions across a node, 
and a rewrite function that uses a computed assertion to 
justify rewriting a node.
\ourlib\ ``lifts'' these node-level functions to work over
control-flow graphs, sets up and solves recursion equations,
and interleaves rewriting with analysis.

\item Analyses and transformations built on \ourlib\ 
are small, simple, and easy to get right.
Moreover, \ourlib\ helps you write correct optimizations:
it statically rules out transformations that violate invariants
of the control-flow graph (Sections \ref{sec:graph-rep} and \ref{sec:rewrites}),
and dynamically it can help find the first transformation that introduces a fault
in a test program (\secref{fuel}). \simon{I wanted to write more about open/closed,
but I like this sentence with its claim to both static and dynamic assistance,
and maybe the open/closed story is hard to understand here.}

% \item \ourlib{} makes use of GADTS and type functions to offer unusually
% strong static guarantees. In particular, nodes, basic blocks, and
% graphs are all statically typed by their open or closedness on entry, and
% their open or closedness on exit (\secref{graph-rep}). For example, an add instruction is
% open on entry and exit, while a branch is open on entry and closed on exit.
% Using these types we can statically guarantee that, say, an add instruction
% is rewritten to a graph that is also open on both entry and exit; and 
% that the user cannot construct a block where an add instruction follows an
% unconditional branch.  We know of no other system that offers 
% static guarantees this strong.

\item \ourlib{} implements a rather subtle algorithm.  
Previous implementations of this algorithm---including three of our
own---are complicated and hard to understand, because all the tricky pieces
(fixed points, interleaved rewriting and transformation, recursive
rewriting, optimization fuel, and so on)
implemented inseparably. \remark{Maybe the title should hearken to
separation of concerns rather than lifting?}
In~this paper, we present a new way
to structure the implementation so that each tricky piece
is handled in just 
one place, separate from all the other tricky
pieces. \remark{Important if true}
The result is sufficiently elegant that 
this paper emphasizes the implementation as an object of interest in
its own right (\secref{engine}).
\end{itemize}
A working prototype of \ourlib{} is available for download 
at \unskip\break \url{http://ghc.cs.tufts.edu/hoopl}.\remark{This is why I wanted
the code in a separate git repo---but we'll get there.}
It is no toy: an ancestor of this library is
embodied in the Glasgow Haskell Compiler, where it optimizes the
imperative {\PAL} code in GHC's back end.  The new design is far
nicer, and will be in GHC shortly.

The API for \ourlib{} seems quite natural, but it requires
relatively sophisticated aspects of Haskell's type system, such
as higher-rank polymorphism, GADTs, and type functions.
As such, \ourlib{} offers a compelling case study in the utility
of these features.


\section{Dataflow analysis {\&} transformation by \rlap{example}}
\seclabel{overview}
\seclabel{constant-propagation}
\seclabel{example:transforms}
\seclabel{example:xforms}

We begin by setting the scene, introducing some vocabulary, and
showing a small motivating example.
A control-flow graph, perhaps representing the body of a procedure,
is a collection of \emph{basic blocks}---or just ``blocks''.
Each block is a sequence of instructions,
beginning with a label and ending with a
control-transfer instruction that branches to other blocks.
% Each block has a label at the beginning,
% a sequence of 
%  -- each of which has a label at the 
% beginning.  Each block may branch to other blocks with arbitrarily
% complex control flow.
The goal of dataflow optimization is to compute valid
\emph{assertions} (or \emph{dataflow facts}), 
then use those assertions to justify code-improving
transformations (or \emph{rewrites}) on a \emph{control-flow graph}.  

Consider a concrete example: constant propagation with constant folding.
On the left we have a basic block; in the middle we have
assertions that hold between statements (or \emph{nodes}) 
in the block; and at
the right we have the result of transforming the block 
based on the assertions:
\begin{verbatim}
      Before        Facts        After
          ------------{}-------------
      x := 3+4                   x := 7
          ----------{x=7}------------
      z := x>5                   z := True
          -------{x=7, z=True}-------
      if z                       goto L1
       then goto L1
       else goto L2
\end{verbatim}
constant propagation works
from top to bottom.  We start with the empty assertion.  
Given the empty assertion and the node @x:=3+4@ can we make a (constant-folding)
transformation?
Yes!  We can replace the node with @x:=7@.
Now, given this transformed node,
and the original assertion, what assertion flows out of the bottom of
the transformed node?  
The assertion \{@x=7@\}.  
Given the assertion \{@x=7@\} and the node @z:=x>5@, can we make a
transformation?  Yes: constant propagation can replace the node with @z:=7>5@.
Now, can we do another transformation?  Yes: constant folding can 
replace the node with @z:=True@.
And so the process continues to the end of the block, where we
can replace the conditional branch with an unconditional one, @goto L1@.

\seclabel{simple-tx}
\paragraph{Interleaved transforamtion and analysis.}
Notice that our example \emph{interleaves} transformation and analysis.
Interleaving makes it far easier to write effective analyses.
If, instead, we \emph{first} analysed the block
and \emph{then} transformed it, the analysis would have to ``predict''
the transformations.
For example, given the incoming fact \{@x=7@\}
and the instruction @z:=x>5@,
a pure analysis could produce the outgoing fact
\{@x=7@, @z=True@\} by simplifying @x>5@ to @True@.
But the subsequent transformation must perform
\emph{exactly the same simplification} when it transforms the instruction to @z:=True@!
If instead we \emph{first} rewrite the node to @z:=True@, 
and \emph{then} apply the transfer function to the rewritten node, 
the transfer function becomes laughably simple: it merely has to see if the
right hand side is a constant (you can see actual code in \secref{const-prop-client}).
The gain is even more compelling if there are a number of interacting 
analyses and/or transformations; for more substantial
examples, consult \citet{lerner-grove-chambers:2002}.

\paragraph{Forwards and backwards.}
Constant propagation works \emph{forwards}, and a fact is typically an
assertion about the program state (such as ``variable @x@ holds value
@7@'').  Many useful analyses work \emph{backwards}, from bottom to
top.  An example is live-variable analysis, where the fact takes the form
``variable @x@ is dead'' and constitutes an assertion about the
\emph{continuation} of a program point.  For example, the fact ``@x@ is
dead'' at a program point P is an assertion that @x@ is unused on any program
path starting at P.  
The accompanying transformation is called dead-code elimination;
if @x@~is dead, this transformation rewrites
the assignment @x=e@ into a no-op.

% ----------------------------------------
\section{Representing control-flow graphs} \seclabel{graph-rep}

\ourlib{} is a library that makes it easy to define dataflow analyses,
and transformations driven by these analyses, on control-flow graphs.
Graphs are composed from smaller units, which we discuss from the
bottom up:
\begin{itemize}
\item A \emph{node} is defined by the library client;
\ourlib{} knows nothing about the representation of nodes (\secref{nodes}).
\item A basic \emph{block} is a sequence of nodes (\secref{blocks}).
\item A \emph{graph} is an arbitrarily complicated control-flow graph,
composed from basic blocks (\secref{graphs}).
\end{itemize}

\subsection{Open and closed}

Nodes, blocks, and graphs share important properties in common.
In particular, each can be \emph{open or closed at entry}
and \emph{open or closed at exit}.  
An \emph{open} point is one at which control may implicitly ``fall through;''
to transfer control at a \emph{closed} point requires an explicit
control-transfer instruction.
For example,
\begin{itemize}
\item A multiply instruction is open on entry (because control can fall into it
from the preceding instruction), and open on exit (because control falls through
to the next instruction).
\item An unconditional branch is open on entry, but closed on exit (because 
control cannot fall through to the next instruction).
\item A label is closed on entry (because in \ourlib{} we do not allow
control to fall through into a branch target), but open on exit.
\end{itemize}
% This taxonomy enables \ourlib{} to enforce invariants:
% only nodes closed at entry can be the targets of branches, and only nodes closed
% at exits can transfer control (see also \secref{edges}).
% As~a consequence, all control transfers originate at control-transfer
% instructions and terminated at labels; this invariant dramatically
% simplifies analysis and transformation. 
These examples concern nodes, but the same classification applies
to blocks and graphs.  For example the block
\begin{code}
   x:=7; y:=x+2; goto L
\end{code}
is open on entry and closed on exit.  
To summarize, suppose E is a node, block, or graph: \simon{Removed the claim
about a unique entry point.}
\begin{itemize}
% \item Regardless of whether E is open or closed, 
% it has a unique entry point where execution begins.
\item If E is open at exit, control leaves E
\emph{only} by ``falling through'' from a unique exit point.
\item If E is closed at exit, control leaves \emph{only}
by explicit branches from closed-on-exit nodes.
\end{itemize}

\remark{We've lost the client's ability to choose the representation
of a full function/procedure/method} \simon{Don't understand this remark}

\subsection{Nodes} \seclabel{nodes}

The primitive constituents of a \ourlib{} control-flow graph are
\emph{nodes}, which are defined by the client.
Typically, a node might represent a machine instruction, such as an
assignment, a call, or a conditional branch.  
But \ourlib{}'s graph representation is polymorphic in the node type,
so each client can define nodes as it likes.
Because they contain nodes defined by the client,
graphs can include arbitrary client-specified data, including
(say) C~statements, method calls in an object-oriented language, or
whatever.


\begin{figure}
\begin{code}
data `Node e x where
  `LabelNode  :: Label -> Node C O
  `Assign     :: Reg   -> Expr -> Node O O
  `Store      :: Expr  -> Expr -> Node O O
  `Branch     :: Label -> Node O C
  `CondBranch :: Expr  -> Label -> Label -> Node O C
    -- ... more constructors ...
\end{code}
\caption{A typical node type, as it might be defined by a client} 
\figlabel{cmm-node}
\end{figure}

\ourlib{} knows \emph{at compile time} whether a node is open or
  closed at entry and exit:
the type of a node has kind @*->*->*@, where the two type parameters
are type-level flags, one for entry and one for exit.
Such a type parameter may be instantiated only with type @O@~(for
open) or type~@C@ (for closed).
As an example,
\figref{cmm-node} shows a typical node type as it might be written by
one of \ourlib's \emph{clients}.
The type parameters are written @e@ and @x@, for
entry and exit respectively.  
The type is a generalized algebraic data type, and
it is defined in using syntax
that gives the type of each constructor.  
\cite{peyton-jones:unification-based-gadts}.
For example, constructor @LabelNode@
takes a @Label@ and returns a node of type @Node C O@, where ``@C@''~says ``closed
at entry'' and ``@O@''~says ``open at exit''.  
(We often abbreviate ``a node of type @Node@~@C@~@O@''
to ``a closed/open node'',  
and similarly for blocks and graphs.)
The types @Label@, @O@, and~@C@ are 
exported by \ourlib{} (\figref{graph}).  

Similarly, an @Assign@ node takes a register and an expression, and
returns a @Node@ open at both entry and exit; the @Store@ node is
similar.  The types @Reg@ and @Expr@ are private to the client, and
\ourlib{} knows nothing of them.  
Finally, the control-transfer nodes @Branch@ and @CondBranch@ are open at entry
and closed at exit.  

Nodes closed on entry are the only targets of control transfers;
nodes open on entry and exit never perform control transfers;
and nodes closed on exit always perform control transfers\footnote{%
To obey these invariants,
a conditional-branch instruction, which typically may either transfer control
\emph{or} fall through, must be represented as a two-target
conditional branch, with the fall-through path in a separate block.  
This representation is standard \cite{appel:modern},
and it costs nothing in practice:
a late-stage code-layout pass can readily reconstruct efficient code.
}.
We~often call these nodes \emph{first}, \emph{middle}, and \emph{last} nodes
respectively, because of the position each type of node occupies in a
basic block.

\subsection{Blocks} \seclabel{blocks}

\begin{figure}
\begin{code}
data `O   -- Open
data `C   -- Closed

data `Block n e x where
 `BUnit :: n e x       -> Block n e x
 `BCat  :: Block n e O -> Block n O x -> Block n e x

data `Graph n e x where
  `GNil  :: Graph n O O
  `GUnit :: Block n O O -> Graph n O O
  `GMany :: Link e (Block n O C) 
        -> Body n
        -> Link x (Block n C O)
        -> Graph n e x

data `Body n where
  `BodyEmpty :: Body n
  `BodyUnit  :: Block n C C -> Body n
  `BodyCat   :: Body n -> Body n -> Body n

data `Link ex t where
  `OpenLink   :: t -> Link O t
  `ClosedLink ::      Link C t

newtype `Label = Label Int

class Edges n where
  entryLabel :: n C x -> Label
  successors :: n e C -> [Label]
\end{code}
\caption{The block and graph types defined by \ourlib} 
\figlabel{graph} \figlabel{edges}
\end{figure}

\ourlib\ combines the client's nodes into
blocks and graphs, which, unlike the nodes, are defined by \ourlib{}
 (\figref{graph}).
A~@Block@ is parameterized over the node type~@n@
as well as over the same flag types that make it open or closed at
entry and exit.

The @BUnit@ constructor lifts a node to become a block; @BCat@
concatenates blocks in sequence. 
It~makes sense to concatenate blocks only when control can fall
through from the first to the second; therefore, 
two blocks may be concatenated {only} if each block is open at
the point of concatenation.
This restriction is enforced by the type of @BCat@, whose first 
argument must be open on exit, and whose second argument must be open on entry.
It~is statically impossible, for example, to put a @Branch@
immediately before an
@Assign@.  
Indeed, the @Block@ type statically guarantees that any
closed/closed @Block@ -- which compiler writers normally 
call a ``basic block'' --
must consist of exactly one closed/open node (such as @Label@
in \figref{cmm-node}), 
followed by zero or more open/open nodes (@Assign@ or @Store@), 
and terminated with exactly one 
open/closed node (@Branch@ or @CondBranch@).  
Using GADTs to enforce these invariants is one of 
\ourlib{}'s innovations.
% Also notice that the definition of @Block@ guarantees that every @Block@ 
% has at least one node.
% SPJ: I think we've agreed to stick with PNil.
% \remark{So what? Who cares?  Why is it important
% that no block is empty?  (And if we had an empty block, we could ditch
% @PNil@ from @PGraph@, which woudl be an improvement.)}

\subsection{Graphs} \seclabel{graphs}

\ourlib{} composes blocks into graphs, which are also defined 
in  \figref{graph}.
Like, @Block@ the data type @Graph@ is parameterized over
both nodes @n@ and its open/closed shape (@e@ and @x@).
It has three constructors.  The first two
deal with the base cases of open/open graphs:
an empty graph is represented by @GNil@ while a single-block graph
is represented by @GUnit@.

More general graphs are represented by @GMany@, which has three
fields:
\begin{itemize}
\item 
The first is the entry link.  If the graph is open on entry,
the entry link is of form $@(OpenLink@\;b@)@$, where $b$ is
an open/closed block.  If the graph is closed on entry then 
there is no distinguished entry block, and the link is of form @ClosedLink@.
\item 
The second field is the @Body@ of the graph, namely a simple 
collection of closed/closed blocks.  We use a new data type 
rather than a list so that we can concatenate in constant time.
\item 
The final field is the exit link, and is dual to the entry link.
\end{itemize}
% 
% \ourlib{} defines two distinct types of control-flow
% graphs.\remark{Why?  Wouldn't it be simpler to have just one type?
% (Not only do I think a reader will have this question, but I myself am
% not liking this direction\ldots)
% }
% The first,
% @Graph@, is entirely straightforward (see \figref{graph}): it is a
% collection of basic blocks, each with its own label.
% To~ensure that each label refers to a unique basic block, we represent
% the graph as a finite map from @Label@ to \mbox{@Block@ @n@ @C@ @C@}.  
% A~procedure might be represented by a @Graph@ together with the @Label@
% at which execution starts.
% 
% As illustrated in \secref{constant-propagation}, a client of \ourlib{} 
% may rewrite a node.
% But what does it rewrite the node \emph{to}?  
% In~\secref{constant-propagation}, it is enough to rewrite a node into
% a new node.
% But in general, a node might be rewritten into a graph, possibly with
% internal control flow:
% \begin{itemize}
% \item
% A redundant assignment might be eliminated by rewriting it into an
% empty graph.
% \item
% A~single node representing a ``safe'' divide instruction (that detects
% overflow) might be rewritten into a small graph of machine
% instructions that requires a test and conditional branch to detect
% overflow.
% \item
% A~single node that copies a large object might be rewritten into an
% internal loop that copies the words of the object individually.
% \end{itemize}
% None of these graphs can be represented as a @Graph@, because a
% @Graph@ is closed at both ends; 
% indeed a @Graph@ does not even have shape parameters @e@ and @x@.
% Rather, the rewrite function should return a ``partial graph'', 
% or @PGraph@, which can be open or closed at either
% end just like a node or block.  
% \remark{Not only don't I like this story, but I think it vitiates one
% of the beautiful parts of the paper: nodes are like blocks are like
% graphs.
% Instead of introducing @Graph@ as standard and treating @PGraph@ as
% special, I'd like to do it exactly the other way round.}
% 
% The definition of @PGraph@ is given in \figref{graph}.
% It includes a @PNil@ constructor for the empty graph,
% and a @PUnit@ constructor to lift a single @Block@ to be a @PGraph@.
% But in general, a graph has many blocks, and is built by
% \finalremark{Tabling the question of the closed/closed empty graph}%
% the @PMany@ constructor:
% \begin{itemize}
% \item
% A~graph that is open at the entry has a ``head'' block that is open at
% the entry and closed at the exit.\remark{Why isn't this called the
% ``entry sequence?''  Seems better than ``head''\ldots}
% A~graph that is closed at the entry doesn't have a ``head'' block.
% This choice is represented by a field of type @Head n O@ or @Head n e@:
% If the field's type is @Head n O@, it is created with the @Head@ value
% constructor, and it contains an entry sequence of type @Block n O C@.
% If the field's type is @Head n C@, it is created with the @NoHead@ value
% constructor, and it contains only a @Label@.\remark{Why a @Label@
% instead of nothing?}
% %\item The first field is called the ``head'' of the @PGraph@.
% %If it is $@(Head@\;b@)@$, the @PGraph@ is open on entry, with initial block $b$.
% %If it is $@(NoHead@\;l@)@$, the @PGraph@ is closed on entry, and execution begins
% %at block $l$. 
% We need the @Label@ $l$, because the @PGraph@ 
% may replace a node, so just as a node closed at entry has a unique entry point,
% so does a @PGraph@.\remark{This argument just seems like special
% pleading to me.   If you want the node analogy, \emph{start} from the
% node analogy.  And the actual reasoning goes like this: if the
% @PGraph@ replaces a node closed at entry, the replacement graph must
% begin with the \emph{same} label as the node it replaces.
% Now this reader starts to wonder why one would store  that label twice.}
% \item
% A~general graph, no matter how it is shaped at the entry or exit,
% contains a collection of labelled closed/closed (basic) blocks.
% This collection is represented as a @Graph@ of blocks,
% as discussed above.
% \item
% A~graph that is open at the exit has a ``tail'' block that is closed at
% the entry and open at the exit.\remark{Why isn't this called the
% ``exit sequence?''  Seems better than ``tail\ldots}
% A~graph that is closed at the exit doesn't have a ``tail'' block.
% The exit sequence (or lack thereof) is represented by a value of type
% @Tail n x@, where @x@~describes the shape of the exit.
% \remark{And if we could get rid of the beastly block IDs, we could
% have the \emph{same} construct here as at the entry sequence, and
%  enjoy a little parallel structure.}
% If the @Graph@ is closed on exit the tail is just @NoTail@.
% If it is open on exit, the tail is a (labelled) @Block@, open on
%  exit.\remark{Why is it labelled?  None of the other blocks closed on
%  entry is labelled?  What's special about this one?}
% \end{itemize}

Graphs compose nicely, with constant-time complexity.
Unlike blocks, two graphs may be concatenated 
when they are both open at the point of concatenation, \emph{and}
when they are both closed (but not in the other two cases):
\par{\small
\begin{code}
`gCat :: Graph n e a -> Graph n a x -> Graph n e x
gCat GNil g2 = g2
gCat g1 GNil = g1

gCat (GUnit ^b1) (GUnit ^b2)             
 = GUnit (b1 `BCat` b2)

gCat (GUnit b) (GMany (OpenLink e) bs x) 
 = GMany (OpenLink (b `BCat` e)) bs x

gCat (GMany e bs (OpenLink x)) (GUnit b2) 
  = GMany e bs (OpenLink (x `BCat` b2))

gCat (GMany e1 bs1 (OpenLink x1)) 
            (GMany (OpenLink e2) bs2 x2)
  = GMany e1 (bs1 `BodyCat` b `BodyCat` bs2) x2
  where
    b = BodyUnit (x1 `BCat` e2)

gCat (GMany e1 bs1 ClosedLink) (GMany ClosedLink bs2 x2)
  = GMany e1 (bs1 `BodyCat` bs2) x2
\end{code}}
This defintion is a nice example of the power of GADTs: the
pattern-matching is exhaustive, and all the open/closed invariants are
statically checked.  For example, consider the second-last equation for @gCat@.
Since the exit link of the first argument is @OpenLink x1@,
we know that $@a@~@C@$, and hence the entry link of the second 
argument must be @(OpenLink e2)@
Moreover @x1@ must be
closed/open, and @e2@ must be open/closed.  So we can compose them in
sequence with @BCat@ to produce a closed/closed block, which can be
added to the @Body@ of the result.

Every graph has a unique representation.  An empty open/open graph is represented
by @GNil@, while a closed/closed one is @gNilCC@:
\begin{code}
gNilCC :: Graph C C
gNilCC = GMany ClosedLink BodyEmpty ClosedLink
\end{code}
The representation of a @Graph@ consisting of a single block @b@ 
depends on the shape of @b@, thus:
\begin{code}
gUnitOO :: Block O O -> Graph O O
gUnitOC :: Block O C -> Graph O C
gUnitCO :: Block O C -> Graph C O
gUnitCC :: Block O C -> Graph C C
gUnitOO b = GUnit b
gUnitOC b = GMany (OpenLink b) BodyEmpty ClosedLink
gUnitCO b = GMany ClosedLink   BodyEmpty   (OpenLink b)
gUnitCC b = GMany ClosedLink   (BodyUnit b) ClosedLink@
\end{code}
Multi-block graphs are similar.
From this table we can see why @GUnit@ is restricted to open/open
blocks; if it was more accommodating then there would be 
multiple representations for a single graph.

\subsection{Labels and successors} \seclabel{edges}

If \ourlib{} knows nothing about nodes, how can it know to which block a
control transfer goes, or what is the @Label@ at the start of a block?
In this situation the standard Haskell idiom is to provide a type class whose methods
provide exactly the needed operations; it is given in \figref{edges}.
The @entryLabel@ method takes a first node (one closed on entry, \secref{nodes})
and returns its @Label@;
the @successors@ method takes a last node (closed on exit) and returns the @Label@s to
which it can transfer control.  In \ourlib{} a middle node (open-open) cannot refer to any @Label@s,
as the absence of an interrogation function implies. 

When defining a node type, a \ourlib{} client should also make it an
instance of @Edges@.  Thus for @Node@ we would write
\begin{code}
instance Edges Node where
  entryLabel (LabelNode l) = l
  successors (Branch b) = [b]
  successors (CondBranch e b1 b2) = [b1,b2]
\end{code}
Again, the pattern-matching for both functions is exhaustive, and
the compiler statically checks this fact.  For example, @entryLabel@ simply
cannot be applied to a @Assign@ or @Branch@ node; the type checker would
reject any such attempt.

\ourlib{} provides instances for the @Block@ type thus:
\begin{code}
instance Edges n => Edges (Block n) where
  entryLabel (BUnit n)  = entryLabel n
  entryLabel (BCat b _) = entryLabel b
  successors (BUnit n)  = successors n
  successors (BCat _ b) = successors b
\end{code}
A slight infelicity is that we cannot make @Graph@ an instance of @Edges@,
because a graph closed on entry has no unique label.  Fortunately, we never
need @Graph@s to be in @Edges@.

\section {Using \ourlib{} to analyse and transform graphs} \seclabel{using-hoopl}
\seclabel{making-simple}
\seclabel{create-analysis}

\begin{figure}
\begin{code}
data `ForwardPass n f
  = `FwdPass { `fp_lattice  :: DataflowLattice f
            , `fp_transfer :: FwdTransfer n f
            , `fp_rewrite  :: FwdRewrite n f }

------- Lattice ----------
data `DataflowLattice a = DataflowLattice
 {`fact_bot    :: a,
  `fact_extend :: a -> a -> (ChangeFlag, a) }

data `ChangeFlag = `NoChange | `SomeChange

------- Transfers ----------
type `FwdTransfer n f 
  = forall e x. n e x -> Fact e f -> Fact x f 

------- Rewrites ----------
type `FwdRewrite n f 
  = forall e x. n e x -> Fact e f
             -> Maybe (FwdRes n f e x)

data `FwdRes n f e x 
  = FwdRes (AGraph n e x) (FwdRewrite n f)

------- Facts ----------
type family   `Fact x f :: *
type instance Fact C f = FactBase f
type instance Fact O f = f
\end{code}
\caption{\ourlib{} API data types}
  \figlabel{api-types}
  \figlabel{lattice-type} \figlabel{lattice}
  \figlabel{transfers}  \figlabel{rewrites}
\end{figure}
% omit mkFactBase :: [(Label, f)] -> FactBase f

Now that we have graphs in hand, we return to dataflow
optimization.
\ourlib{} makes it  easy for a
client to build a new dataflow analysis and optimization.  To do this
the client must supply the following pieces:
\begin{itemize}
\item \emph{A node type} (\secref{graph-rep}).  
Then \ourlib{} supplies the @Block@ and @Graph@ types
that let the client build control-flow graphs out of nodes.
\item \emph{A data type of facts}, and some operations over 
those facts (\secref{facts}).
Each analysis uses its own analysis-specific 
facts, so \ourlib{} is completely polymorphic in 
the fact type.   
\item \emph{A transfer function} that takes an input fact and a node,
and produces the output fact that flows out of the other end of 
the node (\secref{transfers}).  
\item \emph{A rewrite function} that takes an input fact and a node,
and produces either @Nothing@,
or @(Just g)@ where @g@ is an \emph{arbitrary graph} that should
replace the node.  The ability to replace a node by an arbitrary graph is crucial.
We discuss the rewrite function
in \secref{rewrites}.
\end{itemize}
We combine the last three client-supplied arguments 
into a single record of type @ForwardPass@; see \figref{api-types}.
With these pieces in hand, a client can call \ourlib{}'s analysis-and-rewriting
function:
\begin{code}
analyseAndRewriteFwd
  :: Edges n         
  => ForwardPass n f   -- Lattice, transfer, 
                       -- and rewrite functions
  -> Body n                 -- Input body
  -> FactBase f             -- Input fact(s)
  -> FuelMonad (Body n,     -- Result body
                FactBase f) -- ...and its facts

type `FactBase f = LabelMap f
 -- A finite mapping from Labels to facts f
\end{code}
Roughly speaking, given a @ForwardPass@, the 
function @analyseAndRewriteFwd@ can transform a @Body@ into an
optimized @Body@.  One thing that is immediately obvious from this
type is that it is polymorphic in the type of nodes @n@ and facts @f@;
these are entirely under the control of the client.

As well as taking and returning a @Body@, the function also takes
some input facts (the @FactBase@) and produces output facts. A
@FactBase@ is simply a finite mapping from @Label@ to facts.  The
output @FactBase@ maps each @Label@ in the @Body@ to its fact; if
the @Label@ is not in the domain of the @FactBase@ its fact is the
bottom of the lattice.  Similarly the input @FactBase@ supplies any
facts that hold on entry to the @Body@.  For example, in our constant
propagation example, if the @Body@ represents the body of a procedure
with parameters $x,y,z$ we would map the entry @Label@ to a fact
$@[@(x,@Top@), (y,@Top@), (z,@Top@)@]@$, to specify the the procedure
parameters have unknown values.

The user-level model of how @analyseAndRewriteFwd@ works is this.  The
function walks forward over each block in the graph.  At each node, it applies the
rewrite function to incoming fact and the node.  If the rewrite
function returns @Nothing@, the node is retained as part of the output
graph, the transfer function is used to compute the downstream fact,
and we move on to the next node.

In the rest of this section we flesh out other details of the
interface to @analyseAndRewriteFwd@, leaving the implementation for
\secref{engine}.  

\subsection{Dataflow lattices} \seclabel{lattices} \seclabel{facts}

For any one analysis or transformation the client must define a type
of dataflow facts.  A dataflow fact typically represents an assertion
about a program point.  (A program point is simply an edge in the 
control-flow graph.)
\begin{itemize}
\item An assertion about all paths \emph{to} a program point is established
by a \emph{forwards analysis}. For example the assertion ``$@x@=@3@$'' at point P 
claims that variable @x@ holds value @3@ at P, regardless of the
path by which P is reached.
\item An assertion about all paths \emph{from} a program point is 
established by a \emph{backwards analysis}. For example, the 
assertion ``@x@ is dead'' at point P claims that no path from P uses 
variable @x@.
\end{itemize}

A~set of dataflow facts must form a lattice, and \ourlib{} must know
(a) the bottom element of the lattice and (b) how to join (take 
the least upper bound of) two elements.  To ensure that analysis
terminates, it is enough if no fact has more than finitely many
distinct facts above it, so that any sequence of joins will terminate.

In our constant-propagation example, a fact is a finite conjunction of
sub-facts, at most one for each variable.  Each sub-fact has the form ``@x=@$k$'',
for some constant $k$, or ``@x=@$\top$'' (meaning that there is 
no single value held by $x$ at this program point).  For example, 
consider this graph,
where we assume @L1@ is the entry point:
\begin{verbatim}
  L1: x=3; y=4; if z then goto L2 else goto L3
  L2: x=7; goto L3
  L3: ...
\end{verbatim}
Since control flows to @L3@ from two places,
we must join the facts coming from those two places.  The fact at the
end of block @L1@ is (@x=3@, @y=4@, @z=False@), while at the end of
@L2@ the fact is (@x=7@, @y=4@, @z=True@).  
Both blocks branch to @L3@
where they must be joined, giving
the fact (@x=@$\top$, @y=4@, @z=@$\top$) at @L3@, where the $\top$'s indicate
that the analysis cannot
nail down @x@ and @z@ to hold a single value at @L3@\footnote{
In this example @x@ and @z@ really do vary at @L3@, but in general
the analysis might simply be conservative.}.

In practice, joins are computed at labels.
If~$f_{\mathit{id}}$ is the fact currently associated with the
label~$\mathit{id}$, 
and if a transfer function propagates a new fact~$f_{\mathit{new}}$
into the label~$\mathit{id}$, 
the dataflow engine replaces $f_{\mathit{id}}$ with
the join  $f_{\mathit{new}} \join f_{\mathit{id}}$.
Furthermore, the dataflow engine wants to know if
  $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$,
because if not, the analysis has not reached a fixed point.

Concretely, @analyseAndRewriteFwd@ is polymorphic in the fact type
@f@, and requires an argument of type @DataflowLattice f@.  The latter
type is shown in \figref{lattice}, and tells \ourlib{} the bottom and
join operations on the lattice.  In practice, when computing a join,
it is often cheap to learn if the join is equal to one of the
arguments.  Moreover, as noted in the previous paragraph, that
knowledge is precisely what \ourlib{} needs to know.  Hence, instead
of requiring an equality operation on facts (which might be
expensive), we instead ask @fact_extend@ to return a @ChangeFlag@ as
well as the joined value.  The @ChangeFlag@ should be @NoChange@ if
the result is the same as the second argument, and @SomeChange@ if the
result differs.  (Note the assymmetry.)

In the case of constant propagation, \figref{const-prop} shows a
possible concrete represetation of the facts described above.  A fact
is represented as a finite map from a variable to a value of type
@Maybe (WithTop Const)@.
A~variable $x$ maps to @Nothing@ iff $x=\bot$;
$x$~maps to @Just Top@ iff $x=\top$;
and $x$~maps to $@Just@\, (@Elt@\, k)$ iff $x=k$.
Any one procedure has only
finitely many variables; only finitely many facts are computed at any
program point; and in this lattice any one fact can increase at most
twice.  These properties ensure that the dataflow engine will reach a
fixed point.


\subsection{The transfer function} \seclabel{transfers}

A~transfer function is presented with the dataflow fact on the edge coming
into a node, and it computes dataflow facts on the outgoing edge.
In a forward analysis, the dataflow engine starts with the fact at the
beginning of a block and applies the transfer function to successive 
nodes in that block until eventually the transfer function for the last node
computes the facts that are propagated to the block's successors.
For example, consider this graph, with entry at @L1@:
\begin{code}
  L1: x=3; goto L2
  L2: y=x+4; x=x-1; 
      if x>0 then goto L2 else return
\end{code}
A forward analysis starts with the bottom fact \{\} at every label.
Analysing @L1@ propagates this fact forward, by applying the transfer 
function successively to the nodes
of @L1@, emerging with the fact \{@x=3@\} for @L2@.
This is joined with the existing (bottom) fact for @L2@.
Now the analysis propagates @L2@'s fact forward, again using the transfer
function, this time emerging with a new fact \{@x=2@, @y=7@\} for @L2@.
Again, this is joined with the existing fact for @L2@, and the process
is iterated until the facts for each label reach a fixed point.

But wait!  What is the \emph{type} of the transfer function?
If the node is open at exit, it takes a fact and the node and produces a fact.
But what if the node is \emph{closed} on exit?
In that case the transfer function must
produce a list of (@Label@,fact) pairs, one for each outgoing edge.  

\emph{So the type of the transfer function's result 
depends on the open or closed-ness of the node's exit.}
Fortunately, this dependency is precisely what Haskell's (recently added) 
indexed type families can express.
The relevant \ourlib{} definitions are given in \figref{transfers}.
A forward transfer function, of type (@FwdTransfer@ @n@ @f@), 
is a function polymorphic in @e@ and @x@.  It takes a fact and a 
node of type (@n@ @e@ @x@), and produces an
outgoing ``fact-like thing'' of type (@Fact@ @x@ @f@).  The 
type constructor @Fact@
should be thought of as a type-level function; its signature is given in the
@type family@ declaration, while its definition is given by two @type instance@
declarations.  The first says that the fact-like thing coming out a node
open at exit is just a fact @f@. The second says that if the node is closed
on exit the outgoing thing is a mapping from @Label@ to facts.

We have chosen the argument order so that if
\begin{code}
  transfer_fn :: FwdTransfer n f
  node        :: n e x
\end{code}
then @(transfer_fn n)@ is literally a predicate transformer:
\begin{code}
  transfer_fn n :: Fact e f -> Fact x f
\end{code}

\ifpagetuning\enlargethispage{0.5\baselineskip}\fi


\subsection{The rewrite function} \seclabel{rewrites} \seclabel{example-rewrites}

\begin{figure}
\hfuzz=5.7pt % cheating again!
\begin{code}
type `AGraph n e x 
  = [Label] -> (Graph n e x, [Label])

`withLabels :: Int ([Label] -> AGraph n e x)
            -> AGraph n e x
withLabels n fn = \ls -> fn (take n ls, drop n ls)

`mkIfThenElse :: Expr -> AGraph Node O O
             -> AGraph Node O -> AGraph Node O O
mkIfThenElse p t e
  = withLabels 3 $ \[l1,l2,l3] ->
    gUnitOC (BUnit (CondBranch p l1 l2))   `gCat` 
    mkLabel l1 `gCat` t `gCat` mkBranch l3 `gCat`
    mkLabel l2 `gCat` e `gCat` mkBranch l3 `gCat`
    mkLabel l3

mkLabel  l = gUnitCO (BUnit (LabelNode l))          
mkBranch l = gUnitOC (BUnit (Branch l))
\end{code}
\caption{The \texttt{AGraph} type} \figlabel{agraph}
\end{figure}

We compute dataflow facts in order to enable code-improving
transformations on control-flow graphs.
In our constant-propagation example, the dataflow facts may enable us
to simplify an expression by performing constant folding, or to 
turn a conditional branch into an unconditional one.
Similarly, a liveness analysis may allow us to 
rewrite an assignment to a dead register by a no-op; knowing that a register is
live at a call site might provoke us to insert a spill before the call; and so on.

A @ForwardPass@ therefor includes a \emph{rewriting function}
that takes a node and a fact, and optionally returns \ldots what?
At first one might
expect that rewriting should return a new node, but that is not enough.
For example, perhaps we want to replace a high-level operation
with a tree of conditional branches or a loop, which entails introducing new blocks
with arbitrary control flow.
Another important special case is rewriting a node to a no-op.
So in general the rewrite function must return an \emph{arbitrary graph}.

Once the rewrite has been performed, what then?  
Since the rewrite returns a graph, the 
rewritten graph must itself be analysed and (perhaps) rewritten.
So we must call @analyseAndRewriteFwd@ to process the rewritten
graph -- but what @ForwardPass@ should call it with?
There are two common situations:
\begin{itemize}
\item Sometimes we want to analyse and transform the rewritten graph
with an unmodified @ForwardPass@.  So the 
rewritten graph may itself be further rewritten ---  
this is called \emph{deep rewriting}.
When deep rewriting is used, the client's rewrite function must
ensure that the graphs it produces are not rewritten indefinitely (\secref{correctness}).
\item On other occasions we want to analyse \emph{but not further rewrite} the
rewritten graph.  This is called \emph{shallow rewriting}, 
and it is easy to achieve by passing a modified @ForwardPass@, 
whose rewriting function always returns @Nothing@.
\end{itemize}
Deep rewriting is usually essential to achieve the results of
\citet{lerner-grove-chambers:2002}.
But shallow rewriting is also sometimes vital; for example, a rewrite that inserts
a spill before a call might risk repeatedly inserting the same spill if the 
rewrite was iterated. 

Notice that in either case, the nodes following the rewritten node see 
\emph{up-to-date} facts; that is, their input facts are computed from analysing
the \emph{rewritten} upstream nodes.

The shallow-vs-deep choice is really a property of the rewrite function 
itself.  This idea is elegantly expressed by the type of @FwdRewrite@
in \figref{api-types}.  A @FwdRewrite@ takes a node and a suitably-shaped
fact, and returns either @Nothing@, indicating no rewrite; 
or $@(Just@\;@(FwdRes@\;ag\;rw@))@$, indicating that a rewrite 
should be done.  We discuss the results $ag$ and $rw$ in turn.

The graph $ag$ is the rewritten graph.  
It has type @AGraph n e x@ when you 
were probably expecting @Graph n e x@.
The reason for this is that 
if the rewriter makes graphs containing blocks,
it will need to conjure up some fresh @Labels@.
An @AGraph@ makes it possible for the rewriter to do so conveniently,
using @withLabels@ as shown in \figref{agraph}.
This Figure also shows a possible implementation of @AGraph@
and an exmaple of how convenient-to-use @AGraph@ construction
functions are easy to write.

Notice also that 
the rewritten graph $ag$ is statically guaranteed to have
\emph{the same open/closed shape as the node being rewritten}.
So a branch instruction, for example, can only be rewritten to a graph 
closed at the exit.  

The second result $rw$ of a successful rewrite is a 
\emph{new rewriting function} to use when recursively processing
the rewritten graph $ag$. For shallow rewriting this new function is
the constant @Nothing@ function. For deep rewriting it is the original rewriting
function.  But there are other possibilities too.  For example, here is a function
that combines two rewriting funcitons.
\par{\small
\begin{code}
combineFwdRewrite :: FwdRewrite n f 
                  -> FwdRewrite n f 
                  -> FwdRewrite n f
`combineFwdRewrite rw1 rw2 n f
  = case rw1 n f of
      Nothing               -> rw2 n f
      Just (FwdRes g rw1') -> Just $ FwdRes g $
                              combineFwdRewrite rw1' rw2
\end{code}}
What a beautiful type this function has! It tries @rw1@ and if that
declines to rewrite it behaves like @rw2@.  But if
@rw1@ succeeds, returning a new rewriter @rw1'@, the combine function
succeds but returns a new rewrite function obtained by combining @rw1'@
with @rw2@.  (We cannot apply @rw1'@ or @rw2@ 
directly to the graph @g@, because @r1@ returns a graph while @r1'@ 
expects a node.)
Finally, notice that @combineFwdRewrite@ is capable of
combining a deep-rewriting function and a shallow-rewriting function,
to produce a rewriting function that is a combination of deep and shallow.
This is something that the Lerner/Grove/Chambers framwork could not do,
because there was a global shallow/deep flag.

\subsection{Example: Constant propagation and constant folding} \seclabel{const-prop-client}

\begin{figure}
{\small
\begin{code}
-- Types and definition of the lattice
data `HasConst = `Top | `B Bool | `I Integer
type `ConstFact = Map Reg HasConst
`constLattice = DataflowLattice
  { fact_bot    = Map.empty
  , fact_extend = stdMapJoin constFactAdd}
  where
    `constFactAdd new old = (c, j)
      where j = if new == old then new else Top
            c = if j == old then NoChange else SomeChange

-------------------------------------------------------
-- Analysis: register has constant value
`regHasConst :: FwdTransfer Node ConstFact
regHasConst (LabelNode l)       f = lookupFact f l
regHasConst (Store _ _)         f = f
regHasConst (Assign x (Bool b)) f = Map.insert x (B b) f
regHasConst (Assign x (Int  i)) f = Map.insert x (I i) f
regHasConst (Assign x _)        f = Map.insert x Top   f
regHasConst (Branch l)          f = mkFactBase [(l, f)]
regHasConst (CondBranch (Reg x) tid fid) f
  = mkFactBase [(tid, Map.insert x (B True)  f),
                (fid, Map.insert x (B False) f)]
regHasConst (CondBranch _ tid fid) f 
  = mkFactBase [(tid, f), (fid, f)]

-------------------------------------------------------
-- Constant propagation
`constProp :: FwdRewrite Node ConstFact
constProp node facts
  = fmap toAGraph (mapE rewriteE node)
  where
    rewriteE e@(Reg r)
      = case M.lookup r facts of
          Just (B b) -> Just $ Bool b
          Just (I i) -> Just $ Int  i
          _          -> Nothing
    rewriteE e = Nothing

-------------------------------------------------------
-- Simplification ("constant folding")
`simplify :: FwdRewrite Node f
simplify (CondBranch (Bool b) t f) _
  = Just $ toAGraph $ Branch (if b then t else f)
simplify node _ = fmap toAGraph (mapE s_exp node)
  where
    s_exp (Binop Add (Int i1) (Int i2))
       = Just $ Int $ i1 + i2
    ...  -- more cases for constant folding

-- Rewriting expressions
`mapE :: (Expr    -> Maybe Expr) 
      -> Node e x -> Maybe (Node e x)
mapE f (LabelNode _) = Nothing
mapE f (Assign x e)  = fmap (Assign x) $ f e
 ... more cases for rewriting expressions

-------------------------------------------------------
-- Running the analysis
`consPropPass = ForwardPass
   { fp_lattice = constLattice
   , fp_transfer = regHasConst
   , fp_rewrite = constProp `combineFwdRewrite` simplify } 
\end{code}}
\caption{The client for constant propagation and constant folding} \figlabel{const-prop}
\end{figure}
\figref{const-prop} shows how to use @analyseAndRewriteFwd@ to
perform constant propagation and constant folding.
For each register, the analysis may conclude one of three facts:
the register must store a constant value (Boolean or integer),
the register might store a non-constant value,
or nothing is known about the contents of the register (bottom).
We~represent these facts using a finite map from registers to dataflow facts
(@HasConst@).
A register with a constant value maps to @Just v@, where @v@ is the constant value;
a register with a non-constant value maps to @Just Top@;
and a register with an unknown value maps to @Nothing@ (i.e. is not
in the domain of the finite map).

The definition of the lattice (@constLattice@) is straightforward.
The bottom element is an empty map (nothing is known about the contents of any register).
We use the @stdMapJoin@ function to lift the join operation
for a single register (@constFactAdd@) up to the map containing facts
for all registers.

For the transfer function, @regHasConst@, 
there are two interesting kinds of nodes:
assignment and conditional branch.
In the first two cases for assignment, a register gets a constant value,
so we produce a dataflow fact mapping the register to its value.
In the third case for assignment, the register gets a non-constant value,
so we produce a dataflow fact mapping the register to @Top@.
The last interesting case is a conditional branch where the condition
is a register.
Based on whether the conditional-branch flows to the true or false successor,
we can infer whether the value of the register is @True@ or @False@,
so we update the fact flowing to each successor accordingly.

Notice that we do not need to consider complicated cases such as
an assignment @x:=y@ where @y@ holds a constant value @k@.
Instead, we will rely on the interleaving of transformation
and analyses to first transform the assignment to @x:=k@,
which is exactly what our simple transfer function expects.
As we mentioned in \secref{simple-tx},
interleaving makes it possible to write
the simplest imaginable transfer functions, with no loss of performance.

The rewrite function for constant propagation, @constProp@,
simply rewrites each use of a register to its constant value.
We use the auxiliary function @mapE@
to apply @rewriteE@ to each use of a register in each kind of node;
in turn, @rewriteE@ function checks if the register has a constant
value and makes the substitution.  We assume an auxiliary function
\begin{code}
  toAGraph :: Node e x -> AGraph e x
\end{code}
\figref{const-prop} also gives a completely separate rewrite function 
to perform constant
folding, called @simplify@.  It rewrites a conditional branch on a
boolean constant to an unconditional branch, and runs @s_exp@
on every sub-expression to constant sub-expressions.

We have written two @FwdRewrites@ functions 
because they are sometimes independently useful.  But in this case we
want to apply \emph{both} of them,
so we compose them with @combineFwdRewrite@
before putting them in the @constPropPass@ (see \figref{const-prop}).
\remark{There is no @constPropPass@}

Note that @simplify@ does not need to check whether a register holds a
constant value, because it can rely @constProp@ to have replaced the
register by the constant; indeed @simplify@ does not consult the
incoming fact at all, and hence is polymorphic in @f@.

Finally, we combine the lattice, the transfer function, the combined
rewrite functions, into @constPropPass@.  This can be handed to
@analyseAndRewriteFwd@ to process a particular graph.

\subsection{Throttling the dataflow engine using ``optimization fuel''}
\seclabel{vpoiso}
\seclabel{fuel}

Writing and debugging an optimization can be tricky:
an optimization may rewrite the instructions in a procedure hundreds
of times, and any of those rewrites could be incorrect.
To debug such an optimization, we use a powerful technique,
first suggested by Whalley \cite{whalley:isolation},
to identify the first rewrite that 
transforms the procedure from working code to faulty code.

The key idea is to~limit the number of rewrites that
are performed while optimizing a procedure.
Because each rewrite leaves the observable behavior of the
program unchanged, it is safe to suppress rewrites at any point.
Therefore, given a program that fails when compiled with optimization,
the client can use binary search to halt rewriting just before or just after the
rewrite that introduces the failure, thereby identifying the buggy rewrite.
We control the number of rewrites by giving the optimizer
a fixed quantity of ``optimization fuel'';
each rewrite costs one unit of fuel,
and when the fuel is exhausted, no more rewrites are performed.

You will have noticed that the type of @analyseAndRewriteFwd@
returns a value in the @FuelMonad@ (\secref{using-hoopl}).
The @FuelMonad@ is a simple state monad maintaining the supply of unused
fuel.  It also holds a supply of fresh labels, which are used by the rewriter
for making new blocks; more precisely, the client-provided @FwdRewrite@
function produces an @AGraph@ (\figref{rewrites}) which Hoopl converts to
a @Graph@ by supplying the @AGraph@ with a supply of @Labels@ from the @FuelMonad@.

\subsection{Fixpoints and speculative rewrites} \seclabel{fixpoints}

The alert reader will be wondering what happens to rewriting in the presence of
loops.  Many analyses compute a fixpoint starting from unsound
``facts''; for example, a live-variable analysis starts from the
assumption that all variables are dead.  This means \emph{rewrites
performed in any iteration other than the last may be unsound, and
must be discarded}.  That is, each iteration of the fixpoint must
start afresh with the original graph.  

\emph{Nevertheless, those rewrites
should still be performed} (albeit tentatively, but possibly recursively)
so that the correct downstream facts can be computed as accurately as possible.
For example, consider this graph, with entry at @L1@:
\par{\small
\begin{code}
  L1: x=0; goto L2
  L2: x=x+1; if x==10 then goto L3 else goto L2
\end{code}}
The first traversal of block @L2@ will start with the ``fact'' \{x=0\};
but it will propagate the new fact \{x=1\} to @L2@, which will join the
existing fact to get \{x=$\top$\}.
But now suppose that the predicate in the conditional branch was @x<10@ instead
of @x==10@.  Again the first iteration will begin with the tentative fact \{x=0\}.
Using that fact we can rewrite the conditional branch to an unconditional
branch @goto L3@.  Hence no new fact is propagated to @L2@, and we have successfully
cut the loop.  This example is a little contrived, but it illustrates that 
for best results we should:
\begin{itemize}
\item Perform the rewrites on every iteration
\item Begin each new iteration with the orignal, virgin graph.
\end{itemize}
This sort of thing is hard to achieve in an imperative setting, where rewrites
mutate a graph in-place.
But it is trivially easy with an immutable graph:
all we need do is to revert to the original graph at the start
of each fixpoint iteration.

\subsection{Correctness} \seclabel{correctness}

Facts computed by @analyseAndRewriteFwd@ depend on graphs produced by the rewrite
function, which in turn depend on facts computed by the transfer function.
How~do we know this algorithm is sound, or even if it terminates?
A~proof requires its own POPL paper
\cite{lerner-grove-chambers:2002}, but we can give some
intuition.

We require these pre-conditions on the functions supplied to 
\ourlib{}:
\begin{itemize}
\item 
The lattice must have no \emph{infinite ascending chains}; that is,
a sequence of calls to @fact_extend@ must eventually return @NoChange@.
\item 
The transfer function must be 
\emph{monotonic} in the following sense: given a more informative fact in,
it should produce a more informative fact out.
\item 
The rewrite function must be \emph{sound}:
if it replaces a node @n@ by a replacement graph
@g@, then @g@ must be observationally equivalent to @n@ under the 
assumptions expressed by the incoming dataflow fact @f@.%
\footnote{We do not permit a transformation to change
  the @Label@ of a node. We have not found any optimizations
  that are prevented (or even affected) by this restriction.}
\item 
The rewrite function must be \emph{consistent} with the transfer function;
that is, \mbox{@transfer n f@ $\sqsubseteq$ @transfer g f@}.
For example, if the analysis says that @x@ is dead before the node~@n@,
then it had better still be dead if @n@ is replaced by @g@.
\item 
To ensure termination, a transformation that uses deep rewriting
must not return replacement graphs which 
contain nodes that could be rewritten indefinitely.
\end{itemize}
Without the conditions on montonicity and consistency,
our algorithm will terminate, 
but there is no guarantee that it will compute
a fixed point of the analysis.  And that in turn threatens the
soundness of rewrites based on possibly-bogus ``facts''.

However, under the above preconditions:
\begin{itemize} 
\item
The algorithm terminates.  The fixpoint loop must terminate because the 
lattice has no infinite ascending chains. And the user is responsible
for avoiding infinite rewriting in the @RewriteDeep@ case.
\remark{No definition for @RewriteDeep@!}
\item 
The algorithm is sound.  Why? Because if each rewrite is sound (in the sense given above), 
then applying a succession of rewrites is also sound.
Moreover, a~sound analysis of the rewritten graph
may generate only dataflow facts that could have been
generated by a more complicated analysis of the original graph.
\end{itemize}
% \john{If we haven't sold the audience by now, they're not going
%   to be convinced by this quote.}
%
% Why use such a complex algorithm?
% \ifcutting Because interleaving \else
% \citet{lerner-grove-chambers:2002} write
% \begin{quote}
% \emph{Previous efforts to exploit [the mutually beneficial
% interactions of dataflow analyses] either (1)~iteratively performed
% each individual analysis until no further improvements are discovered
% or (2)~developed [handwritten] ``super-analyses'' that manually
% combine conceptually separate analyses. We have devised a new approach
% that allows analyses to be defined independently while still enabling
% them to be combined automatically and profitably. Our approach avoids
% the loss of precision associated with iterating individual analyses
% and the implementation difficulties of manually writing a
% super-analysis.}
% \end{quote}



%%  \simon{But do we apply rewrites even before the analysis reaches a fixed point?
%%  If so, what property do the rewrites have to satisfy to ensure soundness?
%%  If not, even a single rewrite might destroy the fixed-point property of the
%%  current facts.  Or perhaps we iterate the analysis to a fixpoint, and only \emph{then}
%%  do rewriting? If so, do we need the transfer functions at that stage?
%%  
%%  Also the fixed-point of the analysis relies on upward chains. What if
%%  the rewrite pushed it downward?  Or is it the case that a rewrite must
%%  change a node $n$ into a graph $g$ so 
%%  that $\mathit{fwdtrans}(n) \leq \mathit{fwdtrans}(g)$?
%%  
%%  Also the fixpoint calculation requires multiple passses; do the 
%%  rewrites then apply multiple times?
%%  
%%  I'm deliberately playing the role of the reader here, and not peeking at
%%  the code.  I don't think it's enough to say ``go look at Chambers paper''; 
%%  I suggest we say enough (half a column would do it) to address the obvious
%%  questions and point to Chambers for details.
%%  
%%  
%%  \textbf{NR}: Good questions, but let's have a forward reference to \secref{dfengine}}

% Interleaving
% \fi
%  analysis with transformation makes it
% possible to implement useful  transformations using startlingly simple
% client code.
% \john{And maybe this is the place to brag that we now have a startlingly simple
% implementation?}

\finalremark{Doesn't the rewrite have to be have the following property:
for a forward analysis/transform, if (rewrite P s) = Just s',
then (transfer P s $\sqsubseteq$ transfer P s').
For backward: if (rewrite Q s) = Just s', then (transfer Q s' $\sqsubseteq$ transfer Q s).
Works for liveness.
``It works for liveness, so it must be true'' (NR).
If this is true, it's worth a QuickCheck property!
}%
\finalremark{Version 2, after further rumination.  Let's define
$\scriptstyle \mathit{rt}(f,s) = \mathit{transform}(f, \mathit{rewrite}(f,s))$.
 Then $\mathit{rt}$ should
be monotonic in~$f$.  We think this is true of liveness, but we are not sure
whether it's just a generally good idea, or whether it's actually a 
precondition for some (as yet unarticulated) property of \ourlib{} to hold.}%

%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixpoint. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }




\section{\ourlib's dataflow engine}
\seclabel{engine}
\seclabel{dfengine}

\delendum{The earlier sections promised that we'd reveal the lies.
Do we?  I see no mention of @HavingSuccessors@ for example, which is rather important
for polymorphism.  Indeed, a subsection on that point might be a good way
to substantiate the claims of the last bullet of the conclusion.}

In Section \ref{sec:making-simple}
we gave a client's-eye view, showing how to use 
\ourlib\ to create analyses and transformations.
The interface is simple, but 
\emph{implementation} of interleaved analysis and rewriting is 
quite complicated.  \citet{lerner-grove-chambers:2002} 
do not describe their implementation at all.  We have made 
at least three attempts to implement the algorithm all of which
were long, hard to understand, and lacked any static open/closed
shape guarantees.  We are not confident that any of them are correct.

In this section we describe our new implementation.  It is short
(about a third of the size of our last attempt), elegant, and offers
strong static shape guarantees.  

\subsection{Overview}

We will concentrate on implementing @analyseAndRewriteFwd@, whose
type signature is in \secref{using-hoopl}.
The implementation is built on the hierarchy of nodes, blocks, and graphs
that we described in \secref{graph-rep}.  For each such thing
we will develop a function of this type:
\begin{code}
type `ARF thing n f 
  = forall e x. Fact e f -> thing e x 
             -> FuelMonad (Fact x f, RG n e x)
\end{code}
An @ARF@ (short for ``analyse and rewrite forward'') is a combination of
a rewrite and transfer function, and is also 
generalized over nodes, blocks, and partial graphs.
An @ARF@ takes an input fact and a @thing@,
and returns a correspondingly-shaped output fact, and a rewritten graph of type @(RG n e x)@.
Here we intend ``@thing@'' to range over nodes, blocks, and graphs, thus:
\begin{code}
  type `ARF_Node  n f = ARF n         n f
  type `ARF_Block n f = ARF (Block n) n f
  type `ARF_Graph n f = ARF (Graph n) n f
\end{code}
Note that the result is a rewritten graph, 
regardless of whether @thing@ is a node, block, or graph.

We return a rewritten graph @RG@ rather than a @Graph@ 
for two reasons:
\begin{itemize}
\item The client is often interested not only in the facts flowing
out of the graph (which are returned in the @Fact x f@), 
but also in the facts on the \emph{internal} blocks
of the graph. A rewritten graph, of type @(RG n e x)@ is decorated with
these internal facts.
\item A @Graph@ has deliberately restrictive invariants; for example,
a @GMany@ with a @OpenLink@ is always open at exit (\figref{graph}).  It turns
out to be awkward to maintain these invariants \emph{during} rewriting,
but easy to restore them \emph{after} rewriting by ``normalizing'' an @RG@.
\end{itemize}
The normalization function expresses the above two points, by splitting 
a rewritten graph into a @Graph@ and its corresponding @FactBase@:
\begin{code}
`normalizeBody :: Edges n => RG n f C C 
               -> (Body n, FactBase f)
\end{code}
We can now build a modular implementation as follows:
\begin{itemize} 
\item From the supplied @FwdTransfer@ and @FwdRewrite@, produce a @ARF_Node@
that transforms nodes (\secref{arf-node}):  
\par{\small
\begin{code}
 arfNode :: Edges n 
         => DataflowLattice f -> FwdTransfer n f
         -> FwdRewrite n f
         -> ARF_Node n f -> ARF_Node n f
\end{code}}
In addition to the transfer and rewrite functions, @arfNode@ takes the function
to analyse and rewrite the rewritten graph.
\item From this @ARF_Node@, produce a @ARF_Block@ that transforms blocks (\secref{arf-block}):
\par{\small
\begin{code}
 arfBlock :: ARF_Node n f -> ARF_Block n f
\end{code}}
 
\item From  @arfBlock@, produce  @arfBody@ that transforms a @Body@:
\par{\small
\begin{code}
 `arfBody :: forall n f. Edges n
         => DataflowLattice f 
         -> ARF_Node n f 
         -> FactBase f -> Body n 
         -> FuelMonad (FactBase f, (Body n, FactBase f))
\end{code}}
The type is less neat than the others because a @Body@ does not have
the same shape as a node, block, or graph.

\item From @arfBody@, produce @arfGraph@ that tranforms graphs (\secref{arf-graph}):
\par{\small
\begin{code}
  `arfGraph :: DataflowLattice f
           -> ARF_Node n f -> ARF_Graph n f
\end{code}}
\end{itemize}
Given these three functions, it is rather easy to write the main analyser:
\par {\small
\begin{code}
analyseAndRewriteFwd
  :: Edges n         
  => DataflowLattice f -> FwdTransfer n f
  -> FwdRewrite n f -> RewritingDepth
  -> FactBase f -> Body n  
  -> FuelMonad (Body n, FactBase f)

`analyseAndRewriteFwd lat tf rw ^depth fb body
 = do { (_, rg) <- arfBody lat arf_node fb body
      ; return rg }
 where 
  arf_node, rec_node :: ARF_Node n f
  `arf_node = arfNode lat tf rw rec_node

  `rec_node = case depth of
              RewriteShallow -> arfNodeNoRW tf
              RewriteDeep    -> arf_node
\end{code}
}
This beautifully compact code does the following:
\begin{itemize}
\item The result is built by applying @arfBody@ to @arf_node@, the 
function to use when @arfBody@ reaches an individual node.
We discard the out-flowing facts, and return just the pair of ht e
rewritten body and its internal facts.
\item In turn @arf_node@ is built by applying @arfNode@ to the client-supplied
transfer and rewrite functions and, crucially, @rec_node@.  
\item @rec_node@ is used by @arfNode@ when analysing and transforming graphs
produced by rewriting.
When deep rewriting is used, the @rec_node@ engine is simply @arf_node@:
straight recursion. However, if shallow rewriting is required, it instead
uses a simpler analyse-only function on the node, built by @arfNodeNoRW@.
We define the latter in \secref{arf-node}.
\end{itemize}
And that is all.  Note the way that the shallow/deep distinction is made
here, and here alone; none of the other functions are aware of the issue.

\subsection{Analysing and rewriting nodes} \seclabel{arf-node}

The core of the interleaved analyse-and-rewrite idea is expressed
by the functions @arfNode@ and @arfNodeNoRW@:
\par {\small
\begin{code}
arfNodeNoRW :: ForwardTranfsers n f
            -> ARF_Node n f
`arfNodeNoRW tf f n
  = return (transfer_fn node f, RGUnit f (BUnit node))

`arfNode lattice transfer_fn rewrite_fn arf_node f node
  = do { mb_g <- withFuel (rewrite_fn node f)
       ; case mb_g of
           Nothing -> arfNodeNoRW transfer_fn f node
      	   Just ^ag -> do { g <- graphOfAGraph ag
      		         ; arfGraph lattice arf_node f g } }
\end{code}}


\subsection{From nodes to blocks} \seclabel{arf-block}

We start with the second task, of ''lifting'' a @ARF_Node@ to a @ARF_Block@:
\remark{This code is inconsistent with \path{Hoopl7.hs}.  What's up?}
\begin{code}
arfBlock :: forall n f. ARF_Node n f -> ARF_Block n f
`arfBlock (ARF node_trans) = ARF block_trans
  where 
    `block_trans :: ARFR (Block n) n f -- no such type as ARFR ?!
    block_trans (BUnit n)   f 
      = node_trans n f
    block_trans (BCat l r) f 
      = do { (g1,f1) <- block_trans l f
           ; (g2,f2) <- block_trans r f1
	   ; return (g1 `pCat` g2, f2) }
\end{code}
The code is childishly simple.  In the @BUnit@ case 
we appeal to the node-level @ARF_Node@ tranform.
For @BCat@ we recursively apply @block_trans@ to the two
sub-blocks, threading the output fact from the first as the 
input to the second.  Each will produce a rewritten @Graph@, 
which we connect together in sequence with @pCat@. 

\subsection{Rewriting graphs} \seclabel{arf-graph}

Next we tackle ... \simon{not finished}



\section{What we learned}

\simon{I think this section has interesting material, but it's
incomprehensible at the beginning, because our readers won't have the 
mental scaffolding to grok it.  Norman, have at it!}

\john{Is it just me, or have we lost all the text claiming that analyses should
  just be strongest postconds/ weakest preconds? Is this by design?}
\simon{I did remove some material of that sort.  I think I did so because it didn't
seem (to me) to contribute much at the place it then appeared. But I am entirely open
to resurrecting it if you think that would help.  Perhaps in the section on 
correctness, which follows next?  Can you do that?}

One story of this paper is that we can think similar thoughts about
assembly code, IR nodes, basic blocks, and control-flow graphs.
Morever these thoughts are reflected in the types.
Observations:
\begin{itemize}
\item
In compilation and optimization, things are always easiest to analyze
when there is a single entry and single exit.  (Lesson of structured
programming.)
\item
In assembly code, we introduce labels (not present in the hardware) to
be able to code what we mean by branch targets.
\item
In a control-flow graph, unlike in assembly code, \emph{every} edge
out of a control-transfer instruction is associated with a label.
This invariant simplifies the representation and gives us freedom to
lay out assembly code in an order that, e.g., is good for the I-cache,
or that minimizes branch instructions on hot paths.  (Foreshadow that
blocks in our @Graph@ body are unordered.
\item
We distinguish labels, computational instructions, control transfers.
They are distinguished by the number of predecessors and successors.
We~express this distinction in Haskell by attaching to each
entry and exit point one of two types: \emph{open} means there is exactly
one predecessor or successor, and control can ``fall through;''
\emph{closed} means that control cannot fall through, but can be
transferred only by an explicit branch---but the \emph{number} of
successors or predecessors is not constrained \emph{a~priori}.
\item
The open/closed concepts lifts to sequences of straight-line code and
to full control-flow graphs.
As~a special case, a sequence of straight-line code that is closed at
both ends---beginning with a label and ending with a control
transfer---is the familiar \emph{basic block}.
\item
By making open/closed part of the types, we can ensure that a client
of \ourlib{} builds only sequences and graphs that respect our
invariants. 
Moreover, the types provide guidance (really???)
\end{itemize}


A second story of this paper is that we can think of dataflow analysis
as a specialization of program logic.
\begin{itemize}
\item
A forward dataflow analysis is represented by a predicate transformer
that is related to \emph{strongest postconditions}
\cite{floyd:meaning}.\footnote
{In Floyd's paper the output of the predicate transformer is called
the \emph{strongest verifiable consequent}, not the ``strongest
postcondition.''} 
\item
A backward dataflow analysis is represented by a predicate transformer
that is related to \emph{weakest preconditions} \cite{dijkstra:discipline}.
\end{itemize}
A~client of \ourlib{} defines a type of node, a type of predicate
(aka dataflow fact), and a function which, given a node, returns a
predicate transformer.
The beauty of our approach is that we can \emph{lift} a client's
predicate transformer from nodes into sequences and control-flow
graphs.
Moreover, with help from the client, we can iterate the transformer
over an entire function body
until it reaches a fixed point, the result of which is a dataflow
analysis.

In addition, we can do Lerner/Grove/Chambers.

Finally, our implementation has some especially wonderful Haskell
(Simon please flesh out these bits):
\begin{itemize}
\item
Our open/closed tags aren't just phantom types.  By virtue of GADTs,
they are real. 
And we get exhaustiveness checking, etc --- the works.
\item
There's more!  By using higher-rank polymorphism, we can define a
\emph{single} kind of widget, then instantiate it at nodes, blocks,
graphs.
SO WHAT?  This is wonderful because\ldots
\item
More wonderfulness?
\end{itemize}

\section {Related work} \seclabel{related}

\simon{I've moved this section to near the end}

While dataflow analysis and optimization are covered
by a vast literature, 
\emph{design} of optimizers, the topic of this paper, is covered
relatively sparsely.
We therefore focus on the foundations of dataflow analysis,
and on the implementations of comparable dataflow frameworks.

\paragraph{Foundations}

When transfer functions are monotone and lattices are finite in height,
iterative dataflow analysis converges to a fixed point
\cite{kam-ullman:global-iterative-analysis}. 
If~the lattice's join operation distributes over transfer
functions,
this fixed point is equivalent to a join-over-all-paths solution to
the recursive dataflow equations
\cite{kildall:unified-optimization}.\footnote
{Kildall uses meets, not joins.  
Lattice orientation is conventional, and conventions have changed.
We use Dana Scott's
orientation, in which higher elements carry more information.}
\citet{kam-ullman:monotone-flow-analysis} generalize to some
monotone functions.
Each~client of \hoopl\ must guarantee monotonicity,
but for transfer functions that
approximate weakest preconditions or strongest postconditions,
monotonicity falls out naturally.

\ifcutting
\citet{cousot:abstract-interpretation:1977}
\else
\citet{cousot:abstract-interpretation:1977,cousot:systematic-analysis-frameworks}
\fi
introduce abstract interpretation as a technique for developing
lattices for program analysis.
\citet{schmidt:data-flow-analysis-model-checking} shows that
an all-paths dataflow problem can be viewed as model checking an
abstract interpretation.

The soundness of interleaving analysis and transformation,
even when some speculative transformations are not performed on later
iterations, was shown by
\citet{lerner-grove-chambers:2002}.
\ifcutting\else
Muchnick \citeyearpar{muchnick:compiler-implementation} 
presents many examples of both particular analyses and related
algorithms.
\fi

\paragraph{Frameworks}
The Soot framework is designed for analysis and optimization of Java programs
\cite{hendren:soot:2000}.
\john{This citation is probably the best for Soot in general, but there doesn't appear
 to be any formal publication that actually details the dataflow framework part.}
Soot provides a dataflow library that is abstracted over the representation of
the control-flow graph and the representation of instructions.
The interface for defining lattice and analysis functions is similar to our own,
with additional functions required to copy lattice elements (Soot is implemented
in an imperative style) and to invoke the analysis.
But unlike \ourlib, Soot cannot interleave analyses and transformations.

The CIL toolkit \cite{necula:cil:2002}\john{Again, no good citation for same reason}
provides a dataflow framework for optimizing C~programs.
The framework is specialized to a specific control-flow graph
and a specific definition of instructions.
The interface for the client is fairly complicated,
with the evident goal of allowing the client to affect which instructions
the analysis will iterate over.
Like Soot, CIL cannot interleave analyses and transformations.

\john{FYI, LLVM has Pass Managers that try to control the order of passes,
  but I'll be darned if I can find anything that might be termed a dataflow framework.}

The Whirlwind compiler contains the dataflow framework implemented
by \citet{lerner-grove-chambers:2002}.
Their implementation is much like our early efforts:
it is a complicated mix of code that manages the interleaving,
the deep rewriting, and the fixpoint computation.
Our implementation simplifies the problem dramatically
by~separating each of these tasks.

Because speculative transformation is difficult with an imperative implementation,
the implementation in Whirlwind is split into two separate phases.
The first phase runs the interleaved analyses and transformations
to compute the final dataflow facts and a representation of the transformations
that should be applied to the input graph.
Then, a subsequent phase executes the transformations.
With our immutable representation, tentative transformations
can be applied aggressively, and there is no need
for a phase distinction.

\ourlib\ also improves upon Whirlwind's dataflow framework by providing
new support for the optimization-writer:
\begin{itemize}
\item Using static type guarantees, we~rule out a whole
  class of possible bugs: transformations that produced malformed
  control-flow graphs.
\item Using dynamic testing,
  we can isolate the rewrite that transforms a working program
  into a faulty program,
  using Whalley's fault-isolation technique \cite{whalley:isolation}.
\end{itemize}

In our previous work we described a zipper-based representation of control-flow
graphs \cite{ramsey-dias:applicative-flow-graph}, stressing the advantages
of immutability for writing dataflow optimizers.
Our new representation, described in \secref{graph-rep}, is a major improvement:
\begin{itemize}
\item
We can concatenate nodes, blocks, and graphs, in constant time.
Previously, we had to resort to Hughes's
\citeyearpar{hughes:lists-representation:article} technique, representing
a graph as a function.
\item
Most important, errors in concatenation are ruled out at
compile-compile time by Haskell's static
type system.
In~earlier implementations, such errors were not detected until
the compiler~ran, at which point \ourlib\ tried to compensate
for the errors
\ifcutting---but
\else
by inserting branch instructions and then issued a warning message.
But
\fi
the compensation code harbored subtle faults%
\ifcutting\else, which were discovered while developing a new back end
for~GHC\fi. 
\hfuzz=1.6pt % don't want to reword ``errors in concatenation''
\item
There is only one type of node in a control-flow graph,
and we use GADTs to express the differences between
first, middle, and last nodes.
The main benefit is that higher-order functions for manipulating
graphs (e.g. transfer and rewrite functions)
now require only one function argument instead of three.
\end{itemize}

The implementation of \ourlib\ is also a dramatic improvement
over our earlier implementations.
Not only is the code conceptually simpler,
but it is also shorter:
our new implementation is about half as long
as the previous version currently available in the Glasgow Haskell Compiler.



\section{Conclusions}

\john{Lots of obsolete claims here: continuations, monad, separate node types,
HavingSuccessors.}
Compiler textbooks make dataflow optimization appear
difficult and complicated.
In~this paper, we show how to engineer a library, \ourlib, which makes
it easy to build analyses and transformations based on dataflow.
\ourlib\ makes dataflow simple not by using a single magic
ingredient, but by applying ideas that are well understood by 
the programming-language community.
\begin{itemize}
\item
We acknowledge only one program-analysis technique: the solution of
recursion equations over assertions.
%Like our colleagues working in imperative languages, 
We solve the equations by iterating to a fixed point.
% Many equations relate
% properties of program states; some relate properties of paths through
% programs. 
\item
We consider only two
ways of relating assertions: weakest liberal precondition and strongest 
postcondition, which
 correspond 
%\ifpagetuning\else
%respectively
%\fi
to
\ifcutting
``backward'' and ``forward'' dataflow
problems.
\else
``backward dataflow problems'' and ``forward dataflow
problems.''
\fi
\finalremark
{Can we give an example of a property of program states which is
neither, just by way of contrast; ie this we cannot do.}
%%  \item
%%  In a language that admits loops, iterating weakest preconditions or
%%  strongest postconditions typically does not reach a fixed point in
%%  finitely many steps; hence the need for loop invariants
%%  \cite{hoare:axiomatic-basis,dijkstra:discipline,gries:science-programming}.
%%  In \secref{logic-reconciled}, we show that we can guarantee to reach a
%%  fixed point by limiting what we can express in the logic.\remark{needs
%%  a fix}
%%  We~show that many classic analyses can be explained this way;
%%  the great diversity of classic analyses corresponds to a great
%%  diversity of inexpressive logics.
%%  This view leads us to a unifying principle:
%%  \emph{To implement a code-improving transformation, find the least
%%    expressive logic that can justify the transformation, then use that
%%    logic to compute strongest postconditions, which justify the
%%    transformation locally.}
\item
Although our implementation allows graph nodes to be rewritten in any
way that preserves semantics, we describe
three program-transformation techniques:
substitution of equals for equals, 
insertion of assignments to unobserved variables, 
and removal of assignments to unobserved variables
(\secref{example:transforms}).
Substitution of equals for equals is often justified by properties of program
states; for example, if variable~$x$
is always~7, we may substitute~7 for~$x$.\finalremark
{We can also justify substitution of \emph{labels} in goto
  statements by reasoning about continuations.  This is
  probably not the place to mention this fact.}
Insertion and removal of assignments are often justified by properties
of paths through programs;
for example, if an assignment's continuation does not use the variable
assigned~to, that assignment may be removed.

%%  Some compiler texts treat the removal of unreachable code as a
%%  code-improving transformation in its own right.
%%  In~our framework, unreachable code becomes unreachable in the
%%  garbage-collection sense, so no special effort is required to remove
%%  it.
\item
Complex program transformations should be composed from simple
transformations. 
For example, both ``code motion'' and ``induction-variable
elimination'' can be implemented in three stages: insert new assignments;
substitute equals for equals; remove unneeded assignments
(\secref{induction-var-elim}). 

\item 
Because each rewrite leaves the semantics
of the program unchanged, 
we can use 
``optimization fuel'' to limit the number of rewrites.
 When we isolate a fault 
\ifcutting\else in the optimizer \fi
(\secref{vpoiso}), we 
\ifcutting have to debug just \else therefore have the luxury of debugging \fi
 a single
 rewrite, not a complex transformation.
\end{itemize}

We also build on proven implementation techniques
in a way that
makes it easy
to implement classic code improvements.
\begin{itemize}
\item
We use the algorithm of \citet{lerner-grove-chambers:2002} to 
compose analyses and transformations.
This~algorithm makes it easy to compose complex transformations
from simple ones.

Using continuation-passing style and generalized algebraic data types,
we have created a new implementation%
\ifcutting\else\ of the algorithm\fi, 
which works by 
composing three relatively simple functions
(\secref{forward-iterator}). 
The functions are simple
because the static type of a node constrains the number of predecessors
and successors it may have.
And because we can
compare our code with a standard continuation semantics, we have more
confidence in this new implementation than in 
any previous implementation. 
\item
Our code is pure.
Inspired by Huet's~\citeyearpar{huet:zipper} zipper,
we use an applicative representation of
control-flow graphs
\cite{ramsey-dias:applicative-flow-graph}. 
We~improve on our prior work by storing changing dataflow facts
in an explicit dataflow monad,
which
makes it especially easy to implement such
operations as sub-analysis of a replacement graph
(\secref{dataflow-monad});
by using static types to guarantee that each replacement graph can be
spliced in place of the node it replaces
(\secreftwo{subgraphs}{rewrite-functions});
and by simplifying our implementation using continuation-passing style
(\secreftwo{forward-iterator}{forward-actualizer}). 
%
% important, but no longer mentioned in this paper:
%
%%  \item
%%  To \emph{construct} programs, we use a different representation of
%%  flow graphs, one which hides the complexity of the zipper and which
%%  provides a constant-time operation for joining flow graphs in
%%  sequence.
%%  It is inspired in part by Hughes's \citeyearpar{hughes:novel-lists}
%%  representation of lists, which supports a constant-time append operation.
\item
\ourlib\ is polymorphic in the
representations of 
assignments and control-flow operations.
%%  Although our polymorphic representations have been instantiated only
%%  with the low-level intermediate code used by the Glasgow Haskell
%%  Compiler, they are intended eventually to be instantiated with
%%  machine-dependent representations of target-machine instructions, as
%%  part of a larger project of refactoring GHC's back ends.
%
%This design seems obvious in retrospect,
%but we underestimated the degree to which polymorphism would force us to
%separate concerns.
%Introducing polymorphism has made the code simpler, easier
%to understand, and easier to maintain.
By forcing us to separate concerns, introducing polymorphism
made the code simpler, easier to understand, and easier to maintain.
\finalremark
{SLPJ: Is it possible to substantiate this claim by [more] examples?}
In particular, it forced us to make explicit \emph{exactly} what
\ourlib\ 
 must know about flow-graph nodes:
it must be able to find
targets of control-flow operations (constraint
@HavingSuccessors l@, \secref{zdfSolveFwd}).
\end{itemize}
%
% gen and kill are history
%
%%\item
%%Judicious use of Haskell type classes makes is possible to write
%%weakest precondition or strongest postcondition using the ``transfer
%%equations'' that are familiar from compiler textbooks.
%%If you like, you can even write overloaded @gen@ and @kill@ functions.
%%The benefit is that it is easy to compare the actual code with the
%%abstract treatments found in textbooks\ifgenkill \ (\secref{gen-kill})\fi.
Using \ourlib,
you can create a new code improvement in three steps:
create a lattice representation for the assertions you want to
express;
create transfer functions that approximate weakest preconditions or
strongest postconditions;
and 
create rewrite functions that use your assertions to justify
program transformations.  
You can get quickly to the real 
intellectual work of code improvement: identifying interesting
transformations and the assertions that justify them.

\finalremark{Don't forget acknowledgements!!!
Microsoft, Intel, NSF
}

\makeatother

\providecommand\includeftpref{\relax} %% total bafflement -- workaround
\IfFileExists{nrbib.tex}{\bibliography{cs,ramsey}}{\bibliography{cs,ramsey,simon,jd}}
\bibliographystyle{plainnatx}


\clearpage

\appendix


\section{Index of defined identifiers}

This appendix lists every nontrivial identifier used in the body of
the paper.  
For each identifier, we list the page on which that identifier is
defined or discussed---or when appropriate, the figure (with line
number where possible).
For those few identifiers not defined or discussed in text, we give
the type signature and the page on which the identifier is first
referred to.

Some identifiers used in the text are defined in the Haskell Prelude;
for those readers less familiar with Haskell, these identifiers are
listed in Appendix~\ref{sec:prelude}.

\newcommand\dropit[3][]{}

\newcommand\hsprelude[2]{\noindent
  \texttt{#1} defined in the Haskell Prelude\\}
\let\hsprelude\dropit

\newcommand\hspagedef[3][]{\noindent
  \texttt{#2} defined on page~\pageref{#3}.\\}
\newcommand\omithspagedef[3][]{\noindent
  \texttt{#2} not shown (but see page~\pageref{#3}).\\}
\newcommand\omithsfigdef[3][]{\noindent
  \texttt{#2} not shown (but see Figure~\ref{#3} on page~\pageref{#3}).\\}
\newcommand\hsfigdef[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} defined in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hstabdef[3][]{%
  \noindent
  \ifx!#1!
    \texttt{#2} defined in Table~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Table~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hspagedefll[3][]{\noindent
  \texttt{#2} {let}- or $\lambda$-bound on page~\pageref{#3}.\\}
\newcommand\hsfigdefll[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} {let}- or $\lambda$-bound in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} {let}- or $\lambda$-bound on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    

\newcommand\nothspagedef[3][]{\notdefd\ndpage{#1}{#2}{#3}}
\newcommand\nothsfigdef[3][]{\notdefd\ndfig{#1}{#2}{#3}}
\newcommand\nothslinedef[3][]{\notdefd\ndline{#1}{#2}{#3}}

\newcommand\ndpage[3]{\texttt{#2}~(p\pageref{#3})}
\newcommand\ndfig[3]{\texttt{#2}~(Fig~\ref{#3},~p\pageref{#3})}
\newcommand\ndline[3]{%
  \ifx!#1!%
      \ndfig{#1}{#2}{#3}%
  \else
      \texttt{#2}~(Fig~\ref{#3}, line~\lineref{#1}, p\pageref{#3})%
  \fi
}

\newif\ifundefinedsection\undefinedsectionfalse

\newcommand\notdefd[4]{%
  \ifundefinedsection
    , #1{#2}{#3}{#4}%
  \else
    \undefinedsectiontrue
    \par
    \section{Undefined identifiers}
    #1{#2}{#3}{#4}%
  \fi
}

\begingroup
\raggedright

\input{defuse}%
\ifundefinedsection.\fi

\undefinedsectionfalse


\renewcommand\hsprelude[2]{\noindent
  \ifundefinedsection
    , \texttt{#1}%
  \else
    \undefinedsectiontrue
    \par
    \section{Identifiers defined in Haskell Prelude}\label{sec:prelude}
    \texttt{#1}%
  \fi
}
\let\hspagedef\dropit
\let\omithspagedef\dropit
\let\omithsfigdef\dropit
\let\hsfigdef\dropit
\let\hstabdef\dropit
\let\hspagedefll\dropit
\let\hsfigdefll\dropit
\let\nothspagedef\dropit
\let\nothsfigdef\dropit
\let\nothslinedef\dropit

\input{defuse}
\ifundefinedsection.\fi



\endgroup


\iffalse

\section{Dataflow-engine functions}


\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward iterator}
\end{figure*}

\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward actualizer}
\end{figure*}


\fi



\end{document}



% Old captions' text:
% The dataflow fact for the available-reload analysis describes
%   the set of registers for which a reload is available.
%   We list the types of the functions that manipuate sets of available registers,
%   as well as the definition of the lattice.
% The standard gen and kill functions for available expressions
% The transfer functions for the available-reloads analysis.
% Running the available-reloads analysis and extracting the results with \texttt{zdfFpFacts}
% The rewrite functions to insert redundant reloads immediately before uses

% Probably no space for the implementations:
% interAvail (UniverseMinus s) (UniverseMinus s') =
%   UniverseMinus (s `plusVarSet`  s')
% interAvail (AvailVars     s) (AvailVars     s') =
%   AvailVars (s `timesVarSet` s')
% interAvail (AvailVars     s) (UniverseMinus s') =
%   AvailVars (s  `minusVarSet` s')
% interAvail (UniverseMinus s) (AvailVars     s') =
%   AvailVars (s' `minusVarSet` s )
% 
% smallerAvail (AvailVars _) (UniverseMinus _) = True
% smallerAvail (UniverseMinus _) (AvailVars _) = False
% smallerAvail (AvailVars     s) (AvailVars    s')  =
%   sizeVarSet s < sizeVarSet s'
% smallerAvail (UniverseMinus s) (UniverseMinus s') =
%   sizeVarSet s > sizeVarSet s'
% 
% extendAvail (UniverseMinus s) r =
%   UniverseMinus (deleteFromVarSet s r)
% extendAvail (AvailVars     s) r =
%   AvailVars (extendVarSet s r)
% 
% delFromAvail (UniverseMinus s) r =
%   UniverseMinus (extendVarSet s r)
% delFromAvail (AvailVars     s) r =
%   AvailVars (deleteFromVarSet s r)
% 
% elemAvail (UniverseMinus s) r =
%   not $ elemVarSet r s
% elemAvail (AvailVars     s) r =
%   elemVarSet r s



THE FUEL PROBLEM:


Here is the problem:

  A graph has an entry sequence, a body, and an exit sequence.
  Correctly computing facts on and flowing out of the body requires
  iteration; computation on the entry and exit sequences do not, since
  each is connected to the body by exactly one flow edge.

  The problem is to provide the correct fuel supply to the combined
  analysis/rewrite (iterator) functions, so that speculative rewriting
  is limited by the fuel supply.

  I will number iterations from 1 and name the fuel supplies as
  follows:

     f_pre      fuel remaining before analysis/rewriting starts
     f_0        fuel remaining after analysis/rewriting of the entry sequence
     f_i, i>0   fuel remaining after iteration i of the body
     f_post     fuel remaining after analysis/rewriting of the exit sequence

  The issue here is that only the last iteration of the body 'counts'.
  To formalize, I will name fuel consumed:

     C_pre      fuel consumed by speculative rewrites in entry sequence
     C_i        fuel consumed by speculative rewrites in iteration i of body
     C_post     fuel consumed by speculative rewrites in exit sequence

  These quantities should be related as follows:

     f_0    = f_pre - C_pref
     f_i    = f_0 - C_i            where i > 0
     f_post = f_n - C_post         where iteration converges after n steps

When the fuel supply is passed explicitly as parameter and result, it
is fairly easy to see how to keep reusing f_0 at every iteration, then
extract f_n for use before the exit sequence.  It is not obvious to me
how to do it cleanly using the fuel monad.


Norman
