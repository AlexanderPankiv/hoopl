%%  advances over prior work
%%  
%%  (if open/closed) -- detect bad splices at compile time
%%  constant-time access to exit sequence
%%  (indeed exit  node)
%%  
%%  directly splicable in constant amortized time (vs Hughes
%%  technique)
%%  
%%  polymorphic in node type (type classes)
%%  
%%  



\iffalse % submission abstract

We present Hoopl, a Haskell library that makes it easy for compiler
writers to implement program transformations based on dataflow
analyses. The compiler writer must identify (a) logical assertions on
which the transformation will be based; (b) a representation of such
assertions, which should form a lattice of finite height; (c) transfer
functions that approximate weakest preconditions or strongest
postconditions over the assertions; and (d) rewrite functions whose
soundness is justified by the assertions. Hoopl uses the algorithm of
Lerner, Grove, and Chambers (2002), which can compose very simple
analyses and transformations in a way that achieves the same precision
as complex, handwritten "superanalyses." Hoopl will be the workhorse
of a new back end for the Glasgow Haskell Compiler (version 6.12,
forthcoming).

Because the major claim in the paper is that Hoopl makes it easy to
implement program transformations, the paper is filled with examples,
which are written in Haskell.  The paper also sketches the
implementation of Hoopl, including some excerpts from the
implementation. 

\fi


\IfFileExists{timestamp.tex}{\input{dfoptdu.tex}}{}
     % If you are Simon, run un-preprocessed code
     % It not Simon, use version with def-use information

\newif\ifpagetuning \pagetuningtrue  % adjust page breaks

\newif\ifnoauthornotes \noauthornotesfalse
\newif\iftimestamp\timestamptrue  % show MD5 stamp of paper

\timestampfalse % it's submission time

\IfFileExists{timestamp.tex}{}{\timestampfalse}

\newif\ifcutting \cuttingfalse % cutting down to submission size


\newif\ifgenkill\genkillfalse  % have a section on gen and kill
\genkillfalse


\newif\ifnotesinmargin \notesinmarginfalse
% \IfFileExists{notesinline.tex}{\notesinmarginfalse}{\relax}

\documentclass[blockstyle,preprint,natbib,nocopyrightspace]{sigplanconf}

\newcommand\ourlib{Hoopl}
   % higher-order optimization library
   % ('Hoople' was taken -- see hoople.org)
\let\hoopl\ourlib

% l2h substitution ourlib Hoopl
% l2h substitution hoopl Hoopl

\newcommand\fs{\ensuremath{\mathit{fs}}} % dataflow facts, possibly plural

\newcommand\vfilbreak[1][\baselineskip]{%
  \vskip 0pt plus #1 \penalty -200 \vskip 0pt plus -#1 }

\usepackage{alltt}
\usepackage{array}
\newcommand\lbr{\char`\{}
\newcommand\rbr{\char`\}}
 
\clubpenalty=10000
\widowpenalty=10000

\usepackage{verbatim} % allows to define \begin{smallcode}
\newenvironment{smallcode}{\par\unskip\small\verbatim}{\endverbatim}

\newcommand\lineref[1]{line~\ref{line:#1}}
\newcommand\linepairref[2]{lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\linerangeref[2]{\mbox{lines~\ref{line:#1}--\ref{line:#2}}}
\newcommand\Lineref[1]{Line~\ref{line:#1}}
\newcommand\Linepairref[2]{Lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\Linerangeref[2]{\mbox{Lines~\ref{line:#1}--\ref{line:#2}}}

\makeatletter

\let\c@table=
           \c@figure % one counter for tables and figures, please

\newcommand\setlabel[1]{%
  \setlabel@#1!!\@endsetlabel
}
\def\setlabel@#1!#2!#3\@endsetlabel{%
  \ifx*#1*% line begins with label or is empty
     \ifx*#2*% line is empty
        \verbatim@line{}%
     \else
       \@stripbangs#3\@endsetlabel%
       \label{line:#2}%
     \fi
  \else
     \@stripbangs#1!#2!#3\@endsetlabel%
  \fi
}
\def\@stripbangs#1!!\@endsetlabel{%
  \verbatim@line{#1}%
}


\verbatim@line{hello mama}

\newcommand{\numberedcodebackspace}{0.5\baselineskip}

\newcounter{codeline}
\newenvironment{numberedcode}
  {\endgraf
     \def\verbatim@processline{%
        \noindent
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
               %{\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\phantom{: \,}}}%
            \else
               \refstepcounter{codeline}%
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\llap{\arabic{codeline}}: \,}}%
            \fi
        \expandafter\setlabel\expandafter{\the\verbatim@line}%
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
          \vspace*{-\numberedcodebackspace}\par%
        \else
          \the\verbatim@line\par
        \fi}%
   \verbatim
   }
   {\endverbatim}

\makeatother

\newcommand\arrow{\rightarrow}

\newcommand\join{\sqcup}
\newcommand\slotof[1]{\ensuremath{s_{#1}}}
\newcommand\tempof[1]{\ensuremath{t_{#1}}}
\let\tempOf=\tempof
\let\slotOf=\slotof

\makeatletter
\newcommand{\nrmono}[1]{%
  {\@tempdima = \fontdimen2\font\relax
   \texttt{\spaceskip = 1.1\@tempdima #1}}}
\makeatother

\usepackage{times}  % denser fonts
\renewcommand{\ttdefault}{aett} % \texttt that goes better with times fonts
\usepackage{enumerate}
\usepackage{url}
\usepackage{graphicx}
\usepackage{natbib}  % redundant for Simon
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar

\usepackage[ps2pdf,bookmarksopen,breaklinks,pdftitle=dataflow-made-simple]{hyperref}

\newcommand\naive{na\"\i ve}
\newcommand\Naive{Na\"\i ve}

\usepackage{amsfonts}
\newcommand\naturals{\ensuremath{\mathbb{N}}}
\newcommand\true{\ensuremath{\mathbf{true}}}
\newcommand\implies{\supseteq}  % could use \Rightarrow?

\newcommand\PAL{\mbox{C{\texttt{-{}-}}}}
\newcommand\high[1]{\mbox{\fboxsep=1pt \smash{\fbox{\vrule height 6pt
   depth 0pt width 0pt \leavevmode \kern 1pt #1}}}}

\usepackage{tabularx}

%%
%% 2009/05/10: removed 'float' package because it breaks multiple
%% \caption's per {figure} environment.   ---NR
%%
%%  % Put figures in boxes --- WHY??? --NR
%%  \usepackage{float}
%%  \floatstyle{boxed}
%%  \restylefloat{figure}
%%  \restylefloat{table}



% ON LINE THREE, set \noauthornotestrue to suppress notes (or not)

%\newcommand{\qed}{QED}
\ifnotesinmargin
  \long\def\authornote#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}
\else
  % Simon: please set \notesinmarginfalse on the first line
  \long\def\authornote#1{{\em #1\/}}
\fi
\ifnoauthornotes
  \def\authornote#1{\unskip\relax}
\fi

\newcommand{\simon}[1]{\authornote{SLPJ: #1}}
\newcommand{\norman}[1]{\authornote{NR: #1}}
\let\remark\norman
\def\finalremark#1{\relax}
% \let \finalremark \remark % uncomment after submission
\newcommand{\john}[1]{\authornote{JD: #1}}
\newcommand{\todo}[1]{\textbf{To~do:} \emph{#1}}
\newcommand\delendum[1]{\relax\ifvmode\else\unskip\fi\relax}

\newcommand\secref[1]{Section~\ref{sec:#1}}
\newcommand\secreftwo[2]{Sections \ref{sec:#1}~and~\ref{sec:#2}}
\newcommand\seclabel[1]{\label{sec:#1}}

\newcommand\figref[1]{Figure~\ref{fig:#1}}
\newcommand\figreftwo[2]{Figures \ref{fig:#1}~and~\ref{fig:#2}}
\newcommand\figlabel[1]{\label{fig:#1}}

\newcommand\tabref[1]{Table~\ref{tab:#1}}
\newcommand\tablabel[1]{\label{tab:#1}}


\newcommand{\CPS}{\textbf{StkMan}}    % Not sure what to call it.


\usepackage{code}   % At-sign notation

\iftimestamp
\input{timestamp}
\preprintfooter{\mdfivestamp}
\fi

\hyphenation{there-by}

\begin{document}
\title{\ourlib: Dataflow Optimization Made Simple}
%\subtitle{\today}

%\titlebanner{\textsf{\mdseries\itshape Submitted to the 2010 ACM Symposium on Principles
%    of Programming Languages (POPL)}}

\ifnoauthornotes
\makeatletter
\let\HyPsd@Warning=
                \@gobble
\makeatother
\fi

% João


\authorinfo{Norman Ramsey}{Tufts University}{nr@cs.tufts.edu}
\authorinfo{Jo\~ao Dias}{Tufts University}{dias@cs.tufts.edu}
\authorinfo{Simon Peyton Jones}{Microsoft Research}{simonpj@microsoft.com}


\maketitle
 
\begin{abstract}
We present \ourlib, a Haskell library that makes it easy for compiler writers
to implement program transformations based on dataflow analyses.
The compiler writer must identify (a)~logical assertions
on which the transformation will be based;
(b)~a~representation of such assertions, which
\ifcutting
should form a lattice of finite height;
\else
must have a lattice structure
 such that every assertion can be increased at
most finitely many times;
\fi
(c)~transfer functions that approximate weakest preconditions or
strongest postconditions over the assertions; and
(d)~rewrite functions whose soundness is justified by the assertions.
%%  To~guide compiler writers,
%%  we show how dataflow analyses are related to
%%  seminal work on program 
%%  correctness. \simon{The ``next 700'' section sort of does so, but I'm not 
%%  sure it deserves mention in the abstract.}
\ourlib\ uses the algorithm of 
\citet{lerner-grove-chambers:2002}, which 
% enables compiler writers to
can
compose very simple analyses and transformations in a way that achieves
the same precision as complex, handwritten
``super-analyses.''
\ourlib\ will be the workhorse of a new
back end for the Glasgow Haskell Compiler (version~6.12, forthcoming).

\emph{Reviewers:} code examples are indexed at {\small\url{http://bit.ly/jkr3K}}
%%% Source: http://www.cs.tufts.edu/~nr/drop/popl-index.pdf
\end{abstract}

\makeatactive   %  Enable @foo@ notation

\section{Introduction}

If you write a compiler for an imperative language, you can exploit
many years' work on code improvement
(``optimization'').
The work is typically presented
as a long list of analyses and
transformations, each with a different name.
This presentation makes optimization appear complex and difficult.
Another source of complexity is the need for synergistic combinations
of optimizations; you may have to write one ``super-analysis'' per
combination. 

But optimization doesn't have to be complicated.
Most optimizations
work by applying well-understood techniques for
reasoning about programs:
assertions about states, assertions about continuations, and
substitution of equals for equals.  % (\secref{next-700}).
What makes optimization different from classic reasoning techniques
is that in dataflow optimization, assertions are approximated,
and all assertions are computed automatically.

This paper presents \ourlib\ (higher-order optimization library), 
a~Haskell library that makes it easy to implement dataflow optimizations.
Our contributions are as follows:
\begin{itemize}
\item
\ourlib\ defines a simple interface for implementing analyses and transformations:
you provide representations for assertions and for functions that
transform assertions, 
and \ourlib\ computes assertions
 by setting up and solving recursion
equations.
Additional functions you provide use computed
assertions to justify program transformations.
Analyses and transformations built on \ourlib\ 
are small, simple, and easy to get right.
\item
% using LGC, simple implementations of complicated super-analyses using interleaving and speculative
% rewriting
Using the sophisticated algorithm of \citet{lerner-grove-chambers:2002},
%which is \emph{not} easy to get right,
\ourlib\ can perform super-analyses by \emph{interleaving}
simple analyses and transformations.
Interleaving is tricky to implement,
but by using 
generalized algebraic data types 
and continuation-passing style,
our new implementation expresses the algorithm with a clarity and a
degree of 
static checking that has not 
previously been achieved.
\item
\ourlib\ helps you write correct optimizations:
it
statically rules out transformations that violate invariants
of the control-flow graph,
and dynamically it can help find the first transformation that introduces a fault
in a test program \cite{whalley:isolation}.
\item
\ourlib's polymorphic, higher-order design makes it reusable
with many languages.
\hoopl\ is designed to help optimize imperative code
with arbitrary control flow,
including low-level intermediate languages and machine languages.
As \citet{benitez-davidson:portable-optimizer} have shown, all the
classic scalar and loop optimizations can be performed over such
codes.
\end{itemize}






% %Many presentations obscure fundamental principles of
% %code improvement.
% %
% But~most optimizations
% work by applying reasoning techniques that have long been understood
% and used 
% to reason about programs:
% assertions about states, assertions about continuations, and
% substitution of equals for equals.  % (\secref{next-700}).
% % What distinguishes dataflow optimization from classic formal reasoning
% % about programs is that in dataflow optimization, all assertions are
% % computed automatically, and they are
% % \emph{approximated}. 
% 
% This paper presents \ourlib\ (higher-order optimization library), 
% a Haskell library the implements dataflow optimizations using
% simple representations of assertions and the functions that transform
% them.
% Its contributions are as follows:
% %The contribution of this paper is to elucidate a large body of work on code
% %improvement; the body of work known as ``dataflow optimization.''
% %This paper makes two contributions:
% \begin{itemize}
% \item
% \ourlib\ is engineered to make it \emph{easy} to implement
% dataflow-based 
% code-improvement techniques, even in a purely functional setting.
% When built on \ourlib, analyses and transformations
% are small, simple, and easy to get right.
% \item 
% Using the sophisticated algorithm of \citet{lerner-grove-chambers:2002},
%  which is \emph{not} easy to get right,
% \ourlib\ \emph{interleaves} analysis and transformation.
% Our new implementation, which uses generalized
% algebraic data 
% types % \cite{xi:guarded-recursive} 
% and continuation-passing style,
% %By~exploiting Strachey's ideas about compositional semantics and
% %the extra type checking possible with GADTs, we implement
% expresses the algorithm with a clarity and a degree of
% static checking that has not 
% previously been achieved.
% \item
% \ourlib's polymorphic, higher-order design makes it easier to reuse
% these techniques than ever before.
% \end{itemize}
% %
% \hoopl\ is designed to help optimize imperative procedures (after
% inlining). 
% \ourlib\ supports local-level codes with arbitrary control flow,
% including intermediate 
% languages and machine 
% languages.
% As \citet{benitez-davidson:portable-optimizer} have shown, all the
% classic scalar and loop optimizations can be performed over such
% codes.



We introduce dataflow optimization by analyzing and transforming
example code (\secref{example:xforms}),
thinking about and justifying classic optimizations using
Hoare logic and substitution of equals for equals.
To~support our claim that \ourlib\ makes dataflow optimization easy, 
we explain how
to create new dataflow analyses and transformations
(\secref{making-simple}), and we show complete implementations of significant
analyses (\secref{example-analyses}) and transformations
(\secref{example-rewrites}) from the Glasgow Haskell Compiler.
We also sketch a new implementation of interleaving (\secref{engine}).


\section{Dataflow analysis {\&} transformation by \rlap{example}}
\seclabel{constant-propagation}
\seclabel{example:transforms}
\seclabel{example:xforms}

We begin by setting the scene, introducing some vocabulary, and
articulating a small motivating example.
A control-flow graph, perhaps representing the body of a procedure,
is a collection of \emph{basic blocks}
-- or just ``blocks'' -- each of which has a label at the 
beginning.  Each block may branch to other blocks with arbitrarily
complex control flow.
The goal of dataflow optimization is to compute valid
\emph{assertions} (or \emph{dataflow facts}), 
then use those assertions to justify code-improving
transformations (or \emph{rewrites}) on a \emph{control-flow graph}.  

Consider a concrete example: constant propagation.
On the left we have a basic block; in the middle we have
assertions that hold between statements (or \emph{nodes}) 
in the block; and at
the right we have the result of transforming the block 
based on the assertions:
\begin{verbatim}
  Before   	     Facts       After
           	      {}
  x = 3+4  	                 x = 7
           	    {x=7}
  z = x>5  	                 z = False
           	{x=7, z=False}
  if z                           goto L2
   then goto L1
   else goto L2
\end{verbatim}
Constant-propagation works
from top to bottom.  We start with the empty assertion.  
Given the empty assertion and the node @x=3+4@ can we make a (constant-prop)
transformation?
Yes!  We can replace it with @x=7@.  Now, given this transformed node,
and the original assertion, what assertion flows out of the bottom of
the transformed node?  Clearly the assertion \{@x=7@\}.  
Given the assertion \{@x=7@\} and the node @z=x>5@, can we make a
transformation?  Yes: replace it with @z=False@.  And so the process
continues. Notice that in the last node of the block we can transform
a conditional jump to an unconditional one.

Notice that it is very convenient to \emph{interleave}
transformation and analysis.  If we \emph{first} analysed the block
and \emph{then} transformed it, the analysis would have to ``predict''
the transformations.  For example, a pure analysis would have to
evaluate @x>5@ under the assumption that \{@x=7@\} to get the outgoing
fact \{@x=7@, @z=False@\}.  But the subsequent transformation must 
\emph{also} evaluate @x>5@ under the assumption \{@x=7@\}, to transform
to @z=x>5@ to @z=7@!
This duplication of knowledge becomes onerous when the
transformation is complex, or if there are a number of interacting transformations;
we encourage the reader to consult \cite{lerner-grove-chambers} for more substantial
examples.

Constant propagation works \emph{forwards}, and a fact is typically an
assertion about the program state (such as ``variable @x@ holds value
@7@'').  Many useful analyses work \emph{backwards}, from bottom to
top.  An example is live-variable analysis, where the a fact takes the form
``variable @x@ is dead'' and constitute an assertion about the
\emph{continuation} of a program point.  For example, the fact ``@x@ is
dead'' at a program point P is an assertion that @x@ is unused on any program
path starting at P.  The accompanying transformation is called dead-code elimination,
and allows the assignment @x=e@ to be rewritten to a no-op if @x@ is dead.

% ----------------------------------------
\section{Representing control flow graphs} \seclabel{graph-rep}

\ourlib{} is a library that makes it easy to define dataflow analyses,
and transformations driven by these analyses, on control-flow graphs.
In this section we describe the representation of these graphs.
There are three fundamental entities, which we discuss in turn;
\begin{itemize}
\item A \emph{node} is defined by the library client, so that 
\ourlib{} knows nothing about their representation (\secref{nodes}).
\item A (basic) \emph{block} is a linear sequence of nodes (\secref{blocks}).
\item A \emph{graph} is an arbitrarily complicated control flow graph,
composed from basic blocks (\secref{graphs}).
\end{itemize}

\subsection{Nodes} \seclabel{nodes}

The primitive constituents of a \ourlib{} control flow graph are
\emph{nodes}, reprsented by a data type defined by the client.
Typically, a node might represent a machine instruction, such as an
assignment, a call, or a conditional branch.  However, crucially,
\ourlib{}'s graph representation is polymorphic in the node type, so
that graphs can include arbitrary client-specified data, including
(say) C statements, method calls in an object oriented language, or
whatever.

A node may be \emph{open or closed at entry}
and \emph{open or closed at exit}.  
By ``open'' we mean that control may ``fall through'' this path, whereas
that is not allowed in the ``closed'' case.  To take some examples:
\begin{itemize}
\item A multiply instruction is open on entry (because control can fall into it
from the preceding instruction), and open on exit (because control falls through
to the next instruction).
\item An unconditional branch is open on entry, but closed on exit (because 
control cannot fall through to the next instruction).
\item A label is closed on entry (because in \ourlib{} we do not allow
control to fall through into a branch target), but open on exit.
\end{itemize}
This taxonomy embodies certain restrictions, specifically: only nodes
closed at entry can be the targets of branches; and only nodes closed
at exits can transfer control (see also \secref{edges}).
A typical machine-code conditional branch, which transfers control
\emph{and} falls through, must be represented as a two-target
conditional branch, with the fall-through path in a separate block.  
This restriction is quite standard \simon{Please
cite sources}. It dramatically simplifies control-flow graph analyses
and transformations, in practice costs nothing: a simple late-stage
block lineariser can readily reconstruct the efficient code.

\begin{figure}
\begin{code}
data CmmNode e x where
  Label      :: BlockId -> Node C O
  Assign     :: Reg -> Expr -> Node O O
  Store      :: Expr -> Expr -> Node O O
  Branch     :: BlockId -> Node O C
  CondBranch :: BlockId -> BlockId -> Node O C
  ...more constructors...
\end{code}
\caption{A typical node definition} \figlabel{cmm-node}
\end{figure}

\ourlib{} expresses these open/closed properties by parameterising the 
\emph{type} of a node over two (type-level) flags, one for entry and one for exit.
To make this concrete, 
\figref{cmm-node} gives a typical definition of a node type --- but
please remember that this definition is written by the \emph{client} of the
library. The example node type, @CmmNode@, is parameterised @e@ and @x@, for
entry and exit respectively.  It is defined in GADT-style syntax \cite{pj-gadt},
by giving the types of each of of its contructors.  For example, constructor @Label@
takes a @BlockId@ and returns a node of type @CmmNode C O@, where ``@C@'' says ``closed
at entry'' and ``@O@'' says ``open at exit''.  The types @BlockId@, @O@, and @C@ are 
exported by \ourlib{} (\figref{graph}).  

Similarly, an @Assign@ node takes a register and an expression, and
returns a @CmmNode@ open at both entry and exit; the @Store@ node is
similar.  The types @Reg@ and @Expr@ are private to the client, and
\ourlib{} knows nothing of them.  
Finally, the control-transfer nodes @Branch@ and @CondBranch@ are open at entry
and closed at exit.  

In general, nodes closed on entry are the only targets of control transfers;
noces open on entry and exit never perform control transfers;
and nodes closed on exit always perform control transfers.
We often call these \emph{first}, \emph{middle}, and \emph{last} nodes
respectively, for obvious reasons.

\subsection{Blocks} \seclabel{blocks}

\begin{figure}
\begin{code}
data O   -- Open
data C   -- Closed

newtype BlockId = BlockId Int

data Block n e x where
  BNil  :: Block n O O
  BUnit :: n e x -> Block n e x
  BCat  :: Block n e O -> Block n O x -> Block n e x

data Graph n e x where
  GUnit :: Block n e x -> Graph n e x
  GMany :: { g_entry  :: MaybeOC e (Block n O C)
           , g_blocks :: [Block n C C]
           , g_exit   :: MaybeOC x (Block n C O) }
        -> Graph n e x

data MaybeOC oc a where
  Closed :: MaybeOC C a
  Open   :: a -> MaybeOC O a
\end{code}
\caption{The \ourlib{} Block and Graph types} \figlabel{graph}
\end{figure}

So much for nodes.  The types for blocks and graphs are defined by \ourlib{}
itself, with definitions given in \figref{graph}.  \simon{I've removed 
the ``Z'' prefixes.} \simon{I have also added the {\tt BNil} empty block.
It seems jolly useful.}
A @Block@ 
is parameterised over the node type @n@ (of kind @*->*->*@), 
as well as over the same open/closed flags as nodes.  The @BUnit@
constructor lifts a node to become a block, while @BCat@ concatenates
two blocks. Note carefully the type of @BCat@: it requires its first
argument to be open on exit, and its second to be open on entry.
This ensures that control can fall through from the first into the second;
for example, putting a @Branch@ before a @Assign@ is statically prevented.

A @Block@ closed at both entry and exit is what we normally 
call ``a basic block''.  (We will often informally say ``a closed-closed block'' 
rather than ``a block of type (@Block@ @n@ @C@ @C@)'', 
and simlarly for graphs and nodes.)
In the case of @CmmNode@,
the types elegantly ensure that a block of type (@Block@ @CmmNode@ @C@ @C@)
consists of a @Label@, followed
by a sequence of @Assign@ and @Store@ nodes, and finsihing with a @Branch@
or @CondBranch@.  

Finally @BNil@ is a no-op, open at entry and exit.

\subsection{Graphs} \seclabel{graphs}

The third important entity is the @Graph@, also defined by \ourlib{}, in
\figref{graph}.  Just like @Block@, it is parameterised over
the node type @n@, and can be open or closed at entry and exit.
Indeed, the @GUnit@ constructor lifts a single block to be a graph, just
as @BUnit@ lifted a node to a block.   \simon{Since we now have {\tt BNil} 
we don't need an empty graph.  Norman thinks we need a closed-closed empty
graph but I don't see that, at least not yet.}

The excitement comes in the @GMany@ constructor, which allows a graph
to contain an arbitrary non-empty list of @Block@s, each closed at both ends.  The
signature for @GMany@ uses Haskell's named-field syntax to say that
@GMany@ has three argument fields, and to define a name for each.
The @g_blocks@ field contains
the non-empty list\footnote{In our actual implementation it is a bag, so that union
takes constant time.} of closed-closed blocks, but what of @g_entry@
and @g_exit@?

Consider what it means to say that a graph is open at entry.  It means
that the graph begins with an open-closed block, whose final node
transfer control to one of the @g_blocks@. In contrast, a graph closed on entry
has no such distinguished entry block.  This idea of
``nothing if closed, but a open-closed block if open'' is expressed by
the @MaybeOC@ type defined in \figref{graph}.  It is a GADT variant of 
Hakell's existing @Maybe@ type, so that an open-x @Graph@ has
a @g_entry@ field of form @Open b@, where @b@ is an open-closed block;
and a closed-x @Graph@ has a @g_entry@ field of form @Closed@.
The same trick is used at exit.

\simon{In @g\_blocks@ I've used a @[Block n C C]@ instead of a @BlockEnv@.
There's no reason for it to be a finite mapping now.  And from a @Block@
we can get its @BlockId@ with @blockId@.  If we want to be sure that 
that operation is O(1) we could, I suppose, change the rep of @Block@, or
store a @[(BlockId,Block)]@ in @g\_blocks@.}

Now graph concatenation is easy to define:
\begin{code}
gCat :: Graph n e O -> Graph n O x -> Graph n e x
gCat (GUnit b1) (GUnit b2)             
  = GUnit (b1 `BCat` b2)

gCat (GUnit b1) (GMany (Open b2) bs x) 
  = GMany (Open (b1 `BCat` b2)) bs x

gCat (GMany e bs (Open b1)) (GUnit b2) 
   = GMany e bs (Open (b1 `BCat` b2))

gCat (GMany e bs1 (Open b1)) 
     (GMany (Open b2) bs2 x)
   = GMany e (b1 `BCat` b2 : bs1 ++ bs2) x
\end{code}
This defintion is a lovely example of the power of GADTs: the
pattern-matching is exhaustive, and all the open/closed invariants are
statically checked.  For example, in the third equation for @gCat@,
the first argument has type (@Graph@ @n@ @e@ @O@), so the third
field of @GMany@, the @g_exit@ field\footnote{To save space we use positional,
rather than named-field, syntax for @GMany@'s arguments; Haskell allows
one to do either.}, must be of form (@Open b1@).  Moreover @b1@ must be closed-open,
and @b2@ must be open-x.  So we can compose them in sequence with @BCat@ 
to produce a closed-x block, which is the exit block of the compound graph.
The other equations are similar.

\subsection{Labels and successors} \seclabel{edges}

\begin{figure}
\begin{code}
class Edges n where
  blockId    :: n C x -> BlockId
  successors :: n e C -> [BlockId]
\end{code}
\caption{The {\tt Edges} class} \figlabel{edges}
\end{figure}
If \ourlib{} knows nothing about nodes, how can it know to which block a
control transfer goes, or what is the @BlockId@ at the start of a block?
In this situation the standard Haskell idiom is to provide a type class whose methods
provide exactly the needed operations; it is given in \figref{edges}.
The @blockId@ method takes a first node (one closed on entry, \secref{nodes})
and returns its @BlockId@;
the @successors@ method takes a last node (closed on exit) and returns the @BlockId@s to
which it can transfer control.  In \ourlib{} a middle node (open-open) cannot refer to any @BlockId@s,
as the absence of an interrogation function implies. 

When defining a node type, a \ourlib{} client should also make it an
instance of @Edges@.  Thus for @CmmNode@ we would write
\begin{code}
instance Edges CmmNode where
  blockId (Label l) = l
  successors (Branch b) = [b]
  successors (CondBranch e b1 b2) = [b1,b2]
\end{code}
Again, the pattern-matching for both functions is exhaustive, and
the compiler statically checks this fact.  For example, @blockId@ simply
cannot be applied to a @Assign@ or @Branch@ node; the type checker would
reject any such attempt.

\ourlib{} provides instances for its types @Block@ and @Graph@ thus:
\begin{code}
instance Edges n => Edges (Block n) where
  blockId    (BUnit n)  = blockId n
  blockId    (BCat b _) = blockId b
  successors (BUnit n)  = successors n
  successors (BCat _ b) = successors b

instance Edges n => Edges (Graph n) where
  blockId    (GUnit b)      = blockId b
  blockId    (GMany ...)    = ???  -- eek!
  successors (GUnit b)      = successors b
  successors (GMany _ bs _) = concatMap successors bs 
                              `minusList` map blockId bs
\end{code}
\simon{I now realise that this is why I wanted to know the entry block of a graph!
Since an entry-closed node and block both have a unique block id, shouldn't an entry-closed
graph have one too?}
\simon{Also the successors thing is not pretty.}

\section {Using \ourlib{} to analyse and transform graphs} \seclabel{using-hoopl}
\seclabel{making-simple}
\seclabel{create-analysis}

Now that we have graphs in hand, we are ready to return to dataflow
optimisation.  The goal of \ourlib{} is to make it extremely easy for a
client to build a new dataflow analysis and optimisation.  To do this
the client must supply the following pieces:
\begin{itemize}
\item \emph{A node type} (\secref{graph-rep}).  
Then \ourlib{} supplies the @Block@ and @Graph@ types
that let the client build control-flow graphs out of nodes.
\item \emph{A data type of facts}, and some operations over 
those facts (\secref{facts}).
Each analysis uses its own analysis-specific 
facts, so \ourlib{} is completely polymorphic in 
the fact type.   
\item \emph{A transfer function} that takes an input fact and a node,
and produces the output fact that flows out of the other end of 
the node (\secref{transfers}).  
\item \emph{A rewrite function} that takes an input fact and a node,
and produces either @Nothing@,
or @(Just g)@ where @g@ is an \emph{arbitrary graph} that should
replace the node.  The ability to replace a node by a graph is crucial;
for example, perhaps we want to replace a high-level operation
with a set of conditional branches or a loop, which entails introducing new blocks.
Another important special case is replacing the node by the empty graph, 
thereby deleting the node.
We discuss the rewrite function
in \secref{rewrites}
\end{itemize}
With these pieces in hand, a client can call \ourlib{}'s analysis-and-rewriting
function:
\begin{code}
 `analyseAndRewriteFwd
  :: Edges n         
  => DataflowLattice f      -- Lattice
  -> ForwardTransfer n f    -- Transfer function
  -> ForwardRewrite n f     -- Rewrite function
  -> RewritingDepth         -- Rewrite recursively?
  -> Graph n e x            -- Graph or subgraph
  -> Fact e f               -- Input fact(s)
  -> FuelMonad (Graph n e x,  -- Rewritten graph
                Fact x f)     -- Output fact(s)
\end{code}
You can see that the function is polymorphic in the type of nodes @n@ and facts @f@.
Roughly speaking, you can see that, given a lattice, a transfer function, and a 
rewrite function, @analyseAndRewriteFwd@ can transform a @Graph@ 
into an optimised @Graph@.  In the rest of this section we flesh out this
explanation, including describing all the types mentioned in the above signature.
We will continue to use constant propagation as a running example; the complete
example is shown in \figref{const-prop}. \simon{John: this is for you to fill in}

The user-level model of how @analyseAndRewriteFwd@ works is this.  The
function walks forward over the graph.  At each node, it applies the
rewrite function to the node and its incoming fact.  If the rewrite
function returns @Nothing@, the node is retained as part of the output
graph, the transfer function is used to compute the downstream fact,
and we move on to the next node.

If the rewrite function returns @Just g@ then the next step is controlled
hy the @RewritingDepth@ flag:
\begin{itemize}
\item @RewriteDeep@: the entire @analyseAndRewriteFwd@ function is applied to 
the rewritten graph, which in turn may be further rewritten.
\item @RewriteShallow@: the rewritten graph is analysed \emph{but not further rewritten}.
\end{itemize}
Deep rewriting is sometimes essential to achieve the results of
\citet{lerner-grove-chambers:2002}.
Shallow rewriting is also sometimes vital; for example, a rewrite that inserts
a spill before a call might risk repeatedly inserting the same spill if the 
rewrite was iterated. 
When deep rewriting is used, the rewrite functions must
ensure that the graphs they produce are not rewritten indefinitely (\secref{correctness}).

Notice that in either case, the nodes following the rewritten node see 
\emph{up-to-date} facts; that is, their input facts are computed from the result 
of rewriting upstream nodes.

\subsection{Dataflow lattices} \seclabel{lattices} \seclabel{facts}

\begin{figure}
\ifcutting
\begin{code}
data `ChangeFlag = `NoChange | `SomeChange
data `DataflowLattice a = DataflowLattice
 {`fact_bot        :: a,
  `fact_add_to     :: a -> a -> (a, ChangeFlag) }
\end{code}
\else
\begin{code}
data ChangeFlag = NoChange | SomeChange
data DataflowLattice a = DataflowLattice
 { fact_bot        :: a,
   fact_add_to     :: a -> a -> (a, ChangeFlag),
   fact_name       :: String } -- Debugging only
\end{code}
\fi
\caption{Representation of a dataflow lattice} \figlabel{lattice-type} \figlabel{lattice}
\end{figure}

For any one analysis or transformation the client must define a type
of dataflow facts.  A dataflow facts typically represents an assertion
about a program point.  (A program point is simply an edge in the 
control-flow graph.)
\begin{itemize}
\item An assertion about all paths \emph{to} a program point is established
by a \emph{forwards analysis}. For example the assertion ``$@x@=@3@$'' at P 
might claim that variable @x@ holds value @3@ at P, regardless of the
path by which P was reached.
\item An assertion about all paths \emph{from} a program point is 
established by a \emph{backwards analysis}. For example, the 
assertion ``@x@ is dead'' at P might claim that no path from P uses 
variable @x@.
\end{itemize}

A~set of dataflow facts must form a lattice, and \ourlib{} must know
(a) the bottom element of the lattice and (b) how to join (take take 
the least upper bound of) two elements.  To ensure that analysis
terminates, it is enough if no fact has more than finitely many
distinct facts above it, so that any sequence of joins will terminate.

In our constant-propagation example, a fact is a finite conjunction of
sub-facts, at most one for each variable.  Each sub-fact has the form ``@x=@$k$'',
for some constant $k$, or ``@x=@$\top$'' (meaning that there is 
no single value held by $x$ at this program point).  For example, 
consider this graph,
where we assume @L1@ is the entry point:
\begin{verbatim}
  L1: x=3; y=4; if z then goto L2 else goto L3
  L2: x=7; goto L3
  L3: ....
\end{verbatim}
Since control flows to @L3@ from two places
we must join the facts coming from those two places.  The fact at the
first @goto L3@ is (@x=3@, @y=4@, @z=False@), while at the second @goto L3@
the fact is (@x=7@, @y=4@, @z=True@).  Joining these gives the 
the fact (@x=@$\top$, @y=4@, @z=@$\top$) at @L3@, where the $\top$'s indicate
that the analysis cannot
nail down @x@ and @z@ to hold a single value at @L3@\footnote{
In this example @x@ and @z@ really do vary at @L3@, but in general
the analysis might simply be conservative.}.

In general, joins occur at labels.
If~$f_{\mathit{id}}$ is the fact currently associated with the
label~$\mathit{id}$, 
and if a transfer function propagates a new fact~$f_{\mathit{new}}$
into the label~$\mathit{id}$, 
the dataflow engine replaces $f_{\mathit{id}}$ with
the join  $f_{\mathit{new}} \join f_{\mathit{id}}$.
Furthermore, the dataflow engine wants to know if
  $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$,
because if not, the analysis has not reached a fixed point.

Concretely, @analyseAndRewriteFwd@ is polymorphic in the fact type
@f@, and requires an argument of type @DataflowLattice f@.  The latter
type is shown in \figref{lattice}, and tells \ourlib{} the bottom and
join operations on the lattice.  In practice, when computing a join,
it is often cheap to learn if the join is equal to one of the
arguments.  Moreover, as noted in the previous paragraph, that
knowledge is precisely what \ourlib{} needs to know.  Hence, instead
of requiring an equality operation on facts (which might be
expensive), we instead ask @fact_add_to@ to return a @ChangeFlag@ as
well as the joined value.  The @ChangeFlag@ should be @NoChange@ if
the result is the same as the second argument, and @SomeChange@ if the
result differs.  (Note the assymmetry.)

In the case of constant propagation, \figref{constant-prop} shows a
possible concrete represetation of the facts described above.  A fact
is represented as a finite map from a variable to a value of type
\icode|Maybe Const|.  A~variable $x$ maps to @Nothing@ iff $x=\top$;
$x$~maps to $@Just@\;k$ iff $x=k$.  Any one procedure has only
finitely many variables; only finitely many facts are computed at any
program point; and in this lattice any one fact can increase at most
twice.  These properties ensure that the dataflow engine will reach a
fixed point.


\subsection{The transfer function} \seclabel{transfers}

\begin{figure}
\begin{code}
type InFactC  f = BlockId -> f
type OutFactC f = [(BlockId, f)] 

type ForwardTransfers n f 
  = forall e x. n e x -> InFact e f -> OutFact x f 

type BackwardTransfers n f 
  = forall e x. n e x -> InFact x f -> OutFact e f 

type family   InFact e f :: *
type instance InFact O f = f
type instance InFact C f = InFactC f

type family   OutFact x f :: *
type instance OutFact O f = f
type instance OutFact C f = OutFactC f
\end{code}
\caption{Transfer functions for forward and backward analyses} \figlabel{transfers}
\end{figure}
\ifpagetuning\vspace*{-1ex}\fi

A~transfer function is presented with the dataflow fact on the edge coming
into a node, and it computes dataflow facts on outgoing edge.
In a forward analysis, the dataflow engine starts with the fact at the
beginning of a block and applies the transfer function to successive 
nodes in that block until eventually the transfer function for the last node
computes the facts that are propagated to the block's successors.
For example, consider this graph, with entry at @L1@:
\begin{code}
  L1: x=3; goto L2
  L2: y=x+4; x=x-1; 
      if x>0 then goto L2 else return
\end{code}
A forward analysis starts with the bottom fact \{\} at every label.
Analysing @L1@ propagates this fact forward, by applying the transer 
function successively applying the transfer function to the nodes
of @L1@, emerging with the fact \{@x=3@\} for @L2@.
This is joined with the existing (bottom) fact for @L2@.
Now the analysis propagates @L2@'s fact forward, again using the transfer
function, this time emerging with a new fact \{@x=2@,@y=7@\} for @L2@.
Again, this is joined with the existing fact for @L2@, and the process
is iterated until the facts for each label reach a fixed point.

But wait!  What is the \emph{type} of the transfer function?
If the node is open at entry and exit, it takes a fact and the node and produces a fact.
But what if the node is \emph{closed} on entry or exit?
If the node is closed on entry, the transfer function must be given a mapping
from @BlockId@ to facts, which it can use to look up the fact for the entry
@BlockId@.  If the node is closed on exit then the transfer function must
produce a list of (@BlockId@,fact) pairs, one for each outgoing edge.  

\emph{So the types of the transfer function's argument and result each 
depend on the open/closed-ness of the node.}
Fortunately, this is precisely what Haskell's (recently added) 
indexed type families are for.
The relevant \ourlib{} definitions are given in \figref{transfers}.
\simon{For the @InFacts@ I think it's better to have a real function.}
A forward transfer function, of type (@ForwardTransfers@ @n@ @f@), 
is a function polymorphic in @e@ and @x@.  It takes a node of type (@n@ @e@ @x@), and
an incoming fact-like thing of type (@InFact@ @e@ @f@), and produces an
outgoing fact-like thing of type (@OutFact@ @x@ @f@).  The type constructor @InFact@
should be thought of as a type-level function; its signature is given in the
@type family@ declaration, while its definition is given by two @type instance@
declarations.  The first says that the fact-like thing coming into a node
open at entry is just a fact @f@. The second says that if the node is closed
on entry the incoming thing is an @InFactC f@, which is just a function from @BlockId@
to @f@.   The same technique makes the result type of @ForwardTransfers@ vary with
the exit flag @x@.

All the \ourlib{} client needs to do is to write the transfer function, with
this type.  \figref{constant-prop} shows the transfer function for our
constant-propagation example.  The supplied transrer function should be 
\emph{monotonic} in the following sense: given a more informative fact in,
it should produce a more informative fact out.  Otherwise the fixpoint-finder
might go into an infinite flip-flop loop.

\ifpagetuning\enlargethispage{0.5\baselineskip}\fi


\subsection{The rewrite function} \seclabel{rewrites} \seclabel{example-rewrites}

\begin{figure}
\hfuzz=5.7pt % cheating again!
\begin{code}
type AGraph n e x 
  = BlockIdSupply -> (Graph n e x, BlockIdSupply)

type ForwardRewrites n f 
  = forall e x. n e x -> InFact e f -> Maybe (AGraph n e x)
type BackwardRewrites n f 
  = forall e x. n e x -> InFact x f -> Maybe (AGraph n e x)
\end{code}
\caption{Types of forward and backward rewrite functions} \figlabel{rewrites}
\end{figure}

We compute dataflow facts in order to enable code-improving
transformations on control-flow graphs.
In our constant-propagation example, the dataflow facts may enable us
to simplify an expression by performing constant folding, or to 
turn a conditional branch into an unconditional one.
Similarly, a liveness analysis may allow us to 
rewrite an assignment to a dead register by a no-op; knowing that a register is
live at a call site might provoke us to insert a spill before the call; and so on.


Thus motivated, we allow the user to give \ourlib{} a rewriting function
that takes a node and a fact, and optionally returns a graph, of the
same open/closed shape as the node.   The type is given in \figref{rewrites}.
As with the transfer functions, an entry-closed node does not get a single fact
but rather an @InFact f@, using the same @InFact@ function as described
in \figref{transfers}.  The @Maybe@ type expresses that the function can return
@Nothing@, meaning no rewrite, or (@Just g@) where @g@ is the replacement graph.

More precisely, the rewriter returns an @AGraph@, a function that is given 
a supply of fresh @BlockId@s.  If the rewriter makes graphs containing blocks,
it will need to conjure up some fresh @BlockIds@ and the @BlockIdSupply@ makes
that possible. Notice that the open/closed is statically 
guaranteed to be preserved, because
the @e@ and @x@ parameters of the node are also used for the @AGraph@.
So a branch instruction, for example can only be rewritten to a graph 
closed at the exit.

\subsection{Throttling the dataflow engine using ``optimization fuel''}
\seclabel{vpoiso}
\seclabel{fuel}

You will have noticed that the type of @analyseAndRewriteFwd@ is monadic
(\secref{using-hoopl}).
If the @FuelMonad@ determines that rewriting fuel is exhausted, then no more
rewriting is done.  
(Because each rewrite leaves the observable behavior of the
program unchanged, it is safe to suppress rewrites at
any point.)
This allows the Hoopl library client to use a very powerful
debugging technique for dataflow transformations,
first suggested by Whalley\cite{whalley:isolation}.
In~normal operation, the optimizer has unlimited fuel.  But suppose that the program
being compiled works without optimisation, but fails with optimisation. Then
the client can use binary search to halt rewriting just before or just after the
rewrite that introduces the failure, thereby identifying the buggy rewrite.

The @FuelMonad@ is a simple state monad maintaining the supply of unused
fuel.  It also holds a supply of fresh labels, which are used by the rewriter
for making new blocks; more precisely, the client-provide @ForwardRewrites@
function produces an @AGraph@ (\figref{rewrites}) which Hoopl converts to
a @Graph@ by supplying the @AGraph@ with a supply of @BlockIds@ from the @FuelMonad@.

\subsection{Fixpoints and speculative rewrites}

The alert reader will be wondering what happens in the presence of
loops.  Many analyses compute a fixpoint starting from unsound
``facts''; for example, a live-variable analysis starts from the
assumption that all variables are dead.  This means \emph{rewrites
performed in any iteration other than the last may be unsound, and
must be discarded}.  That is, each iteration of the fixpoint must
start afresh with the original graph.  Nevertheless, those rewrites
must still be performed (albeit tentatively, but possibly recursively)
so that the correct downstream facts can be computed.

This sort of thing is hard to achieve in an imperative setting, where rewrites
mutate a graph in-place.  But it is trivially easy in a functional program:
all we need do is to revert to the original graph at the start
of each fixpoint iteration.

\subsection{Correctness} \seclabel{correctness}

\simon{This section seems a bit laboured}

Facts computed by the @analyseAndRewriteFwd@ depend on graphs produced by rewrite
function, which in turn depend on facts computed by the transfer function.
How~do we know this algorithm is sound, or even if it terminates?
A~proof requires its own POPL paper
\cite{lerner-grove-chambers:2002}, but we can give some
intuition.

The rewrite function must, of course, be \emph{sound}:
if it replaces a node @n@ by a replacement graph
@g@, then $g$ should be observationally equivalent to $n$ under the 
assumptions expressed by the incoming dataflow fact @f@.
Moreover the rewrite function must be \emph{consistent} with the transfer function
that is, (@transfer n f@ $\sqsubseteq$ @transfer g f@).
For example, if the analysis says that x is dead before the node @n@,
then it had better still be dead if @n@ is replaced by @g@.  Without
the consistency condition there would be no guarantee of reaching a fixed point.
\simon{I still think this is true, although you do not (see comments in the source file. We 
should debate.}
Given these assumptions:

\begin{itemize} 
\item
The algorithm is sound because if each rewrite is sound (in the sense given above), 
then applying a succession of rewrites is also sound.
Moreover, a~sound analysis of the rewritten graph
may generate only dataflow facts that could have been
generated by a more complicated analysis of the original graph.
\item
No matter what the transfer functions and rewrite functions do,
the dataflow engine uses the dataflow lattice's join operation to ensure that
facts at labels never decrease. 
As~long as
\ifcutting
 no fact may
\else
 the lattice permits no fact to 
\fi
increase infinitely many
times, analysis
\ifcutting
 terminates.
\else
 is guaranteed to terminate.
\fi
\end{itemize}
Thus to guarantee soundness and termination, client code must supply 
sound transfer functions,
sound rewrite functions,
and a
lattice with no infinite ascending chains.
And unless client code specifies shallow rewriting,
\ifcutting
\else which tells the
dataflow engine never to rewrite a replacement graph,
\fi
rewrite functions must not return replacement
graphs which contain nodes that could be rewritten indefinitely.

Why use such a complex algorithm?
\ifcutting Because interleaving \else
\citet{lerner-grove-chambers:2002} write
\begin{quote}
\emph{Previous efforts to exploit [the mutually beneficial
interactions of dataflow analyses] either (1)~iteratively performed
each individual analysis until no further improvements are discovered
or (2)~developed [handwritten] ``super-analyses'' that manually
combine conceptually separate analyses. We have devised a new approach
that allows analyses to be defined independently while still enabling
them to be combined automatically and profitably. Our approach avoids
the loss of precision associated with iterating individual analyses
and the implementation difficulties of manually writing a
super-analysis.}
\end{quote}



%%  \simon{But do we apply rewrites even before the analysis reaches a fixed point?
%%  If so, what property do the rewrites have to satisfy to ensure soundness?
%%  If not, even a single rewrite might destroy the fixed-point property of the
%%  current facts.  Or perhaps we iterate the analysis to a fixpoint, and only \emph{then}
%%  do rewriting? If so, do we need the transfer functions at that stage?
%%  
%%  Also the fixed-point of the analysis relies on upward chains. What if
%%  the rewrite pushed it downward?  Or is it the case that a rewrite must
%%  change a node $n$ into a graph $g$ so 
%%  that $\mathit{fwdtrans}(n) \leq \mathit{fwdtrans}(g)$?
%%  
%%  Also the fixpoint calculation requires multiple passses; do the 
%%  rewrites then apply multiple times?
%%  
%%  I'm deliberately playing the role of the reader here, and not peeking at
%%  the code.  I don't think it's enough to say ``go look at Chambers paper''; 
%%  I suggest we say enough (half a column would do it) to address the obvious
%%  questions and point to Chambers for details.
%%  
%%  
%%  \textbf{NR}: Good questions, but let's have a forward reference to \secref{dfengine}}

Interleaving
\fi
 analysis with transformation makes it
possible to implement useful  transformations using startlingly simple
client code.
In the rest of this section we present two examples:
\secref{sink-reloads} shows how to insert a reload instruction just
before each use of each spilled variable, and
\secref{dead-code-elim} shows how to eliminate dead assignments.
When these two transformations are run in sequence, the effect is to
sink reloads and produce programs like the example shown in
\secref{spill-reload-example}. 

\finalremark{Doesn't the rewrite have to be have the following property:
for a forward analysis/transform, if (rewrite P s) = Just s',
then (transfer P s $\sqsubseteq$ transfer P s').
For backward: if (rewrite Q s) = Just s', then (transfer Q s' $\sqsubseteq$ transfer Q s).
Works for liveness.
``It works for liveness, so it must be true'' (NR).
If this is true, it's worth a QuickCheck property!
}%
\finalremark{Version 2, after further rumination.  Let's define
$\scriptstyle \mathit{rt}(f,s) = \mathit{transform}(f, \mathit{rewrite}(f,s))$.
 Then $\mathit{rt}$ should
be monotonic in~$f$.  We think this is true of liveness, but we are not sure
whether it's just a generally good idea, or whether it's actually a 
precondition for some (as yet unarticulated) property of \ourlib{} to hold.}%

%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixpoint. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }




\section{\ourlib's dataflow engine}
\seclabel{engine}
\seclabel{dfengine}

\delendum{The earlier sections promised that we'd reveal the lies.
Do we?  I see no mention of @HavingSuccessors@ for example, which is rather important
for polymorphism.  Indeed, a subsection on that point might be a good way
to substantiate the claims of the last bullet of the conclusion.}

In Sections \ref{sec:making-simple}
through~\ref{sec:rewrites},
we give the user's eye view, showing how to use 
\ourlib\ to create analyses and transformations.
We now sketch the implementation of the main part of \ourlib:
the dataflow engine.
While a full description of the implementation is beyond the scope of
this paper, a sketch 
demonstrates the new ideas that make this implementation simpler
than the original:
using pure functional code throughout;
using an explicit state monad to manage the computation of fixed
points;
giving each type of graph node its own analysis function, 
which also performs speculative rewriting;
and \simon{Complete this list}.

\subsection{Overview}

We will concentrate on implementing @analyseAndRewriteFwd@, whose
tpye signature is in \secref{running-rewriter}.
The implementation is built on the hierarchy of nodes, blocks, and graphs
that we described in \secref{graph-rep}.  For each such thing
we will develop a function of this type:
\begin{code}
newtype GFT thing n f 
  = GFT (forall e x. 
         thing e x 
         -> InFact e f
         -> FuelMonad (Graph n e x, OutFact x f))
\end{code}
Here we intend @thing@ to range over nodes, blocks, and graphs, thus:
\begin{code}
type GFT_Node  n f = GFT n         n f
type GFT_Block n f = GFT (Block n) n f
type GFT_Graph n f = GFT (Graph n) n f
\end{code}
A GFT (short for ``generic forward transfer'') takes a @thing@ and an input fact of
appropriate shape, and returns a @Graph@ and a suitably-shaped output fact.
Note that the result is a @Graph@, regardless of whether @thing@ is a node, block, or graph.

We can now build a modular implementation as follows:
\begin{itemize} 
\item From the supplied @ForwardTransfers@ and @ForwardRewrites@, produce a @GFT_Node@
tha transforms nodes:  
\begin{code}
gftNodeRewrite :: ForwardTransfers n f
               -> ForwardRewrites n f
               -> GFT_Graph n f
               -> GFT_Node n f
\end{code}
In addition to the transfer and rewrite functions, @gftNodeRewrite@ takes the function
to analyse and rewrite the rewritten graph.
\item From this @GFT_Node@, produce a @GFT_Block@ that transforms blocks.
\begin{code}
gftBlock :: GFT_Node n f -> GFT_Block n f
\end{code}
 
\item From this @GFT_Block@, produce a @GFT_Graph@ that tranforms graphs.
\begin{code}
gftGraph :: DataflowLattice f
         -> GFT_Block n f -> GFT_Graph n f
\end{code}
This is where the fixpoint computation takes place, so @gftGraph@ must
take a @DataflowLattice@ as an additional argument.
\end{itemize}
Given these three functions, it is rather easy to write the main analyser:
\par {\small
\begin{code}
analyseAndRewrite
   :: forall n f. Edges n
   => RewritingDepth
   -> DataflowLattice f
   -> ForwardTransfers n f
   -> ForwardRewrites n f
   -> GFT_Graph n f

analyseAndRewrite depth lattice tf rw
 = gft_g
 where 
  gft_g, gft_g_sh, gft_g_rec :: GFT_Graph n f
  gft_g     = gftGraph lattice (gftBlock gft_n)
  gft_g_sh  = gftGraph lattice (gftBlock gft_node_sh)
  gft_g_rec = case depth of
                RewriteShallow -> gft_g_sh
                RewriteDeep    -> gft_g

  gft_n, gft_n_sh :: GFT_Node n f
  gft_n    = gftNodeRewrite tf rw gft_graph_rec
  gft_n_sh = gftNodeTransfer transfers
\end{code}
}
This beautifully compact code does the following:
\begin{itemize}
\item The result, @gft_g@ is built by applying @gftGraph@ and @gftBlock@ to
the node-rewriting engine, @gft_n@.
\item The node-rewriting engine is constructed @gftNodeRewrite@, instructing the
latter to use @gft_g_rec@ for recursive applications to rewritten graphs.
\item When deep rewriting is used, the @gft_g_rec@ engine is simply a recursive
use of the main function, @gft_g@. But if shallow rewriting is required, it instead
uses the shallow engine @gft_g_sh@. 
\item This shallow engine does no rewriting at all: it just performs the dataflow
analysis.  However, it can still be constructed by applying @gftGraph@ and @gftBlock@
to the shallow node engine, @gft_n_sh@.
\item Finally, the shallow node engine, which merely performs dataflow transfers on a single
node, with no rewriting, is built with @gftNodeTransfer@.
\end{itemize}
And that is all.  Note the way that the shallow/deep distinction is made
here, and here alone; none of the other functions are aware of the issue.


\subsection{From nodes to blocks}

We start with the second task, of ''lifting'' a @GFT_Node@ to a @GFT_Block@:
\begin{code}
gftBlock :: forall n f. GFT_Node n f -> GFT_Block n f
gftBlock (GFT node_trans) = GFT block_trans
  where 
    block_trans :: GFTR (Block n) n f
    block_trans (BUnit n)   f 
      = node_trans n f
    block_trans (BCat l r) f 
      = do { (g1,f1) <- block_trans l f
           ; (g2,f2) <- block_trans r f1
	   ; return (g1 `gCat` g2, f2) }
\end{code}
The code is childishly simple.  In the @BUnit@ case 
we appeal to the node-level @GFT_Node@ tranform.
For @BCat@ we recursively apply @block_trans@ to the two
sub-blocks, threading the output fact from the first as the 
input to the second.  Each will produce a rewritten @Graph@, 
which we connect together in sequence with @gCat@. 

\subsection{Rewriting graphs}

Next we tackle .... \simon{not finished}




\section {Related work}

\simon{I've moved this section to near the end}

While dataflow analysis and optimization are covered
by a vast literature, 
\emph{design} of optimizers, the topic of this paper, is covered
relatively sparsely.
We therefore focus on foundations.

When transfer functions are monotone and lattices are finite in height,
iterative dataflow analysis converges to a fixed point
\cite{kam-ullman:global-iterative-analysis}. 
If~the lattice's join operation distributes over transfer
functions,
this fixed point is equivalent to a join-over-all-paths solution to
the recursive dataflow equations
\cite{kildall:unified-optimization}.\footnote
{Kildall uses meets, not joins.  
Lattice orientation is conventional, and conventions have changed.
We use Dana Scott's
orientation, in which higher elements carry more information.}
\citet{kam-ullman:monotone-flow-analysis} generalize to some
monotone functions.
Each~client of \hoopl\ must guarantee monotonicity,
but for transfer functions that
approximate weakest preconditions or strongest postconditions,
monotonicity falls out naturally.

\ifcutting
\citet{cousot:abstract-interpretation:1977}
\else
\citet{cousot:abstract-interpretation:1977,cousot:systematic-analysis-frameworks}
\fi
introduce abstract interpretation as a technique for developing
lattices for program analysis.
\citet{schmidt:data-flow-analysis-model-checking} shows that
an all-paths dataflow problem can be viewed as model checking an
abstract interpretation.

The soundness of interleaving analysis and transformation,
even when some speculative transformations are not performed on later
iterations, was shown by
\citet{lerner-grove-chambers:2002}.
\ifcutting\else
Muchnick \citeyearpar{muchnick:compiler-implementation} 
presents many examples of both particular analyses and related
algorithms.
\fi

In our previous work we describes a zipper-based representation of control-flow
grpahs \cite{ramsey-dias:applicative-flow-graph}, stressing the advantages
of immutability for writing dataflow optimisers.
Our new representation, described in \secref{graph-rep}, is a major improvement:
\begin{itemize}
\item
We can find the exit point of a graph in constant time. \simon{What does this mean?}
\item
We can concatenate nodes, blocks, and graphs, in constant time.
Previously, we had to resort to Hughes's
\citeyearpar{hughes:lists-representation:article} technique, representing
a graph as a function.
\item
Most important, errors in concatenation are ruled out at
compile-compile time by Haskell's static
type system.
In~earlier implementations, such errors were not detected until
the compiler~ran, at which point \ourlib\ tried to compensate
for the errors%
\ifcutting---but
\else
by inserting branch instructions and then issued a warning message.
But
\fi
the compensation code harbored subtle faults%
\ifcutting\else, which were discovered while developing a new back end
for~GHC\fi. 
\hfuzz=1.6pt % don't want to reword ``errors in concatenation''
\end{itemize}

\section{Conclusions}

Compiler textbooks make dataflow optimization appear
difficult and complicated.
In~this paper, we show how to engineer a library, \ourlib, which makes
it easy to build analyses and transformations based on dataflow.
\ourlib\ makes dataflow simple not by using a single magic
ingredient, but by applying ideas that are well understood by 
the programming-language community.
\begin{itemize}
\item
We acknowledge only one program-analysis technique: the solution of
recursion equations over assertions.
%Like our colleagues working in imperative languages, 
We solve the equations by iterating to a fixed point.
% Many equations relate
% properties of program states; some relate properties of paths through
% programs. 
\item
We consider only two
ways of relating assertions: weakest liberal precondition and strongest 
postcondition, which
 correspond 
%\ifpagetuning\else
%respectively
%\fi
to
\ifcutting
``backward'' and ``forward'' dataflow
problems.
\else
``backward dataflow problems'' and ``forward dataflow
problems.''
\fi
\finalremark
{Can we give an example of a property of program states which is
neither, just by way of contrast; ie this we cannot do.}
%%  \item
%%  In a language that admits loops, iterating weakest preconditions or
%%  strongest postconditions typically does not reach a fixed point in
%%  finitely many steps; hence the need for loop invariants
%%  \cite{hoare:axiomatic-basis,dijkstra:discipline,gries:science-programming}.
%%  In \secref{logic-reconciled}, we show that we can guarantee to reach a
%%  fixed point by limiting what we can express in the logic.\remark{needs
%%  a fix}
%%  We~show that many classic analyses can be explained this way;
%%  the great diversity of classic analyses corresponds to a great
%%  diversity of inexpressive logics.
%%  This view leads us to a unifying principle:
%%  \emph{To implement a code-improving transformation, find the least
%%    expressive logic that can justify the transformation, then use that
%%    logic to compute strongest postconditions, which justify the
%%    transformation locally.}
\item
Although our implementation allows graph nodes to be rewritten in any
way that preserves semantics, we describe
three program-transformation techniques:
substitution of equals for equals, 
insertion of assignments to unobserved variables, 
and removal of assignments to unobserved variables
(\secref{example:transforms}).
Substitution of equals for equals is often justified by properties of program
states; for example, if variable~$x$
is always~7, we may substitute~7 for~$x$.\finalremark
{We can also justify substitution of \emph{labels} in goto
  statements by reasoning about continuations.  This is
  probably not the place to mention this fact.}
Insertion and removal of assignments are often justified by properties
of paths through programs;
for example, if an assignment's continuation does not use the variable
assigned~to, that assignment may be removed.

%%  Some compiler texts treat the removal of unreachable code as a
%%  code-improving transformation in its own right.
%%  In~our framework, unreachable code becomes unreachable in the
%%  garbage-collection sense, so no special effort is required to remove
%%  it.
\item
Complex program transformations should be composed from simple
transformations. 
For example, both ``code motion'' and ``induction-variable
elimination'' can be implemented in three stages: insert new assignments;
substitute equals for equals; remove unneeded assignments
(\secref{induction-var-elim}). 

\item 
Because each rewrite leaves the semantics
of the program unchanged, 
we can use 
``optimization fuel'' to limit the number of rewrites.
 When we isolate a fault 
\ifcutting\else in the optimizer \fi
(\secref{vpoiso}), we 
\ifcutting have to debug just \else therefore have the luxury of debugging \fi
 a single
 rewrite, not a complex transformation.
\end{itemize}

We also build on proven implementation techniques
in a way that
makes it easy
to implement classic code improvements.
\begin{itemize}
\item
We use the algorithm of \citet{lerner-grove-chambers:2002} to 
compose analyses and transformations.
This~algorithm makes it easy to compose complex transformations
from simple ones.

Using continuation-passing style and generalized algebraic data types,
we have created a new implementation%
\ifcutting\else\ of the algorithm\fi, 
which works by 
composing three relatively simple functions
(\secref{forward-iterator}). 
The functions are simple
because the static type of a node constrains the number of predecessors
and successors it may have.
And because we can
compare our code with a standard continuation semantics, we have more
confidence in this new implementation than in 
any previous implementation. 
\item
Our code is pure.
Inspired by Huet's~\citeyearpar{huet:zipper} zipper,
we use an applicative representation of
control-flow graphs
\cite{ramsey-dias:applicative-flow-graph}. 
We~improve on our prior work by storing changing dataflow facts
in an explicit dataflow monad,
which
makes it especially easy to implement such
operations as sub-analysis of a replacement graph
(\secref{dataflow-monad});
by using static types to guarantee that each replacement graph can be
spliced in place of the node it replaces
(\secreftwo{subgraphs}{rewrite-functions});
and by simplifying our implementation using continuation-passing style
(\secreftwo{forward-iterator}{forward-actualizer}). 
%
% important, but no longer mentioned in this paper:
%
%%  \item
%%  To \emph{construct} programs, we use a different representation of
%%  flow graphs, one which hides the complexity of the zipper and which
%%  provides a constant-time operation for joining flow graphs in
%%  sequence.
%%  It is inspired in part by Hughes's \citeyearpar{hughes:novel-lists}
%%  representation of lists, which supports a constant-time append operation.
\item
\ourlib\ is polymorphic in the
representations of 
assignments and control-flow operations.
%%  Although our polymorphic representations have been instantiated only
%%  with the low-level intermediate code used by the Glasgow Haskell
%%  Compiler, they are intended eventually to be instantiated with
%%  machine-dependent representations of target-machine instructions, as
%%  part of a larger project of refactoring GHC's back ends.
%
%This design seems obvious in retrospect,
%but we underestimated the degree to which polymorphism would force us to
%separate concerns.
%Introducing polymorphism has made the code simpler, easier
%to understand, and easier to maintain.
By forcing us to separate concerns, introducing polymorphism
made the code simpler, easier to understand, and easier to maintain.
\finalremark
{SLPJ: Is it possible to substantiate this claim by [more] examples?}
In particular, it forced us to make explicit \emph{exactly} what
\ourlib\ 
 must know about flow-graph nodes:
it must be able to find
targets of control-flow operations (constraint
@HavingSuccessors l@, \secref{zdfSolveFwd}).
\end{itemize}
%
% gen and kill are history
%
%%\item
%%Judicious use of Haskell type classes makes is possible to write
%%weakest precondition or strongest postcondition using the ``transfer
%%equations'' that are familiar from compiler textbooks.
%%If you like, you can even write overloaded @gen@ and @kill@ functions.
%%The benefit is that it is easy to compare the actual code with the
%%abstract treatments found in textbooks\ifgenkill \ (\secref{gen-kill})\fi.
Using \ourlib,
you can create a new code improvement in three steps:
create a lattice representation for the assertions you want to
express;
create transfer functions that approximate weakest preconditions or
strongest postconditions;
and 
create rewrite functions that use your assertions to justify
program transformations.  
You can get quickly to the real 
intellectual work of code improvement: identifying interesting
transformations and the assertions that justify them.

\finalremark{Don't forget acknowledgements!!!
Microsoft, Intel, NSF
}

\makeatother

\providecommand\includeftpref{\relax} %% total bafflement -- workaround
\IfFileExists{nrbib.tex}{\bibliography{cs,ramsey}}{\bibliography{cs,ramsey,simon,jd}}
\bibliographystyle{plainnatx}


\clearpage

\appendix


\section{Index of defined identifiers}

This appendix lists every nontrivial identifier used in the body of
the paper.  
For each identifier, we list the page on which that identifier is
defined or discussed---or when appropriate, the figure (with line
number where possible).
For those few identifiers not defined or discussed in text, we give
the type signature and the page on which the identifier is first
referred to.

Some identifiers used in the text are defined in the Haskell Prelude;
for those readers less familiar with Haskell, these identifiers are
listed in Appendix~\ref{sec:prelude}.

\newcommand\dropit[3][]{}

\newcommand\hsprelude[2]{\noindent
  \texttt{#1} defined in the Haskell Prelude\\}
\let\hsprelude\dropit

\newcommand\hspagedef[3][]{\noindent
  \texttt{#2} defined on page~\pageref{#3}.\\}
\newcommand\omithspagedef[3][]{\noindent
  \texttt{#2} not shown (but see page~\pageref{#3}).\\}
\newcommand\omithsfigdef[3][]{\noindent
  \texttt{#2} not shown (but see Figure~\ref{#3} on page~\pageref{#3}).\\}
\newcommand\hsfigdef[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} defined in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hstabdef[3][]{%
  \noindent
  \ifx!#1!
    \texttt{#2} defined in Table~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Table~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hspagedefll[3][]{\noindent
  \texttt{#2} {let}- or $\lambda$-bound on page~\pageref{#3}.\\}
\newcommand\hsfigdefll[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} {let}- or $\lambda$-bound in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} {let}- or $\lambda$-bound on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    

\newcommand\nothspagedef[3][]{\notdefd\ndpage{#1}{#2}{#3}}
\newcommand\nothsfigdef[3][]{\notdefd\ndfig{#1}{#2}{#3}}
\newcommand\nothslinedef[3][]{\notdefd\ndline{#1}{#2}{#3}}

\newcommand\ndpage[3]{\texttt{#2}~(p\pageref{#3})}
\newcommand\ndfig[3]{\texttt{#2}~(Fig~\ref{#3},~p\pageref{#3})}
\newcommand\ndline[3]{%
  \ifx!#1!%
      \ndfig{#1}{#2}{#3}%
  \else
      \texttt{#2}~(Fig~\ref{#3}, line~\lineref{#1}, p\pageref{#3})%
  \fi
}



\newif\ifundefinedsection\undefinedsectionfalse

\newcommand\notdefd[4]{%
  \ifundefinedsection
    , #1{#2}{#3}{#4}%
  \else
    \undefinedsectiontrue
    \par
    \section{Undefined identifiers}
    #1{#2}{#3}{#4}%
  \fi
}

\begingroup
\raggedright

\input{defuse}%
\ifundefinedsection.\fi

\undefinedsectionfalse


\renewcommand\hsprelude[2]{\noindent
  \ifundefinedsection
    , \texttt{#1}%
  \else
    \undefinedsectiontrue
    \par
    \section{Identifiers defined in Haskell Prelude}\label{sec:prelude}
    \texttt{#1}%
  \fi
}
\let\hspagedef\dropit
\let\omithspagedef\dropit
\let\omithsfigdef\dropit
\let\hsfigdef\dropit
\let\hstabdef\dropit
\let\hspagedefll\dropit
\let\hsfigdefll\dropit
\let\nothspagedef\dropit
\let\nothsfigdef\dropit
\let\nothslinedef\dropit

\input{defuse}
\ifundefinedsection.\fi



\endgroup


\iffalse

\section{Dataflow-engine functions}


\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward iterator}
\end{figure*}

\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward actualizer}
\end{figure*}


\fi



\end{document}



% Old captions' text:
% The dataflow fact for the available-reload analysis describes
%   the set of registers for which a reload is available.
%   We list the types of the functions that manipuate sets of available registers,
%   as well as the definition of the lattice.
% The standard gen and kill functions for available expressions
% The transfer functions for the available-reloads analysis.
% Running the available-reloads analysis and extracting the results with \texttt{zdfFpFacts}
% The rewrite functions to insert redundant reloads immediately before uses

% Probably no space for the implementations:
% interAvail (UniverseMinus s) (UniverseMinus s') =
%   UniverseMinus (s `plusVarSet`  s')
% interAvail (AvailVars     s) (AvailVars     s') =
%   AvailVars (s `timesVarSet` s')
% interAvail (AvailVars     s) (UniverseMinus s') =
%   AvailVars (s  `minusVarSet` s')
% interAvail (UniverseMinus s) (AvailVars     s') =
%   AvailVars (s' `minusVarSet` s )
% 
% smallerAvail (AvailVars _) (UniverseMinus _) = True
% smallerAvail (UniverseMinus _) (AvailVars _) = False
% smallerAvail (AvailVars     s) (AvailVars    s')  =
%   sizeVarSet s < sizeVarSet s'
% smallerAvail (UniverseMinus s) (UniverseMinus s') =
%   sizeVarSet s > sizeVarSet s'
% 
% extendAvail (UniverseMinus s) r =
%   UniverseMinus (deleteFromVarSet s r)
% extendAvail (AvailVars     s) r =
%   AvailVars (extendVarSet s r)
% 
% delFromAvail (UniverseMinus s) r =
%   UniverseMinus (extendVarSet s r)
% delFromAvail (AvailVars     s) r =
%   AvailVars (deleteFromVarSet s r)
% 
% elemAvail (UniverseMinus s) r =
%   not $ elemVarSet r s
% elemAvail (AvailVars     s) r =
%   elemVarSet r s



THE FUEL PROBLEM:


Here is the problem:

  A graph has an entry sequence, a body, and an exit sequence.
  Correctly computing facts on and flowing out of the body requires
  iteration; computation on the entry and exit sequences do not, since
  each is connected to the body by exactly one flow edge.

  The problem is to provide the correct fuel supply to the combined
  analysis/rewrite (iterator) functions, so that speculative rewriting
  is limited by the fuel supply.

  I will number iterations from 1 and name the fuel supplies as
  follows:

     f_pre      fuel remaining before analysis/rewriting starts
     f_0        fuel remaining after analysis/rewriting of the entry sequence
     f_i, i>0   fuel remaining after iteration i of the body
     f_post     fuel remaining after analysis/rewriting of the exit sequence

  The issue here is that only the last iteration of the body 'counts'.
  To formalize, I will name fuel consumed:

     C_pre      fuel consumed by speculative rewrites in entry sequence
     C_i        fuel consumed by speculative rewrites in iteration i of body
     C_post     fuel consumed by speculative rewrites in exit sequence

  These quantities should be related as follows:

     f_0    = f_pre - C_pref
     f_i    = f_0 - C_i            where i > 0
     f_post = f_n - C_post         where iteration converges after n steps

When the fuel supply is passed explicitly as parameter and result, it
is fairly easy to see how to keep reusing f_0 at every iteration, then
extract f_n for use before the exit sequence.  It is not obvious to me
how to do it cleanly using the fuel monad.


Norman
