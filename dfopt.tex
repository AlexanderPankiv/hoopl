%%  advances over prior work
%%  
%%  (if open/closed) -- detect bad splices at compile time
%%  constant-time access to exit sequence
%%  (indeed exit  node)
%%  
%%  directly splicable in constant amortized time (vs Hughes
%%  technique)
%%  
%%  polymorphic in node type (type classes)
%%  
%%  



\iffalse % submission abstract

We present Hoopl, a Haskell library that makes it easy for compiler
writers to implement program transformations based on dataflow
analyses. The compiler writer must identify (a) logical assertions on
which the transformation will be based; (b) a representation of such
assertions, which should form a lattice of finite height; (c) transfer
functions that approximate weakest preconditions or strongest
postconditions over the assertions; and (d) rewrite functions whose
soundness is justified by the assertions. Hoopl uses the algorithm of
Lerner, Grove, and Chambers (2002), which can compose very simple
analyses and transformations in a way that achieves the same precision
as complex, handwritten "superanalyses." Hoopl will be the workhorse
of a new back end for the Glasgow Haskell Compiler.

Because the major claim in the paper is that Hoopl makes it easy to
implement program transformations, the paper is filled with examples,
which are written in Haskell.  The paper also sketches the
implementation of Hoopl, including some excerpts from the
implementation. 

\fi


\IfFileExists{timestamp.tex}{\input{dfoptdu.tex}}{}
     % If you are Simon, run un-preprocessed code
     % It not Simon, use version with def-use information

\newif\ifpagetuning \pagetuningtrue  % adjust page breaks

\newif\ifnoauthornotes \noauthornotesfalse
\newif\iftimestamp\timestamptrue  % show MD5 stamp of paper

\timestamptrue % it's submission time

\IfFileExists{timestamp.tex}{}{\timestampfalse}

\newif\ifcutting \cuttingfalse % cutting down to submission size


\newif\ifgenkill\genkillfalse  % have a section on gen and kill
\genkillfalse


\newif\ifnotesinmargin \notesinmarginfalse
\IfFileExists{notesinmargin.tex}{\notesinmargintrue}{\relax}

\documentclass[blockstyle,preprint,natbib,nocopyrightspace]{sigplanconf}

\newcommand\ourlib{Hoopl}
   % higher-order optimization library
   % ('Hoople' was taken -- see hoople.org)
\let\hoopl\ourlib

% l2h substitution ourlib Hoopl
% l2h substitution hoopl Hoopl

\newcommand\fs{\ensuremath{\mathit{fs}}} % dataflow facts, possibly plural

\newcommand\vfilbreak[1][\baselineskip]{%
  \vskip 0pt plus #1 \penalty -200 \vskip 0pt plus -#1 }

\usepackage{alltt}
\usepackage{array}
\newcommand\lbr{\char`\{}
\newcommand\rbr{\char`\}}
 
\clubpenalty=10000
\widowpenalty=10000

\usepackage{verbatim} % allows to define \begin{smallcode}
\newenvironment{smallcode}{\par\unskip\small\verbatim}{\endverbatim}

\newcommand\lineref[1]{line~\ref{line:#1}}
\newcommand\linepairref[2]{lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\linerangeref[2]{\mbox{lines~\ref{line:#1}--\ref{line:#2}}}
\newcommand\Lineref[1]{Line~\ref{line:#1}}
\newcommand\Linepairref[2]{Lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\Linerangeref[2]{\mbox{Lines~\ref{line:#1}--\ref{line:#2}}}

\makeatletter

\let\c@table=
           \c@figure % one counter for tables and figures, please

\newcommand\setlabel[1]{%
  \setlabel@#1!!\@endsetlabel
}
\def\setlabel@#1!#2!#3\@endsetlabel{%
  \ifx*#1*% line begins with label or is empty
     \ifx*#2*% line is empty
        \verbatim@line{}%
     \else
       \@stripbangs#3\@endsetlabel%
       \label{line:#2}%
     \fi
  \else
     \@stripbangs#1!#2!#3\@endsetlabel%
  \fi
}
\def\@stripbangs#1!!\@endsetlabel{%
  \verbatim@line{#1}%
}


\verbatim@line{hello mama}

\newcommand{\numberedcodebackspace}{0.5\baselineskip}

\newcounter{codeline}
\newenvironment{numberedcode}
  {\endgraf
     \def\verbatim@processline{%
        \noindent
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
               %{\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\phantom{: \,}}}%
            \else
               \refstepcounter{codeline}%
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\llap{\arabic{codeline}}: \,}}%
            \fi
        \expandafter\setlabel\expandafter{\the\verbatim@line}%
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
          \vspace*{-\numberedcodebackspace}\par%
        \else
          \the\verbatim@line\par
        \fi}%
   \verbatim
   }
   {\endverbatim}

\makeatother

\newcommand\arrow{\rightarrow}

\newcommand\join{\sqcup}
\newcommand\slotof[1]{\ensuremath{s_{#1}}}
\newcommand\tempof[1]{\ensuremath{t_{#1}}}
\let\tempOf=\tempof
\let\slotOf=\slotof

\makeatletter
\newcommand{\nrmono}[1]{%
  {\@tempdima = \fontdimen2\font\relax
   \texttt{\spaceskip = 1.1\@tempdima #1}}}
\makeatother

\usepackage{times}  % denser fonts
\renewcommand{\ttdefault}{aett} % \texttt that goes better with times fonts
\usepackage{enumerate}
\usepackage{url}
\usepackage{graphicx}
\usepackage{natbib}  % redundant for Simon
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar

\usepackage[ps2pdf,bookmarksopen,breaklinks,pdftitle=dataflow-made-simple]{hyperref}

\newcommand\naive{na\"\i ve}
\newcommand\Naive{Na\"\i ve}

\usepackage{amsfonts}
\newcommand\naturals{\ensuremath{\mathbb{N}}}
\newcommand\true{\ensuremath{\mathbf{true}}}
\newcommand\implies{\supseteq}  % could use \Rightarrow?

\newcommand\PAL{\mbox{C{\texttt{-{}-}}}}
\newcommand\high[1]{\mbox{\fboxsep=1pt \smash{\fbox{\vrule height 6pt
   depth 0pt width 0pt \leavevmode \kern 1pt #1}}}}

\usepackage{tabularx}

%%
%% 2009/05/10: removed 'float' package because it breaks multiple
%% \caption's per {figure} environment.   ---NR
%%
%%  % Put figures in boxes --- WHY??? --NR
%%  \usepackage{float}
%%  \floatstyle{boxed}
%%  \restylefloat{figure}
%%  \restylefloat{table}



% ON LINE THREE, set \noauthornotestrue to suppress notes (or not)

%\newcommand{\qed}{QED}
\ifnotesinmargin
  \long\def\authornote#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}
\else
  % Simon: please set \notesinmarginfalse on the first line
  \long\def\authornote#1{{\em #1\/}}
\fi
\ifnoauthornotes
  \def\authornote#1{\unskip\relax}
\fi

\newcommand{\simon}[1]{\authornote{SLPJ: #1}}
\newcommand{\norman}[1]{\authornote{NR: #1}}
\let\remark\norman
\def\finalremark#1{\relax}
% \let \finalremark \remark % uncomment after submission
\newcommand{\john}[1]{\authornote{JD: #1}}
\newcommand{\todo}[1]{\textbf{To~do:} \emph{#1}}
\newcommand\delendum[1]{\relax\ifvmode\else\unskip\fi\relax}

\newcommand\secref[1]{Section~\ref{sec:#1}}
\newcommand\secreftwo[2]{Sections \ref{sec:#1}~and~\ref{sec:#2}}
\newcommand\seclabel[1]{\label{sec:#1}}

\newcommand\figref[1]{Figure~\ref{fig:#1}}
\newcommand\figreftwo[2]{Figures \ref{fig:#1}~and~\ref{fig:#2}}
\newcommand\figlabel[1]{\label{fig:#1}}

\newcommand\tabref[1]{Table~\ref{tab:#1}}
\newcommand\tablabel[1]{\label{tab:#1}}


\newcommand{\CPS}{\textbf{StkMan}}    % Not sure what to call it.


\usepackage{code}   % At-sign notation

\iftimestamp
\input{timestamp}
\preprintfooter{\mdfivestamp}
\fi

\hyphenation{there-by}

\begin{document}
%\title{\ourlib: Dataflow Optimization Made Simple}
\title{Implementing Dataflow Analysis and Optimization by Lifting Node Functions to Basic Blocks and Control-Flow Graphs}
%\subtitle{\today}

%\titlebanner{\textsf{\mdseries\itshape Submitted to the 2010 ACM Symposium on Principles
%    of Programming Languages (POPL)}}

\ifnoauthornotes
\makeatletter
\let\HyPsd@Warning=
                \@gobble
\makeatother
\fi

% João


\authorinfo{Norman Ramsey}{Tufts University}{nr@cs.tufts.edu}
\authorinfo{Jo\~ao Dias}{Tufts University}{dias@cs.tufts.edu}
\authorinfo{Simon Peyton Jones}{Microsoft Research}{simonpj@microsoft.com}


\maketitle
 
\begin{abstract}
\remark{Needs a complete rewrite}
We present \ourlib, a Haskell library that makes it easy for compiler writers
to implement program transformations based on dataflow analyses.
The compiler writer must identify (a)~logical assertions
on which the transformation will be based;
(b)~a~representation of such assertions, which
\ifcutting
should form a lattice of finite height;
\else
must have a lattice structure
 such that every assertion can be increased at
most finitely many times;
\fi
(c)~transfer functions that approximate weakest preconditions or
strongest postconditions over the assertions; and
(d)~rewrite functions whose soundness is justified by the assertions.
%%  To~guide compiler writers,
%%  we show how dataflow analyses are related to
%%  seminal work on program 
%%  correctness. \simon{The ``next 700'' section sort of does so, but I'm not 
%%  sure it deserves mention in the abstract.}
\ourlib\ uses the algorithm of 
\citet{lerner-grove-chambers:2002}, which 
% enables compiler writers to
can
compose very simple analyses and transformations in a way that achieves
the same precision as complex, handwritten
``super-analyses.''
\ourlib\ will be the workhorse of a new
back end for the Glasgow Haskell Compiler (version~6.12, forthcoming).

\emph{Reviewers:} code examples are indexed at {\small\url{http://bit.ly/jkr3K}}
%%% Source: http://www.cs.tufts.edu/~nr/drop/popl-index.pdf
\end{abstract}

\makeatactive   %  Enable @foo@ notation

\section{Introduction}

%%% \simon{``Control-flow graph'' (hyphenated) should be consistent
%%% now.}
%%%   Confirmed, and expunge a couple of unnecessary hyphens (NR).

\remark{This is an intro the old paper.  We need an intro to the new paper.}


If you write a compiler for an imperative language, you can exploit
many years' work on code improvement
(``optimization'').
The work is typically presented
as a long list of analyses and
transformations, each with a different name.
This presentation makes optimization appear complex and difficult.
Another source of complexity is the need for synergistic combinations
of optimizations; you may have to write one ``super-analysis'' per
combination. 

But optimization doesn't have to be complicated.
Most optimizations
work by applying well-understood techniques for
reasoning about programs:
assertions about states, assertions about continuations, and
substitution of equals for equals.  % (\secref{next-700}).
What makes optimization different from classic reasoning techniques
is that in dataflow optimization, assertions are approximated,
and all assertions are computed automatically.

This paper presents \ourlib\ (higher-order optimization library), 
a~Haskell library that makes it easy to implement dataflow optimizations.
Our contributions are as follows:
\begin{itemize}
\item
\ourlib\ defines a simple interface for implementing analyses and transformations:
you provide representations for assertions and for functions that
transform assertions, 
and \ourlib\ computes assertions
 by setting up and solving recursion
equations.
Additional functions you provide use computed
assertions to justify program transformations.
Analyses and transformations built on \ourlib\ 
are small, simple, and easy to get right.
\item
% using LGC, simple implementations of complicated super-analyses using interleaving and speculative
% rewriting
Using the sophisticated algorithm of \citet{lerner-grove-chambers:2002},
%which is \emph{not} easy to get right,
\ourlib\ can perform super-analyses by \emph{interleaving}
simple analyses and transformations.
Interleaving is tricky to implement,
but by using 
generalized algebraic data types 
and continuation-passing style\john{No longer true.},
our new implementation expresses the algorithm with a clarity and a
degree of 
static checking that has not 
previously been achieved.
\item
\ourlib\ helps you write correct optimizations:
it
statically rules out transformations that violate invariants
of the control-flow graph,
and dynamically it can help find the first transformation that introduces a fault
in a test program \cite{whalley:isolation}.
\item
\ourlib's polymorphic, higher-order design makes it reusable
with many languages.
\hoopl\ is designed to help optimize imperative code
with arbitrary control flow,
including low-level intermediate languages and machine languages.
As \citet{benitez-davidson:portable-optimizer} have shown, all the
classic scalar and loop optimizations can be performed over such
codes.
\end{itemize}






% %Many presentations obscure fundamental principles of
% %code improvement.
% %
% But~most optimizations
% work by applying reasoning techniques that have long been understood
% and used 
% to reason about programs:
% assertions about states, assertions about continuations, and
% substitution of equals for equals.  % (\secref{next-700}).
% % What distinguishes dataflow optimization from classic formal reasoning
% % about programs is that in dataflow optimization, all assertions are
% % computed automatically, and they are
% % \emph{approximated}. 
% 
% This paper presents \ourlib\ (higher-order optimization library), 
% a Haskell library the implements dataflow optimizations using
% simple representations of assertions and the functions that transform
% them.
% Its contributions are as follows:
% %The contribution of this paper is to elucidate a large body of work on code
% %improvement; the body of work known as ``dataflow optimization.''
% %This paper makes two contributions:
% \begin{itemize}
% \item
% \ourlib\ is engineered to make it \emph{easy} to implement
% dataflow-based 
% code-improvement techniques, even in a purely functional setting.
% When built on \ourlib, analyses and transformations
% are small, simple, and easy to get right.
% \item 
% Using the sophisticated algorithm of \citet{lerner-grove-chambers:2002},
%  which is \emph{not} easy to get right,
% \ourlib\ \emph{interleaves} analysis and transformation.
% Our new implementation, which uses generalized
% algebraic data 
% types % \cite{xi:guarded-recursive} 
% and continuation-passing style,
% %By~exploiting Strachey's ideas about compositional semantics and
% %the extra type checking possible with GADTs, we implement
% expresses the algorithm with a clarity and a degree of
% static checking that has not 
% previously been achieved.
% \item
% \ourlib's polymorphic, higher-order design makes it easier to reuse
% these techniques than ever before.
% \end{itemize}
% %
% \hoopl\ is designed to help optimize imperative procedures (after
% inlining). 
% \ourlib\ supports local-level codes with arbitrary control flow,
% including intermediate 
% languages and machine 
% languages.
% As \citet{benitez-davidson:portable-optimizer} have shown, all the
% classic scalar and loop optimizations can be performed over such
% codes.



We introduce dataflow optimization by analyzing and transforming
example code (\secref{example:xforms}),
thinking about and justifying classic optimizations using
Hoare logic and substitution of equals for equals.
To~support our claim that \ourlib\ makes dataflow optimization easy, 
we explain how
to create new dataflow analyses and transformations
(\secref{making-simple}), and we show complete implementations of significant
analyses (\secref{example-analyses}) and transformations
(\secref{example-rewrites}) from the Glasgow Haskell Compiler.
We also sketch a new implementation of interleaving (\secref{engine}).

\subsection{The new story}

Just notes!

One story of this paper is that we can think similar thoughts about
assembly code, IR nodes, basic blocks, and control-flow graphs.
Morever these thoughts are reflected in the types.
Observations:
\begin{itemize}
\item
In compilation and optimization, things are always easiest to analyze
when there is a single entry and single exit.  (Lesson of structured
programming.)
\item
In assembly code, we introduce labels (not present in the hardware) to
be able to code what we mean by branch targets.
\item
In a control-flow graph, unlike in assembly code, \emph{every} edge
out of a control-transfer instruction is associated with a label.
This invariant simplifies the representation and gives us freedom to
lay out assembly code in an order that, e.g., is good for the I-cache,
or that minimizes branch instructions on hot paths.  (Foreshadow that
blocks in our @Graph@ body are unordered.
\item
We distinguish labels, computational instructions, control transfers.
They are distinguished by the number of predecessors and successors.
We~express this distinction in Haskell by attaching to each
entry and exit point one of two types: \emph{open} means there is exactly
one predecessor or successor, and control can ``fall through;''
\emph{closed} means that control cannot fall through, but can be
transferred only by an explicit branch---but the \emph{number} of
successors or predecessors is not constrained \emph{a~priori}.
\item
The open/closed concepts lifts to sequences of straight-line code and
to full control-flow graphs.
As~a special case, a sequence of straight-line code that is closed at
both ends---beginning with a label and ending with a control
transfer---is the familiar \emph{basic block}.
\item
By making open/closed part of the types, we can ensure that a client
of \ourlib{} builds only sequences and graphs that respect our
invariants. 
Moreover, the types provide guidance (really???)
\end{itemize}


A second story of this paper is that we can think of dataflow analysis
as a specialization of program logic.
\begin{itemize}
\item
A forward dataflow analysis is represented by a predicate transformer
that is related to \emph{strongest postconditions} \cite{floyd}.
\item
A backward dataflow analysis is represented by a predicate transformer
that is related to \emph{weakest preconditions} \cite{dijkstra:discipline}.
\end{itemize}
A~client of \ourlib{} defines a type of node, a type of predicate
(aka dataflow fact), and a function which, given a node, returns a
predicate transformer.
The beauty of our approach is that we can \emph{lift} a client's
predicate transformer from nodes into sequences and control-flow
graphs.
Moreover, with help from the client, we can iterate the transformer
over an entire function body
until it reaches a fixed point, the result of which is a dataflow
analysis.

In addition, we can do Lerner/Grove/Chambers.

Finally, our implementation has some especially wonderful Haskell
(Simon please flesh out these bits):
\begin{itemize}
\item
Our open/closed tags aren't just phantom types.  By virtue of GADTs,
they are real. 
And we get exhaustiveness checking, etc --- the works.
\item
There's more!  By using higher-rank polymorphism, we can define a
\emph{single} kind of widget, then instantiate it at nodes, blocks,
graphs.
SO WHAT?  This is wonderful because\ldots
\item
More wonderfulness?
\end{itemize}




\section{Dataflow analysis {\&} transformation by \rlap{example}}
\seclabel{constant-propagation}
\seclabel{example:transforms}
\seclabel{example:xforms}

We begin by setting the scene, introducing some vocabulary, and
showing a small motivating example.
A control-flow graph, perhaps representing the body of a procedure,
is a collection of \emph{basic blocks}---or just ``blocks''.
Each block is a sequence of instructions,
beginning with a label and ending with a
control-transfer instruction that branches to other blocks.
% Each block has a label at the beginning,
% a sequence of 
%  -- each of which has a label at the 
% beginning.  Each block may branch to other blocks with arbitrarily
% complex control flow.
The goal of dataflow optimization is to compute valid
\emph{assertions} (or \emph{dataflow facts}), 
then use those assertions to justify code-improving
transformations (or \emph{rewrites}) on a \emph{control-flow graph}.  

Consider a concrete example: constant propagation with constant folding.
On the left we have a basic block; in the middle we have
assertions that hold between statements (or \emph{nodes}) 
in the block; and at
the right we have the result of transforming the block 
based on the assertions:
\begin{verbatim}
  Before        Facts        After
                  {}
  x := 3+4                   x := 7
                 {x=7}
  z := x>5                   z := True
              {x=7, z=True}
  if z                       goto L1
   then goto L1
   else goto L2
\end{verbatim}
Constant-propagation works
from top to bottom.  We start with the empty assertion.  
Given the empty assertion and the node @x:=3+4@ can we make a (constant-folding)
transformation?
Yes!  We can replace it with @x:=7@.
\simon{Is the distinction between constant-folding and constant-prop really
helpful here, or is it just pedantry?  One would hardly want do to constant-prop
without constant folding!  And probably vice versa.}
\john{I think it's important for 3 reasons: (1) any compiler text will describe
them separately, (2) the code is simpler when they're implemented separately
(the payoff of this approach),
(3) constant-folding can be useful without constant-prop (as in QC--)
[I~agree ---NR]}
Now, given this transformed node,
and the original assertion, what assertion flows out of the bottom of
the transformed node?  
The assertion \{@x=7@\}.  
Given the assertion \{@x=7@\} and the node @z:=x>5@, can we make a
transformation?  Yes: replace the node with @z:=True@.
And so the process continues to the end of the bloc, where we
transform
a conditional branch to an unconditional one, @goto L1@.

\seclabel{simple-tx}
\paragraph{Interleaved transforamtion and analysis.}
Notice that our example \emph{interleaves} transformation and analysis;
interleaving makes it easier to write effective analyses.
If, instead, we \emph{first} analysed the block
and \emph{then} transformed it, the analysis would have to ``predict''
the transformations.
For example, given the incoming fact \{@x=7@\}
and the instruction @z:=x>5@,
a pure analysis would produce the outgoing fact
\{@x=7@, @z=True@\} by simplifying @x>5@ to @True@.
But the subsequent transformation must perform
\emph{exactly the same simplification} when it transforms the instruction to @z:=True@!
If instead we \emph{first} rewrite the node to @z:=True@, 
and \emph{then} apply the transfer function to the rewritten node, 
the transfer function becomes laughably simple: it merely has to see if the
right hand side is a constant.
The gain is even more compelling if there are a number of interacting 
analyses and/or transformations;
for more substantial
examples, consult \citet{lerner-grove-chambers:2002}.

\paragraph{Forwards and backwards.}
Constant propagation works \emph{forwards}, and a fact is typically an
assertion about the program state (such as ``variable @x@ holds value
@7@'').  Many useful analyses work \emph{backwards}, from bottom to
top.  An example is live-variable analysis, where the fact takes the form
``variable @x@ is dead'' and constitutes an assertion about the
\emph{continuation} of a program point.  For example, the fact ``@x@ is
dead'' at a program point P is an assertion that @x@ is unused on any program
path starting at P.  
The accompanying transformation is called dead-code elimination;
if @x@~is dead, this transformation rewrites
the assignment @x=e@ into a no-op.

% ----------------------------------------
\section{Representing control-flow graphs} \seclabel{graph-rep}

\ourlib{} is a library that makes it easy to define dataflow analyses,
and transformations driven by these analyses, on control-flow graphs.
Graphs are composed from smaller units, which we discuss from the
bottom up:
\begin{itemize}
\item A \emph{node} is defined by the library client;
\ourlib{} knows nothing about the representation of nodes (\secref{nodes}).
\item A basic \emph{block} is a sequence of nodes (\secref{blocks}).
\item A \emph{graph} is an arbitrarily complicated control-flow graph,
composed from basic blocks (\secref{graphs}).
\end{itemize}

\subsection{Nodes} \seclabel{nodes}

The primitive constituents of a \ourlib{} control-flow graph are
\emph{nodes}, which are defined by the client.
Typically, a node might represent a machine instruction, such as an
assignment, a call, or a conditional branch.  
But \ourlib{}'s graph representation is polymorphic in the node type,
so each client can define nodes as it likes.
Because they contain nodes defined by the client,
graphs can include arbitrary client-specified data, including
(say) C~statements, method calls in an object-oriented language, or
whatever.

Each node must be \emph{open or closed at entry}
and \emph{open or closed at exit}.  
An \emph{open} point is one at which control may implicitly ``fall through;''
to  transfer control at a \emph{close} point requires an explicit
control-tranfer instruction.
For example,
\begin{itemize}
\item A multiply instruction is open on entry (because control can fall into it
from the preceding instruction), and open on exit (because control falls through
to the next instruction).
\item An unconditional branch is open on entry, but closed on exit (because 
control cannot fall through to the next instruction).
\item A label is closed on entry (because in \ourlib{} we do not allow
control to fall through into a branch target), but open on exit.
\end{itemize}
This taxonomy enables \ourlib{} to enforce invariants:
only nodes
closed at entry can be the targets of branches, and only nodes closed
at exits can transfer control (see also \secref{edges}).
As~a consequence, all control transfers originate at control-transfer
instructions and terminated at labels; this invariant dramatically
simplifies analysis and transformation. 
\footnote{%
To obey these invariants,
a conditional-branch instruction, which typically may either transfer control
\emph{or} fall through, must be represented as a two-target
conditional branch, with the fall-through path in a separate block.  
This representation is standard \cite{appel:modern},
and it costs nothing in practice:
a late-stage code-layout pass can readily reconstruct efficient code.
}

\begin{figure}
\begin{code}
data `Node e x where
  `Label      :: BlockId -> Node C O
  `Assign     :: Reg     -> Expr -> Node O O
  `Store      :: Expr    -> Expr -> Node O O
  `Branch     :: BlockId -> Node O C
  `CondBranch :: Expr -> BlockId -> BlockId -> Node O C
    ...more constructors...
\end{code}
\caption{A typical node type, as it might be defined by a client} 
\figlabel{cmm-node}
\end{figure}

\ourlib{} knows \emph{at compile time} whether a node is open or
  closed at entry and exit:
the type of a node has kind @*->*->*@, where the two type paramters
are type-level flags, one for entry and one for exit.
Such a type parameter may be instantiated only with type @O@~(for
open) or type~@C@ (for closed).
As an example,
\figref{cmm-node} shows a typical node type as it might be written by
one of \ourlib's \emph{clients}.
The type parameters are written @e@ and @x@, for
entry and exit respectively.  
The type is a generalized algebraic data type, and
it is defined in using syntax
that gives the type of each constructor.  
\cite{peyton-jones:unification-based-gadts}.
For example, constructor @Label@
takes a @BlockId@ and returns a node of type @Node C O@, where ``@C@''~says ``closed
at entry'' and ``@O@''~says ``open at exit''.  
(We often abbreviate ``a node of type (@Node@ @C@ @O@)''
to ``a closed/open node'',  
and similarly for blocks and graphs.)
The types @BlockId@, @O@, and @C@ are 
exported by \ourlib{} (\figref{graph}).  

Similarly, an @Assign@ node takes a register and an expression, and
returns a @Node@ open at both entry and exit; the @Store@ node is
similar.  The types @Reg@ and @Expr@ are private to the client, and
\ourlib{} knows nothing of them.  
Finally, the control-transfer nodes @Branch@ and @CondBranch@ are open at entry
and closed at exit.  

In general, nodes closed on entry are the only targets of control transfers;
nodes open on entry and exit never perform control transfers;
and nodes closed on exit always perform control transfers.
We often call these \emph{first}, \emph{middle}, and \emph{last} nodes
respectively, because of the position each type of node occupies in a
basic block.

\subsection{Blocks} \seclabel{blocks}

\begin{figure}
\begin{code}
data `O   -- Open
data `C   -- Closed

data `Block n e x where
 `BUnit :: n e x -> Block n e x
 `BCat  :: Block n e O -> Block n O x -> Block n e x

newtype 'BlockId = BlockId Int
type `Graph n = BlockMap (Block n C C)
-- (BlockMap a) is a finite map from BlockId to 'a'

data `PGraph n e x where
 `PNil  :: PGraph n O O
 `PUnit :: Block n O O -> PGraph n O O
 `PMany :: Head n e -> Graph n
       -> Tail n x -> PGraph n e x

data `Head n e where
 `NoHead :: BlockId -> Head n C
 Head   :: Block n O C -> Head n O

data Tail n x where
 `NoTail :: Tail n C
 `Tail   :: BlockId -> Block n C O -> Tail n O

-- Building Graphs
addBlock   :: BlockId -> Block n C C
           -> Graph n -> Graph n
unionGraph :: Graph n -> Graph n -> Graph n
\end{code}
\caption{The \ourlib{} Block and Graph types} 
\figlabel{graph}
\end{figure}

The types for blocks and graphs are defined by \ourlib{}
itself, with definitions given in \figref{graph}.
A @Block@ is parameterised over the node type @n@
as well as over the same open/closed flags as nodes.  
Since the node type is itself parameterised over its open/closed flags,
@n@ has kind @*->*->*@.

The @BUnit@ constructor lifts a node to become a block, while @BCat@
concatenates two blocks. Note carefully the type of @BCat@: it
requires its first argument to be open on exit, and its second to be
open on entry.  This ensures that control can fall through from the
first into the second; for example, putting a @Branch@ before a
@Assign@ is statically prevented.

A @Block@ closed at both entry and exit is what we normally 
call ``a basic block''.  
In the case of @Node@, for example,
the types elegantly ensure that a block of type (@Block@ @Node@ @C@ @C@)
consists of a @Head@, followed
by a sequence of @Assign@ and @Store@ nodes, and finishing with a @Branch@
or @CondBranch@.  
Also notice that the type guarantees that every @Block@ 
has at least one node.


\subsection{Graphs} \seclabel{graphs}

\ourlib{} defines two distinct types of control-flow graphs.  The first,
@Graph@, is entirely straightforward (see \figref{graph}): it is simply
a collection of closed/closed @Block@s, represented as a
a finite mapping from a @BlockId@ to @Block@.  We choose this representation
to stress that each @Block@ in the graph has a distinct entry @BlockId@.
A procedure might be represented by a @Graph@ together with the @BlockId@
at which execution starts.

As we saw in \secref{constant-propagation}, the \ourlib{} client specifies
a rewrite function that optionally rewrites a node.  But what does it
rewrite the node \emph{to}?  At first one might
expect that rewriting returns a new node, but that is not enough.
For example, perhaps we want to replace a high-level operation
with a tree of conditional branches or a loop, which entails introducing new blocks.
Another important special case is rewriting a node to a no-op.
So in general the rewrite function must return an \emph{arbitrary graph}.
But this graph cannot be a @Graph@, because a @Graph@ is closed at both ends;
indeed a @Graph@ does not even have shape parameters @e@ and @x@.
Rather, the rewrite function should return a ``partial graph'', 
or @PGraph@, which can be open or closed at either
end just like a node or block.  

The definition of @PGraph@ is given in \figref{graph}.
It includes a @PNil@ constructor for the empty graph,
and a @PUnit@ constructor to lift an @Block@ to be @PGraph@.
We will soon return to the question
of why @GUnit@ is restricted to blocks open on exit.
\simon{Norman thinks we need a closed-closed empty
graph but I don't see that, at least not yet.}

The most general form of @PGraph@ uses the @PMany@ constructor,
which has three fields:
\begin{itemize}
\item The first field is called the ``head'' of the @PGraph@.
If it is $@(Head@\;b@)@$, the @PGraph@ is open on entry, with initial block $b$.
If it is $@(NoHead@\;l@)@$, the @PGraph@ is closed on entry, and execution begins
at block $l$. We need the @BlockId@ $l$, because the @PGraph@ 
may replace a node, so just as a node closed at entry has a unique entry point,
so does a @PGraph@.
\item The second argument is simply an arbitrary @Graph@ of blocks,
as already discussed.
\item The third field is the ``tail'' of the @PGraph@.  If
the @Graph@ is closed on exit the tail is just @NoTail@.  If it is
open on exit, the tail is a (labelled) @Block@, open on exit.
\end{itemize}
Partial graphs compose nicely.  For example, concatenation is easy to define:
\par{\small
\begin{code}
`pCat :: Graph n e O -> Graph n O x -> Graph n e x
pCat PNil g2 = g2
 pCat g1 PNil = g1

pCat (PUnit b1) (PUnit b2)             
 = PUnit (b1 `BCat` b2)

pCat (PUnit b) (PMany (Head e) bs x) 
 = PMany (Head (b `BCat` e)) bs x

pCat (PMany e bs (Tail l x)) (PUnit b2) 
 = PMany e bs (Tail l (x `BCat` b2))

pCat (PMany e1 bs1 (Tail l x1)) (PMany (Head e2) bs2 x2)
 = PMany e1 (addBlock l (x1 `BCat` e2) bs1 
   `unionBlocks` bs2) x2
\end{code}}
This defintion is a nice example of the power of GADTs: the
pattern-matching is exhaustive, and all the open/closed invariants are
statically checked.  For example, in the last equation for @pCat@, the
first argument has type (@Graph@ @n@ @e@ @O@), so the third field of
@GMany@ must be of form (@Tail l bx1@).  Moreover @bx1@ must be
closed/open, and @be2@ must be open/closed.  So we can compose them in
sequence with @BCat@ to produce a closed/closed block, which can be
added to the @BlockGraph@ of the result.

Every graph has a unique representation.  An empty graph is represented
by @GNil@.  The representation of a @PGraph@ consisting of a single block @b@ 
depends on the shape of @b@, thus:
\begin{center}
\begin{tabular}{ll}
O/O & @PUnit b@ \\
O/C & @PMany (Head b) emptyG NoTail@ \\
C/O & @PMany (NoHead l) emptyG (Tail l b)@ \\
C/C & @PMany (NoHead l) (unitG l b) NoTail@
\end{tabular}
\end{center}
where @l@ is the entry @BlockId@ of @b@.
Multi-block graphs are similar.
From this table we can see why @PUnit@ is restricted to open/open
blocks; if it was more accommodating then there would be 
multiple representations for a single graph.

\subsection{Summary}

Nodes, blocks, and partial graphs share some 
characteristics in common.  Suppose E is a node, block, or
graph:
\begin{itemize}
\item Regardless of whether E is open or closed, 
it has a unique entry point where execution begins.
\item If E is open at exit, control leaves E
\emph{only} by ``falling through'' from a unique exit point.
\item If E is closed at exit, control leaves \emph{only}
by explicit branches from closed-on-exit nodes.
\end{itemize}

% 
% \subsection{Labels and successors} \seclabel{edges}
% 
% \begin{figure}
% \begin{code}
% class Edges n where
%   blockId    :: n C x -> BlockId
%   successors :: n e C -> [BlockId]
% \end{code}
% \caption{The {\tt Edges} class} \figlabel{edges}
% \end{figure}
% If \ourlib{} knows nothing about nodes, how can it know to which block a
% control transfer goes, or what is the @BlockId@ at the start of a block?
% In this situation the standard Haskell idiom is to provide a type class whose methods
% provide exactly the needed operations; it is given in \figref{edges}.
% The @blockId@ method takes a first node (one closed on entry, \secref{nodes})
% and returns its @BlockId@;
% the @successors@ method takes a last node (closed on exit) and returns the @BlockId@s to
% which it can transfer control.  In \ourlib{} a middle node (open-open) cannot refer to any @BlockId@s,
% as the absence of an interrogation function implies. 
% 
% When defining a node type, a \ourlib{} client should also make it an
% instance of @Edges@.  Thus for @Node@ we would write
% \begin{code}
% instance Edges Node where
%   blockId (Label l) = l
%   successors (Branch b) = [b]
%   successors (CondBranch e b1 b2) = [b1,b2]
% \end{code}
% Again, the pattern-matching for both functions is exhaustive, and
% the compiler statically checks this fact.  For example, @blockId@ simply
% cannot be applied to a @Assign@ or @Branch@ node; the type checker would
% reject any such attempt.
% 
% \ourlib{} provides instances for its types @Block@ and @Graph@ thus:
% \begin{code}
% instance Edges n => Edges (Block n) where
%   blockId    (BUnit n)  = blockId n
%   blockId    (BCat b _) = blockId b
%   successors (BUnit n)  = successors n
%   successors (BCat _ b) = successors b
% 
% instance Edges n => Edges (Graph n) where
%   blockId    (GUnit b)      = blockId b
%   blockId    (GMany ...)    = ???  -- eek!
%   successors (GUnit b)      = successors b
%   successors (GMany _ bs _) = concatMap successors bs 
%                               `minusList` map blockId bs
% \end{code}
% \simon{I now realise that this is why I wanted to know the entry block of a graph!
% Since an entry-closed node and block both have a unique block id, shouldn't an entry-closed
% graph have one too?}
% \simon{Also the successors thing is not pretty.}
% 

\section {Using \ourlib{} to analyse and transform graphs} \seclabel{using-hoopl}
\seclabel{making-simple}
\seclabel{create-analysis}

Now that we have graphs in hand, we are ready to return to dataflow
optimisation.  The goal of \ourlib{} is to make it extremely easy for a
client to build a new dataflow analysis and optimisation.  To do this
the client must supply the following pieces:
\begin{itemize}
\item \emph{A node type} (\secref{graph-rep}).  
Then \ourlib{} supplies the @Block@ and @Graph@ types
that let the client build control-flow graphs out of nodes.
\item \emph{A data type of facts}, and some operations over 
those facts (\secref{facts}).
Each analysis uses its own analysis-specific 
facts, so \ourlib{} is completely polymorphic in 
the fact type.   
\item \emph{A transfer function} that takes an input fact and a node,
and produces the output fact that flows out of the other end of 
the node (\secref{transfers}).  
\item \emph{A rewrite function} that takes an input fact and a node,
and produces either @Nothing@,
or @(Just g)@ where @g@ is an \emph{arbitrary graph} that should
replace the node.  The ability to replace a node by an arbitrary graph is crucial;
We discuss the rewrite function
in \secref{rewrites}.
\end{itemize}
With these pieces in hand, a client can call \ourlib{}'s analysis-and-rewriting
function:
\begin{code}
  analyseAndRewriteFwd
  :: Edges n         
  => DataflowLattice f      -- Lattice
  -> ForwardTransfer n f    -- Transfer function
  -> ForwardRewrite n f     -- Rewrite function
  -> RewritingDepth         -- Rewrite recursively?
  -> Fact e f               -- Input fact(s)
  -> Graph n e x            -- Graph or subgraph
  -> FuelMonad (Fact x f,     -- Output fact(s)
                Graph n e x)  -- Rewritten graph
\end{code}
Roughly speaking, given a lattice, a transfer function, and a 
rewrite function, @analyseAndRewriteFwd@ can transform a @Graph@ 
into an optimised @Graph@.  
One thing that is immediately obvious from this type is that it is
polymorphic  in the type of nodes @n@ and facts @f@; these are entirely
under the control of the client.
In the rest of this section we flesh out other details of the 
interface to @analyseAndRewriteFwd@, leaving the implementation for \secref{engine}.

\subsection{Example: Constant propagation and constant folding}

Meanwhile, it may be helpful to see how @analyseAndRewriteFwd@ can
be called by a client.  
We will continue to use constant propagation with constant folding as a running example;
the complete example is shown in \figref{const-prop}.

For each register, the analysis may conclude one of three facts:
the register must store a constant value (Boolean or integer),
the register might store a non-constant value,
or nothing is known about the contents of the register (bottom).
We~represent these facts using a finite map from registers to dataflow facts
(@HasConst@).
A register with a constant value maps to @Just v@, where @v@ is the constant value;
a register with a non-constant value maps to @Just Top@;
and a register with an unknown value maps to @Nothing@.

The definition of the lattice (@constLattice@) is straightforward.
The bottom element is an empty map (nothing is known about the contents of any register).
We use the @stdMapJoin@ function to lift the join operation
for a single register (@constFactAdd@) up to the map containing facts
for all registers.

For the transfer function, there are two interesting kinds of nodes:
assignment and conditional branch.
In the first two cases for assignment, a register gets a constant value,
so we produce a dataflow fact mapping the register to its value.
In the thired case for assignment, the register gets a non-constant value,
so we produce a dataflow fact mapping the register to @Top@.
The last interesting case is a conditional branch where the condition
is a register.
Based on whether the conditional-branch flows to the true or false successor,
we can infer whether the value of the register is @True@ or @False@,
so we update the fact flowing to each successor accordingly.

Notice that we don't need to consider complicated cases such as
an assignment @x:=y@ where @y@ holds a constant value @k@.
Instead, we will rely on the interleaving of transformation
and analyses to first transform the assignment to @x:=k@,
which is exactly what our simple transfer function expects.
As we mentioned in \secref{simple-tx},
interleaving makes it possible to write
the simplest imaginable transfer functions, with no loss of performance.

The rewrite function for constant propagation
rewrites each use of a register to its constant value.
The @rewriteE@ function checks if the register has a constant
value and makes the substitution.
We use the auxiliary function @mapE@
to apply @rewriteE@ to each use of a register in each kind of node.

The rewrite function for constant folding (@simplify@) is similar;
the main purpose is to simplify expressions that contain only constants.
The function @exps@ simplifies a variety of simple expressions;
we then use the function @s@ to lift the simplification to nodes.
There is one additional case for the node-simplifying function:
we also convert a conditional branch with a known condition
to an unconditional branch.

Note that the @simplify@ function is polymorphic in the type of fact
because it does not need to rely on any dataflow facts.
For example, we don't need to~check whether a register holds a constant
value because we can rely on the constant-propagation transformation
to~replace the register with its constant value.

We can invoke the analysis by calling the function @analyseAndRewriteFwd@.
with the lattice, the transfer function, the rewrite functions,
an input dataflow fact (bottom),
and the input graph.
Because constant-propagation and constant-folding are mutually beneficial,
we use the @combine@ function and deep rewriting to apply both transformations
recursively.

\begin{figure}
{\small
\begin{code}
-- Types and definition of the lattice
data `HasConst = `Top | `B Bool | `I Integer
type `ConstFact = Map Reg HasConst
constLattice = DataflowLattice
  { fact_bot    = Map.empty
  , fact_extend = stdMapJoin constFactAdd }
  where
    constFactAdd new old = (c, j)
      where j = if new == old then new else Top
            c = if j == old then NoChange else SomeChange

-- Analysis: register has constant value
`regHasConst :: ForwardTransfer Node ConstFact
regHasConst f (Label l)           = f
regHasConst f (Assign x (Bool b)) = Map.insert x (B b) f
regHasConst f (Assign x (Int  i)) = Map.insert x (I i) f
regHasConst f (Assign x _)        = Map.insert x Top   f
regHasConst f (Store _ _)         = f
regHasConst f (Branch l)          = [(l, f)]
regHasConst f (Cond (Reg x) tid fid) 
  = [(tid, Map.insert x (B True)  f),
     (fid, Map.insert x (B False) f)]
regHasConst f (Cond _ tid fid)    = [(tid, f), (fid, f)]

-- Constant propagation
`constProp :: ForwardRewrite Node ConstFact
constProp facts node
  = fmap toAGraph (mapE rewriteE node)
  where
    rewriteE e@(Reg r) =
      = case M.lookup r facts of
          Just (B b) -> Just $ Bool b
          Just (I i) -> Just $ Int  i
          _          -> Nothing
    rewriteE e = Nothing

-- Simplification ("constant folding")
`simplify :: ForwardRewrite Node f
simplify _ (Cond (Bool b) t f)
  = Just $ Branch (if b then t else f)
simplify _ node = fmap toAGraph (mapE s_exp node)
  where
    s_exp (Binop Add (Int i1) (Int i2))
       = Just $ Int $ i1 + i2
    ....  -- more cases for constant folding

-- Rewriting expressions
`mapE :: (Expr    -> Maybe Expr) 
      -> Node e x -> Maybe (Node e x)
mapE f (Label _)    = Nothing
mapE f (Assign x e) = fmap (Assign x) $ f e
 ... more cases for rewriting expressions

-- Combining transformations
`combineFRW :: ForwardRewrite n f
            -> ForwardRewrite n f
            -> ForwardRewrite n f
combine r1 r2 
   = \n f -> case r1 n f of Nothing -> r2 n f
                            Just g  -> Just g

-- Running the analysis
`rewriteFwd  = analyseAndRewriteFwd constLattice regHasConst
                  (constProp `combineFRW` simplify) 
                  RewriteDeep 
                  graph noFacts
\end{code}}
\caption{The client for constant-propagation and constant-folding} \figlabel{const-prop}
\end{figure}

\subsection{Interleaved analysis and rewriting}

The user-level model of how @analyseAndRewriteFwd@ works is this.  The
function walks forward over the graph.  At each node, it applies the
rewrite function to incoming fact and the node.  If the rewrite
function returns @Nothing@, the node is retained as part of the output
graph, the transfer function is used to compute the downstream fact,
and we move on to the next node.

If the rewrite function returns @Just g@ then the next step is controlled
by the @RewritingDepth@ flag:
\begin{itemize}
\item @RewriteDeep@: the entire @analyseAndRewriteFwd@ function is applied to 
the rewritten graph, which in turn may be further rewritten.
\item @RewriteShallow@: the rewritten graph is analysed \emph{but not further rewritten}.
\end{itemize}
Deep rewriting is usually essential to achieve the results of
\citet{lerner-grove-chambers:2002}.
But shallow rewriting is also sometimes vital; for example, a rewrite that inserts
a spill before a call might risk repeatedly inserting the same spill if the 
rewrite was iterated. 
When deep rewriting is used, the rewrite functions must
ensure that the graphs they produce are not rewritten indefinitely (\secref{correctness}).

Notice that in either case, the nodes following the rewritten node see 
\emph{up-to-date} facts; that is, their input facts are computed from the result 
of rewriting upstream nodes.

\subsection{Dataflow lattices} \seclabel{lattices} \seclabel{facts}

\begin{figure}
\ifcutting
\begin{code}
data `ChangeFlag = `NoChange | `SomeChange
data `DataflowLattice a = DataflowLattice
 {`fact_bot    :: a,
  `fact_extend :: a -> a -> (ChangeFlag, a) }
\end{code}
\else
\begin{code}
data ChangeFlag = NoChange | SomeChange
data DataflowLattice a = DataflowLattice
 { fact_bot    :: a,
   fact_extend :: a -> a -> (ChangeFlag, a) }
\end{code}
\fi
\caption{Representation of a dataflow lattice} \figlabel{lattice-type} \figlabel{lattice}
\end{figure}

For any one analysis or transformation the client must define a type
of dataflow facts.  A dataflow fact typically represents an assertion
about a program point.  (A program point is simply an edge in the 
control-flow graph.)
\begin{itemize}
\item An assertion about all paths \emph{to} a program point is established
by a \emph{forwards analysis}. For example the assertion ``$@x@=@3@$'' at point P 
claims that variable @x@ holds value @3@ at P, regardless of the
path by which P is reached.
\item An assertion about all paths \emph{from} a program point is 
established by a \emph{backwards analysis}. For example, the 
assertion ``@x@ is dead'' at point P claims that no path from P uses 
variable @x@.
\end{itemize}

A~set of dataflow facts must form a lattice, and \ourlib{} must know
(a) the bottom element of the lattice and (b) how to join (take 
the least upper bound of) two elements.  To ensure that analysis
terminates, it is enough if no fact has more than finitely many
distinct facts above it, so that any sequence of joins will terminate.

In our constant-propagation example, a fact is a finite conjunction of
sub-facts, at most one for each variable.  Each sub-fact has the form ``@x=@$k$'',
for some constant $k$, or ``@x=@$\top$'' (meaning that there is 
no single value held by $x$ at this program point).  For example, 
consider this graph,
where we assume @L1@ is the entry point:
\begin{verbatim}
  L1: x=3; y=4; if z then goto L2 else goto L3
  L2: x=7; goto L3
  L3: ....
\end{verbatim}
Since control flows to @L3@ from two places,
we must join the facts coming from those two places.  The fact at the
end of block @L1@ is (@x=3@, @y=4@, @z=False@), while at the end of
@L2@ the fact is (@x=7@, @y=4@, @z=True@).  
\john{Note to self: implementation isn't propagating constant value from branch condition.}
Both blocks branch to @L3@
where they must be joined, giving
the fact (@x=@$\top$, @y=4@, @z=@$\top$) at @L3@, where the $\top$'s indicate
that the analysis cannot
nail down @x@ and @z@ to hold a single value at @L3@\footnote{
In this example @x@ and @z@ really do vary at @L3@, but in general
the analysis might simply be conservative.}.

In practice, joins are computed at labels.
If~$f_{\mathit{id}}$ is the fact currently associated with the
label~$\mathit{id}$, 
and if a transfer function propagates a new fact~$f_{\mathit{new}}$
into the label~$\mathit{id}$, 
the dataflow engine replaces $f_{\mathit{id}}$ with
the join  $f_{\mathit{new}} \join f_{\mathit{id}}$.
Furthermore, the dataflow engine wants to know if
  $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$,
because if not, the analysis has not reached a fixed point.

Concretely, @analyseAndRewriteFwd@ is polymorphic in the fact type
@f@, and requires an argument of type @DataflowLattice f@.  The latter
type is shown in \figref{lattice}, and tells \ourlib{} the bottom and
join operations on the lattice.  In practice, when computing a join,
it is often cheap to learn if the join is equal to one of the
arguments.  Moreover, as noted in the previous paragraph, that
knowledge is precisely what \ourlib{} needs to know.  Hence, instead
of requiring an equality operation on facts (which might be
expensive), we instead ask @fact_extend@ to return a @ChangeFlag@ as
well as the joined value.  The @ChangeFlag@ should be @NoChange@ if
the result is the same as the second argument, and @SomeChange@ if the
result differs.  (Note the assymmetry.)

In the case of constant propagation, \figref{const-prop} shows a
possible concrete represetation of the facts described above.  A fact
is represented as a finite map from a variable to a value of type
@Maybe (WithTop Const)@.
A~variable $x$ maps to @Nothing@ iff $x=\bot$;
$x$~maps to @Just Top@ iff $x=\top$;
and $x$~maps to $@Just@\, (@Elt@\, k)$ iff $x=k$.
Any one procedure has only
finitely many variables; only finitely many facts are computed at any
program point; and in this lattice any one fact can increase at most
twice.  These properties ensure that the dataflow engine will reach a
fixed point.


\subsection{The transfer function} \seclabel{transfers}

\begin{figure}
\begin{code}
type `ForwardTransfer n f 
  = forall e x. f -> n e x -> TailFactF x f 
type `BackwardTransfer n f 
  = forall e x. TailFactB x f -> n e x -> f

type `ForwardRewrite n f 
  = forall e x. f -> n e x -> Maybe (AGraph n e x)
             -> Maybe (AGraph n e x)
type `BackwardRewrite n f 
  = forall e x. TailFactB x f -> n e x 

type family   `TailFactF x f :: *
type instance TailFactF O f = f
type instance TailFactF C f = [(BlockId,f)]

type family   `TailFactB x f :: *
type instance TailFactB C f = BlockMap f
type instance TailFactB O f = f
\end{code}
\caption{Transfer functions for forward and backward analyses} 
  \figlabel{transfers}  \figlabel{rewrites}
\end{figure}
\ifpagetuning\vspace*{-1ex}\fi

A~transfer function is presented with the dataflow fact on the edge coming
into a node, and it computes dataflow facts on the outgoing edge.
In a forward analysis, the dataflow engine starts with the fact at the
beginning of a block and applies the transfer function to successive 
nodes in that block until eventually the transfer function for the last node
computes the facts that are propagated to the block's successors.
For example, consider this graph, with entry at @L1@:
\begin{code}
  L1: x=3; goto L2
  L2: y=x+4; x=x-1; 
      if x>0 then goto L2 else return
\end{code}
A forward analysis starts with the bottom fact \{\} at every label.
Analysing @L1@ propagates this fact forward, by applying the transfer 
function successively to the nodes
of @L1@, emerging with the fact \{@x=3@\} for @L2@.
This is joined with the existing (bottom) fact for @L2@.
Now the analysis propagates @L2@'s fact forward, again using the transfer
function, this time emerging with a new fact \{@x=2@, @y=7@\} for @L2@.
Again, this is joined with the existing fact for @L2@, and the process
is iterated until the facts for each label reach a fixed point.

But wait!  What is the \emph{type} of the transfer function?
If the node is open at exit, it takes a fact and the node and produces a fact.
But what if the node is \emph{closed} on exit?
In that case the transfer function must
produce a list of (@BlockId@,fact) pairs, one for each outgoing edge.  

\emph{So the type of the transfer function's result 
depends on the open or closed-ness of the node's exit.}
Fortunately, this dependency is precisely what Haskell's (recently added) 
indexed type families can express.
The relevant \ourlib{} definitions are given in \figref{transfers}.
A forward transfer function, of type (@ForwardTransfer@ @n@ @f@), 
is a function polymorphic in @e@ and @x@.  It takes a fact and a 
node of type (@n@ @e@ @x@), and produces an
outgoing ``fact-like thing'' of type (@TailFactF@ @x@ @f@).  The 
type constructor @TailFactF@
should be thought of as a type-level function; its signature is given in the
@type family@ declaration, while its definition is given by two @type instance@
declarations.  The first says that the fact-like thing coming out a node
open at exit is just a fact @f@. The second says that if the node is closed
on exit the outgoing thing is a list of @(BlockId,f)@ pairs.

All the \ourlib{} client needs to do is to write the transfer function, with
this type.  \figref{const-prop} shows the transfer function for our
constant-propagation example.

\ifpagetuning\enlargethispage{0.5\baselineskip}\fi


\subsection{The rewrite function} \seclabel{rewrites} \seclabel{example-rewrites}

\begin{figure}
\hfuzz=5.7pt % cheating again!
\begin{code}
type AGraph n e x 
  = BlockIdSupply -> (Graph n e x, BlockIdSupply)

`mkIfThenElse :: Expr -> AGraph Node O O
             -> AGraph Node O -> AGraph Node O O
mkIfThenElse p t e
  = withBlockIds 3 $ \[l1,l2,l3] ->
    let entry = BUnit (CondBranch p l1 l2)
        exit  = BUnit Head
        bg = flattenG l1 (t `catNode` Branch l3)
	       `unionBlocks`
             flattenG l2 (e `catNode` Branch l3)
        
    GMany entry bg (Tail l3 exit)
          
\end{code}
\caption{The \texttt{AGraph} type} \figlabel{agraph}
\end{figure}

We compute dataflow facts in order to enable code-improving
transformations on control-flow graphs.
In our constant-propagation example, the dataflow facts may enable us
to simplify an expression by performing constant folding, or to 
turn a conditional branch into an unconditional one.
Similarly, a liveness analysis may allow us to 
rewrite an assignment to a dead register by a no-op; knowing that a register is
live at a call site might provoke us to insert a spill before the call; and so on.


The @analyseAndRewriteFwd@ takes a \emph{rewriting function}
that takes a node and a fact, and optionally returns a graph. 
The type is given in \figref{rewrites}.
The @Maybe@ type expresses that the function can return
@Nothing@, meaning no rewrite, or (@Just g@) where @g@ is the replacement graph.
Notice that if the rewrite function returns a grpah, it is statically
guaranteed to have
\emph{the same open/closed shape as the node being rewritten}.
So a branch instruction, for example, can only be rewritten to a graph 
closed at the exit.

You will have noticed that the rewriter actually 
returns an @AGraph@ rather than a @Graph@.
The reason for this is that 
if the rewriter makes graphs containing blocks,
it will need to conjure up some fresh @BlockIds@.
An @AGraph@ makes it possible for the rewriter to do so conveniently,
using @withLabel@:
\par{\small
\begin{code}
`withLabel :: (BlockId -> AGraph n e x) -> AGraph n e x
withLabel fn = \(b:bs) -> (fn b, bs)
\end{code}}

\subsection{Throttling the dataflow engine using ``optimization fuel''}
\seclabel{vpoiso}
\seclabel{fuel}

Writing and debugging an optimization can be tricky:
an optimization may rewrite the instructions in a procedure hundreds
of times, and any of those rewrites could be incorrect.
To debug such an optimization, we use a powerful technique,
first suggested by Whalley \cite{whalley:isolation},
to identify the first rewrite that 
transforms the procedure from working code to faulty code.

The key idea is to~limit the number of rewrites that
are performed while optimizing a procedure.
Because each rewrite leaves the observable behavior of the
program unchanged, it is safe to suppress rewrites at any point.
Therefore, given a program that fails when compiled with optimisation,
the client can use binary search to halt rewriting just before or just after the
rewrite that introduces the failure, thereby identifying the buggy rewrite.
We control the number of rewrites by giving the optimiser
a fixed quantity of ``optimization fuel'';
each rewrite costs one unit of fuel,
and when the fuel is exhausted, no more rewrites are performed.

You will have noticed that the type of @analyseAndRewriteFwd@
returns a value in the @FuelMonad@ (\secref{using-hoopl}).
The @FuelMonad@ is a simple state monad maintaining the supply of unused
fuel.  It also holds a supply of fresh labels, which are used by the rewriter
for making new blocks; more precisely, the client-provided @ForwardRewrite@
function produces an @AGraph@ (\figref{rewrites}) which Hoopl converts to
a @Graph@ by supplying the @AGraph@ with a supply of @BlockIds@ from the @FuelMonad@.

\subsection{Fixpoints and speculative rewrites}

The alert reader will be wondering what happens to rewriting in the presence of
loops.  Many analyses compute a fixpoint starting from unsound
``facts''; for example, a live-variable analysis starts from the
assumption that all variables are dead.  This means \emph{rewrites
performed in any iteration other than the last may be unsound, and
must be discarded}.  That is, each iteration of the fixpoint must
start afresh with the original graph.  

\emph{Nevertheless, those rewrites
should still be performed} (albeit tentatively, but possibly recursively)
so that the correct downstream facts can be computed as accurately as possible.
For example, consider this graph, with entry at @L1@:
\par{\small
\begin{code}
  L1: x=0; goto L2
  L2: x=x+1; if x==10 then goto L3 else goto L2
\end{code}}
The first traversal of block @L2@ will start with the ``fact'' \{x=0\};
but it will propagate the new fact \{x=1\} to @L2@, which will join the
existing fact to get \{x=$\top$\}.
But now suppose that the predicate in the conditional branch was @x<10@ instead
of @x==10@.  Again the first iteration will begin with the tentative fact \{x=0\}.
Using that fact we can rewrite the conditional branch to an unconditional
branch @goto L3@.  Hence no new fact is propagated to @L2@, and we have successfully
cut the loop.  This example is a little contrived, but it illustrates that 
for best results we should:
\begin{itemize}
\item Perform the rewrites on every iteration
\item Begin each new iteration with the orignal, virgin graph.
\end{itemize}
This sort of thing is hard to achieve in an imperative setting, where rewrites
mutate a graph in-place.  But it is trivially easy in a functional program:
all we need do is to revert to the original graph at the start
of each fixpoint iteration.

\john{Is it just me, or have we lost all the text claiming that analyses should
  just be strongest postconds/ weakest preconds? Is this by design?}
\simon{I did remove some material of that sort.  I think I did so because it didn't
seem (to me) to contribute much at the place it then appeared. But I am entirely open
to resurrecting it if you think that would help.  Perhaps in the section on 
correctness, which follows next?  Can you do that?}

\subsection{Correctness} \seclabel{correctness}

\simon{This section seems a bit laboured}

Facts computed by @analyseAndRewriteFwd@ depend on graphs produced by the rewrite
function, which in turn depend on facts computed by the transfer function.
How~do we know this algorithm is sound, or even if it terminates?
A~proof requires its own POPL paper
\cite{lerner-grove-chambers:2002}, but we can give some
intuition.

The transfer function must be 
\emph{monotonic} in the following sense: given a more informative fact in,
it should produce a more informative fact out.

The rewrite function must, of course, be \emph{sound}:
if it replaces a node @n@ by a replacement graph
@g@, then @g@ must be observationally equivalent to @n@ under the 
assumptions expressed by the incoming dataflow fact @f@.%
\footnote{We do not permit a transformation to change
  the @BlockId@ of a node. We have not found any optimizations
  that are prevented (or even affected) by this restriction.}
Moreover the rewrite function must be \emph{consistent} with the transfer function;
that is, \mbox{@transfer n f@ $\sqsubseteq$ @transfer g f@}.
For example, if the analysis says that @x@ is dead before the node~@n@,
then it had better still be dead if @n@ is replaced by @g@.
Also, to ensure termination, a transformation that uses deep rewriting
must not return replacement graphs which 
contain nodes that could be rewritten indefinitely.

Without the conditions on montonicity and consistency,
our algorithm will still terminate,
but there is no guarantee that it will compute
a fixed point of the analysis.
Given these conditions:

\begin{itemize} 
\item
The algorithm is sound because if each rewrite is sound (in the sense given above), 
then applying a succession of rewrites is also sound.
Moreover, a~sound analysis of the rewritten graph
may generate only dataflow facts that could have been
generated by a more complicated analysis of the original graph.
\item
No matter what the transfer functions and rewrite functions do,
the dataflow engine uses the dataflow lattice's join operation to ensure that
facts at labels never decrease. 
As~long as
\ifcutting
 no fact may
\else
 the lattice permits no fact to 
\fi
increase infinitely many
times, analysis
\ifcutting
 terminates.
\else
 is guaranteed to terminate.
\fi
\end{itemize}
Thus to guarantee soundness and termination, client code must supply 
sound transfer functions,
sound rewrite functions,
and a
lattice with no infinite ascending chains.

% \john{If we haven't sold the audience by now, they're not going
%   to be convinced by this quote.}
%
% Why use such a complex algorithm?
% \ifcutting Because interleaving \else
% \citet{lerner-grove-chambers:2002} write
% \begin{quote}
% \emph{Previous efforts to exploit [the mutually beneficial
% interactions of dataflow analyses] either (1)~iteratively performed
% each individual analysis until no further improvements are discovered
% or (2)~developed [handwritten] ``super-analyses'' that manually
% combine conceptually separate analyses. We have devised a new approach
% that allows analyses to be defined independently while still enabling
% them to be combined automatically and profitably. Our approach avoids
% the loss of precision associated with iterating individual analyses
% and the implementation difficulties of manually writing a
% super-analysis.}
% \end{quote}



%%  \simon{But do we apply rewrites even before the analysis reaches a fixed point?
%%  If so, what property do the rewrites have to satisfy to ensure soundness?
%%  If not, even a single rewrite might destroy the fixed-point property of the
%%  current facts.  Or perhaps we iterate the analysis to a fixpoint, and only \emph{then}
%%  do rewriting? If so, do we need the transfer functions at that stage?
%%  
%%  Also the fixed-point of the analysis relies on upward chains. What if
%%  the rewrite pushed it downward?  Or is it the case that a rewrite must
%%  change a node $n$ into a graph $g$ so 
%%  that $\mathit{fwdtrans}(n) \leq \mathit{fwdtrans}(g)$?
%%  
%%  Also the fixpoint calculation requires multiple passses; do the 
%%  rewrites then apply multiple times?
%%  
%%  I'm deliberately playing the role of the reader here, and not peeking at
%%  the code.  I don't think it's enough to say ``go look at Chambers paper''; 
%%  I suggest we say enough (half a column would do it) to address the obvious
%%  questions and point to Chambers for details.
%%  
%%  
%%  \textbf{NR}: Good questions, but let's have a forward reference to \secref{dfengine}}

% Interleaving
% \fi
%  analysis with transformation makes it
% possible to implement useful  transformations using startlingly simple
% client code.
% \john{And maybe this is the place to brag that we now have a startlingly simple
% implementation?}

\finalremark{Doesn't the rewrite have to be have the following property:
for a forward analysis/transform, if (rewrite P s) = Just s',
then (transfer P s $\sqsubseteq$ transfer P s').
For backward: if (rewrite Q s) = Just s', then (transfer Q s' $\sqsubseteq$ transfer Q s).
Works for liveness.
``It works for liveness, so it must be true'' (NR).
If this is true, it's worth a QuickCheck property!
}%
\finalremark{Version 2, after further rumination.  Let's define
$\scriptstyle \mathit{rt}(f,s) = \mathit{transform}(f, \mathit{rewrite}(f,s))$.
 Then $\mathit{rt}$ should
be monotonic in~$f$.  We think this is true of liveness, but we are not sure
whether it's just a generally good idea, or whether it's actually a 
precondition for some (as yet unarticulated) property of \ourlib{} to hold.}%

%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixpoint. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }




\section{\ourlib's dataflow engine}
\seclabel{engine}
\seclabel{dfengine}

\delendum{The earlier sections promised that we'd reveal the lies.
Do we?  I see no mention of @HavingSuccessors@ for example, which is rather important
for polymorphism.  Indeed, a subsection on that point might be a good way
to substantiate the claims of the last bullet of the conclusion.}

In Section \ref{sec:making-simple}
we gave a client's-eye view, showing how to use 
\ourlib\ to create analyses and transformations.
The interface is simple, but 
\emph{implementation} of interleaved analysis and rewriting is 
quite complicated.  \citet{lerner-grove-chambers:2002} 
do not describe their implementation at all.  We have made 
at least three attempts to implement the algorithm all of which
were long, hard to understand, and lacked any static open/closed
shape guarantees.  We are not confident that any of them are correct.

In this section we describe our new implementation.  It is short
(about a third of the size of our last attempt), elegant, and offers
strong static shape guarantees.  

\subsection{Overview}

We will concentrate on implementing @analyseAndRewriteFwd@, whose
tpye signature is in \secref{running-rewriter}.
The implementation is built on the hierarchy of nodes, blocks, and graphs
that we described in \secref{graph-rep}.  For each such thing
we will develop a function of this type:
\begin{code}
type `ARF thing n f 
  = forall e x. f -> thing e x 
             -> FuelMonad (OutFact x f, RG n e x)
\end{code}
Here we intend ``@thing@'' to range over nodes, blocks, and graphs, thus:
\begin{code}
  type `ARF_Node  n f = ARF n         n f
  type `ARF_Block n f = ARF (Block n) n f
  type `ARF_Graph n f = ARF (Graph n) n f
\end{code}
An @ARF@ (short for ``analyse and rewrite forward'') takes an input fact and a @thing@,
and returns a correspondingly-shaped output fact, and a rewritten graph of type @(RG n e x)@.
Note that the result is a rewritten graph, 
regardless of whether @thing@ is a node, block, or graph.

Why do we return @RG@ not @Graph@?  We cannot simply return 
a @Graph@ for two reasons:
\begin{itemize}
\item The client is often interested not only in the facts flowing
out of the graph, but also in the facts on the \emph{internal} blocks
of the graph.  A rewritten graph, of type @(RG n e x)@ is decorated with
these internal facts.
\item The @Graph@ has deliberately restrictive invariants; for example,
a @GMany@ with a @Tail@ is always open at exit (\figref{graph}).  It turns
out to be awkward to maintain these invariants \emph{during} rewriting,
but easy to restore them \emph{after} rewriting by ``normalising'' an @RG@.
\end{itemize}
The normalisation function expresses the above two points, by splitting 
a rewritten graph into a @BlockGraph@ and its corresponding @FactBase@:
\begin{code}
`normalise :: BlockId -> f
          -> RG n f C C -> (BlockGraph n, FactBase f)
\end{code}

We can now build a modular implementation as follows:
\begin{itemize} 
\item From the supplied @ForwardTransfer@ and @ForwardRewrite@, produce a @ARF_Node@
that transforms nodes (\secref{arf-node}):  
\begin{code}
  `arfNode :: DataflowLattice f
          -> ForwardTransfer n f
          -> ForwardRewrite n f
          -> ARF_Node n f
          -> ARF_Node n f
\end{code}
In addition to the transfer and rewrite functions, @arfNode@ takes the function
to analyse and rewrite the rewritten graph.
\item From this @ARF_Node@, produce a @ARF_Block@ that transforms blocks (\secref{arf-block}):
\begin{code}
  `arfBlock :: ARF_Node n f -> ARF_Block n f
\end{code}
 
\item From this @ARF_Block@, produce a @ARF_Graph@ that tranforms graphs (\secref{arf-graph}):
\begin{code}
  `arfGraph :: DataflowLattice f
           -> ARF_Node n f -> ARF_Graph n f
\end{code}
This is where the fixpoint computation takes place, so @arfGraph@ must
take a @DataflowLattice@ as an additional argument.
\end{itemize}
Given these three functions, it is rather easy to write the main analyser:
\par {\small
\begin{code}
`analyseAndRewriteFwd
   :: forall n f. Edges n
   => DataflowLattice f   -> ForwardTransfer n f
   -> ForwardRewrite n f -> RewritingDepth
   -> ARF_Graph n f
analyseAndRewriteFwd lat tf rw depth
 = arfGraph lat arf_node
 where 
  arf_node, rec_node :: ARF_Node n f
  arf_node = arfNode lat tf rw rec_node

  rec_node = case depth of
              RewriteShallow -> arfNodeNoRW tf
              RewriteDeep    -> arf_node
\end{code}
}
This beautifully compact code does the following:
\begin{itemize}
\item The result is built by applying @arfGraph@ to @arf_node@, the 
function to use when @arfGraph@ reaches an individual node.
\item In turn @arf_node@ is built by applying @arfNode@ to the client-supplied
transfer and rewrite functions and, crucially, @rec_node@.  
\item @rec_node@ is used by @arfNode@ when analysing and transforming graphs
produced by rewriting.
When deep rewriting is used, the @rec_node@ engine is simply @arf_node@:
straight recursion. However, if shallow rewriting is required, it instead
uses a simpler analyse-only function on the node, built by @arfNodeNoRW@.
We define the latter in \secref{arf-node}.
\end{itemize}
And that is all.  Note the way that the shallow/deep distinction is made
here, and here alone; none of the other functions are aware of the issue.
\john{Are we showing the code that does the interleaving? I feel like that's
a pretty essential part of the story.}

\subsection{Analysing and rewriting nodes}

The core of the interleaved analyse-and-rewrite idea is expressed
by the functions @arfNode@ and @arfNodeNoRW@:

\begin{code}
arfNodeNoRW :: ForwardTranfsers n f
            -> ARF_Node n f
arfNodeNoRW tf f n = return (f, nodeToGraph n)

arfNode transfer_fn rewrite_fn graph_trans f node
  = do { mb_g <- withFuel (rewrite_fn f node)
       ; case mb_g of
           Nothing -> arfNodeNoRW transfer_fn f node
      	   Just ag -> do { g <- graphOfAGraph ag
      		         ; graph_trans f g } }
\end{code}

\subsection{From nodes to blocks}

We start with the second task, of ''lifting'' a @ARF_Node@ to a @ARF_Block@:
\begin{code}
arfBlock :: forall n f. ARF_Node n f -> ARF_Block n f
arfBlock (ARF node_trans) = ARF block_trans
  where 
    block_trans :: ARFR (Block n) n f
    block_trans (BUnit n)   f 
      = node_trans n f
    block_trans (BCat l r) f 
      = do { (g1,f1) <- block_trans l f
           ; (g2,f2) <- block_trans r f1
	   ; return (g1 `pCat` g2, f2) }
\end{code}
The code is childishly simple.  In the @BUnit@ case 
we appeal to the node-level @ARF_Node@ tranform.
For @BCat@ we recursively apply @block_trans@ to the two
sub-blocks, threading the output fact from the first as the 
input to the second.  Each will produce a rewritten @Graph@, 
which we connect together in sequence with @pCat@. 

\subsection{Rewriting graphs}

Next we tackle .... \simon{not finished}




\section {Related work}

\simon{I've moved this section to near the end}

While dataflow analysis and optimization are covered
by a vast literature, 
\emph{design} of optimizers, the topic of this paper, is covered
relatively sparsely.
We therefore focus on foundations.

When transfer functions are monotone and lattices are finite in height,
iterative dataflow analysis converges to a fixed point
\cite{kam-ullman:global-iterative-analysis}. 
If~the lattice's join operation distributes over transfer
functions,
this fixed point is equivalent to a join-over-all-paths solution to
the recursive dataflow equations
\cite{kildall:unified-optimization}.\footnote
{Kildall uses meets, not joins.  
Lattice orientation is conventional, and conventions have changed.
We use Dana Scott's
orientation, in which higher elements carry more information.}
\citet{kam-ullman:monotone-flow-analysis} generalize to some
monotone functions.
Each~client of \hoopl\ must guarantee monotonicity,
but for transfer functions that
approximate weakest preconditions or strongest postconditions,
monotonicity falls out naturally.

\ifcutting
\citet{cousot:abstract-interpretation:1977}
\else
\citet{cousot:abstract-interpretation:1977,cousot:systematic-analysis-frameworks}
\fi
introduce abstract interpretation as a technique for developing
lattices for program analysis.
\citet{schmidt:data-flow-analysis-model-checking} shows that
an all-paths dataflow problem can be viewed as model checking an
abstract interpretation.

The soundness of interleaving analysis and transformation,
even when some speculative transformations are not performed on later
iterations, was shown by
\citet{lerner-grove-chambers:2002}.
\ifcutting\else
Muchnick \citeyearpar{muchnick:compiler-implementation} 
presents many examples of both particular analyses and related
algorithms.
\fi

In our previous work we described a zipper-based representation of control-flow
graphs \cite{ramsey-dias:applicative-flow-graph}, stressing the advantages
of immutability for writing dataflow optimisers.
Our new representation, described in \secref{graph-rep}, is a major improvement:
\begin{itemize}
\item
We can find the exit point of a graph in constant time.
\simon{What does this mean?}
\john{It means we don't need to do a linear search through a list of blocks;
instead, we can look at the g\_exit. I think this used to be vaguely relevant for
splicing graphs.}
\item
We can concatenate nodes, blocks, and graphs, in constant time.
Previously, we had to resort to Hughes's
\citeyearpar{hughes:lists-representation:article} technique, representing
a graph as a function.
\item
Most important, errors in concatenation are ruled out at
compile-compile time by Haskell's static
type system.
In~earlier implementations, such errors were not detected until
the compiler~ran, at which point \ourlib\ tried to compensate
for the errors
\ifcutting---but
\else
by inserting branch instructions and then issued a warning message.
But
\fi
the compensation code harbored subtle faults%
\ifcutting\else, which were discovered while developing a new back end
for~GHC\fi. 
\hfuzz=1.6pt % don't want to reword ``errors in concatenation''
\end{itemize}

\section{Conclusions}

\john{Lots of obsolete claims here: continuations, monad, separate node types,
HavingSuccessors.}
Compiler textbooks make dataflow optimization appear
difficult and complicated.
In~this paper, we show how to engineer a library, \ourlib, which makes
it easy to build analyses and transformations based on dataflow.
\ourlib\ makes dataflow simple not by using a single magic
ingredient, but by applying ideas that are well understood by 
the programming-language community.
\begin{itemize}
\item
We acknowledge only one program-analysis technique: the solution of
recursion equations over assertions.
%Like our colleagues working in imperative languages, 
We solve the equations by iterating to a fixed point.
% Many equations relate
% properties of program states; some relate properties of paths through
% programs. 
\item
We consider only two
ways of relating assertions: weakest liberal precondition and strongest 
postcondition, which
 correspond 
%\ifpagetuning\else
%respectively
%\fi
to
\ifcutting
``backward'' and ``forward'' dataflow
problems.
\else
``backward dataflow problems'' and ``forward dataflow
problems.''
\fi
\finalremark
{Can we give an example of a property of program states which is
neither, just by way of contrast; ie this we cannot do.}
%%  \item
%%  In a language that admits loops, iterating weakest preconditions or
%%  strongest postconditions typically does not reach a fixed point in
%%  finitely many steps; hence the need for loop invariants
%%  \cite{hoare:axiomatic-basis,dijkstra:discipline,gries:science-programming}.
%%  In \secref{logic-reconciled}, we show that we can guarantee to reach a
%%  fixed point by limiting what we can express in the logic.\remark{needs
%%  a fix}
%%  We~show that many classic analyses can be explained this way;
%%  the great diversity of classic analyses corresponds to a great
%%  diversity of inexpressive logics.
%%  This view leads us to a unifying principle:
%%  \emph{To implement a code-improving transformation, find the least
%%    expressive logic that can justify the transformation, then use that
%%    logic to compute strongest postconditions, which justify the
%%    transformation locally.}
\item
Although our implementation allows graph nodes to be rewritten in any
way that preserves semantics, we describe
three program-transformation techniques:
substitution of equals for equals, 
insertion of assignments to unobserved variables, 
and removal of assignments to unobserved variables
(\secref{example:transforms}).
Substitution of equals for equals is often justified by properties of program
states; for example, if variable~$x$
is always~7, we may substitute~7 for~$x$.\finalremark
{We can also justify substitution of \emph{labels} in goto
  statements by reasoning about continuations.  This is
  probably not the place to mention this fact.}
Insertion and removal of assignments are often justified by properties
of paths through programs;
for example, if an assignment's continuation does not use the variable
assigned~to, that assignment may be removed.

%%  Some compiler texts treat the removal of unreachable code as a
%%  code-improving transformation in its own right.
%%  In~our framework, unreachable code becomes unreachable in the
%%  garbage-collection sense, so no special effort is required to remove
%%  it.
\item
Complex program transformations should be composed from simple
transformations. 
For example, both ``code motion'' and ``induction-variable
elimination'' can be implemented in three stages: insert new assignments;
substitute equals for equals; remove unneeded assignments
(\secref{induction-var-elim}). 

\item 
Because each rewrite leaves the semantics
of the program unchanged, 
we can use 
``optimization fuel'' to limit the number of rewrites.
 When we isolate a fault 
\ifcutting\else in the optimizer \fi
(\secref{vpoiso}), we 
\ifcutting have to debug just \else therefore have the luxury of debugging \fi
 a single
 rewrite, not a complex transformation.
\end{itemize}

We also build on proven implementation techniques
in a way that
makes it easy
to implement classic code improvements.
\begin{itemize}
\item
We use the algorithm of \citet{lerner-grove-chambers:2002} to 
compose analyses and transformations.
This~algorithm makes it easy to compose complex transformations
from simple ones.

Using continuation-passing style and generalized algebraic data types,
we have created a new implementation%
\ifcutting\else\ of the algorithm\fi, 
which works by 
composing three relatively simple functions
(\secref{forward-iterator}). 
The functions are simple
because the static type of a node constrains the number of predecessors
and successors it may have.
And because we can
compare our code with a standard continuation semantics, we have more
confidence in this new implementation than in 
any previous implementation. 
\item
Our code is pure.
Inspired by Huet's~\citeyearpar{huet:zipper} zipper,
we use an applicative representation of
control-flow graphs
\cite{ramsey-dias:applicative-flow-graph}. 
We~improve on our prior work by storing changing dataflow facts
in an explicit dataflow monad,
which
makes it especially easy to implement such
operations as sub-analysis of a replacement graph
(\secref{dataflow-monad});
by using static types to guarantee that each replacement graph can be
spliced in place of the node it replaces
(\secreftwo{subgraphs}{rewrite-functions});
and by simplifying our implementation using continuation-passing style
(\secreftwo{forward-iterator}{forward-actualizer}). 
%
% important, but no longer mentioned in this paper:
%
%%  \item
%%  To \emph{construct} programs, we use a different representation of
%%  flow graphs, one which hides the complexity of the zipper and which
%%  provides a constant-time operation for joining flow graphs in
%%  sequence.
%%  It is inspired in part by Hughes's \citeyearpar{hughes:novel-lists}
%%  representation of lists, which supports a constant-time append operation.
\item
\ourlib\ is polymorphic in the
representations of 
assignments and control-flow operations.
%%  Although our polymorphic representations have been instantiated only
%%  with the low-level intermediate code used by the Glasgow Haskell
%%  Compiler, they are intended eventually to be instantiated with
%%  machine-dependent representations of target-machine instructions, as
%%  part of a larger project of refactoring GHC's back ends.
%
%This design seems obvious in retrospect,
%but we underestimated the degree to which polymorphism would force us to
%separate concerns.
%Introducing polymorphism has made the code simpler, easier
%to understand, and easier to maintain.
By forcing us to separate concerns, introducing polymorphism
made the code simpler, easier to understand, and easier to maintain.
\finalremark
{SLPJ: Is it possible to substantiate this claim by [more] examples?}
In particular, it forced us to make explicit \emph{exactly} what
\ourlib\ 
 must know about flow-graph nodes:
it must be able to find
targets of control-flow operations (constraint
@HavingSuccessors l@, \secref{zdfSolveFwd}).
\end{itemize}
%
% gen and kill are history
%
%%\item
%%Judicious use of Haskell type classes makes is possible to write
%%weakest precondition or strongest postcondition using the ``transfer
%%equations'' that are familiar from compiler textbooks.
%%If you like, you can even write overloaded @gen@ and @kill@ functions.
%%The benefit is that it is easy to compare the actual code with the
%%abstract treatments found in textbooks\ifgenkill \ (\secref{gen-kill})\fi.
Using \ourlib,
you can create a new code improvement in three steps:
create a lattice representation for the assertions you want to
express;
create transfer functions that approximate weakest preconditions or
strongest postconditions;
and 
create rewrite functions that use your assertions to justify
program transformations.  
You can get quickly to the real 
intellectual work of code improvement: identifying interesting
transformations and the assertions that justify them.

\finalremark{Don't forget acknowledgements!!!
Microsoft, Intel, NSF
}

\makeatother

\providecommand\includeftpref{\relax} %% total bafflement -- workaround
\IfFileExists{nrbib.tex}{\bibliography{cs,ramsey}}{\bibliography{cs,ramsey,simon,jd}}
\bibliographystyle{plainnatx}


\clearpage

\appendix


\section{Index of defined identifiers}

This appendix lists every nontrivial identifier used in the body of
the paper.  
For each identifier, we list the page on which that identifier is
defined or discussed---or when appropriate, the figure (with line
number where possible).
For those few identifiers not defined or discussed in text, we give
the type signature and the page on which the identifier is first
referred to.

Some identifiers used in the text are defined in the Haskell Prelude;
for those readers less familiar with Haskell, these identifiers are
listed in Appendix~\ref{sec:prelude}.

\newcommand\dropit[3][]{}

\newcommand\hsprelude[2]{\noindent
  \texttt{#1} defined in the Haskell Prelude\\}
\let\hsprelude\dropit

\newcommand\hspagedef[3][]{\noindent
  \texttt{#2} defined on page~\pageref{#3}.\\}
\newcommand\omithspagedef[3][]{\noindent
  \texttt{#2} not shown (but see page~\pageref{#3}).\\}
\newcommand\omithsfigdef[3][]{\noindent
  \texttt{#2} not shown (but see Figure~\ref{#3} on page~\pageref{#3}).\\}
\newcommand\hsfigdef[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} defined in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hstabdef[3][]{%
  \noindent
  \ifx!#1!
    \texttt{#2} defined in Table~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Table~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hspagedefll[3][]{\noindent
  \texttt{#2} {let}- or $\lambda$-bound on page~\pageref{#3}.\\}
\newcommand\hsfigdefll[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} {let}- or $\lambda$-bound in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} {let}- or $\lambda$-bound on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    

\newcommand\nothspagedef[3][]{\notdefd\ndpage{#1}{#2}{#3}}
\newcommand\nothsfigdef[3][]{\notdefd\ndfig{#1}{#2}{#3}}
\newcommand\nothslinedef[3][]{\notdefd\ndline{#1}{#2}{#3}}

\newcommand\ndpage[3]{\texttt{#2}~(p\pageref{#3})}
\newcommand\ndfig[3]{\texttt{#2}~(Fig~\ref{#3},~p\pageref{#3})}
\newcommand\ndline[3]{%
  \ifx!#1!%
      \ndfig{#1}{#2}{#3}%
  \else
      \texttt{#2}~(Fig~\ref{#3}, line~\lineref{#1}, p\pageref{#3})%
  \fi
}



\newif\ifundefinedsection\undefinedsectionfalse

\newcommand\notdefd[4]{%
  \ifundefinedsection
    , #1{#2}{#3}{#4}%
  \else
    \undefinedsectiontrue
    \par
    \section{Undefined identifiers}
    #1{#2}{#3}{#4}%
  \fi
}

\begingroup
\raggedright

% \input{defuse}%
\ifundefinedsection.\fi

\undefinedsectionfalse


\renewcommand\hsprelude[2]{\noindent
  \ifundefinedsection
    , \texttt{#1}%
  \else
    \undefinedsectiontrue
    \par
    \section{Identifiers defined in Haskell Prelude}\label{sec:prelude}
    \texttt{#1}%
  \fi
}
\let\hspagedef\dropit
\let\omithspagedef\dropit
\let\omithsfigdef\dropit
\let\hsfigdef\dropit
\let\hstabdef\dropit
\let\hspagedefll\dropit
\let\hsfigdefll\dropit
\let\nothspagedef\dropit
\let\nothsfigdef\dropit
\let\nothslinedef\dropit

% \input{defuse}
\ifundefinedsection.\fi



\endgroup


\iffalse

\section{Dataflow-engine functions}


\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward iterator}
\end{figure*}

\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward actualizer}
\end{figure*}


\fi



\end{document}



% Old captions' text:
% The dataflow fact for the available-reload analysis describes
%   the set of registers for which a reload is available.
%   We list the types of the functions that manipuate sets of available registers,
%   as well as the definition of the lattice.
% The standard gen and kill functions for available expressions
% The transfer functions for the available-reloads analysis.
% Running the available-reloads analysis and extracting the results with \texttt{zdfFpFacts}
% The rewrite functions to insert redundant reloads immediately before uses

% Probably no space for the implementations:
% interAvail (UniverseMinus s) (UniverseMinus s') =
%   UniverseMinus (s `plusVarSet`  s')
% interAvail (AvailVars     s) (AvailVars     s') =
%   AvailVars (s `timesVarSet` s')
% interAvail (AvailVars     s) (UniverseMinus s') =
%   AvailVars (s  `minusVarSet` s')
% interAvail (UniverseMinus s) (AvailVars     s') =
%   AvailVars (s' `minusVarSet` s )
% 
% smallerAvail (AvailVars _) (UniverseMinus _) = True
% smallerAvail (UniverseMinus _) (AvailVars _) = False
% smallerAvail (AvailVars     s) (AvailVars    s')  =
%   sizeVarSet s < sizeVarSet s'
% smallerAvail (UniverseMinus s) (UniverseMinus s') =
%   sizeVarSet s > sizeVarSet s'
% 
% extendAvail (UniverseMinus s) r =
%   UniverseMinus (deleteFromVarSet s r)
% extendAvail (AvailVars     s) r =
%   AvailVars (extendVarSet s r)
% 
% delFromAvail (UniverseMinus s) r =
%   UniverseMinus (extendVarSet s r)
% delFromAvail (AvailVars     s) r =
%   AvailVars (deleteFromVarSet s r)
% 
% elemAvail (UniverseMinus s) r =
%   not $ elemVarSet r s
% elemAvail (AvailVars     s) r =
%   elemVarSet r s



THE FUEL PROBLEM:


Here is the problem:

  A graph has an entry sequence, a body, and an exit sequence.
  Correctly computing facts on and flowing out of the body requires
  iteration; computation on the entry and exit sequences do not, since
  each is connected to the body by exactly one flow edge.

  The problem is to provide the correct fuel supply to the combined
  analysis/rewrite (iterator) functions, so that speculative rewriting
  is limited by the fuel supply.

  I will number iterations from 1 and name the fuel supplies as
  follows:

     f_pre      fuel remaining before analysis/rewriting starts
     f_0        fuel remaining after analysis/rewriting of the entry sequence
     f_i, i>0   fuel remaining after iteration i of the body
     f_post     fuel remaining after analysis/rewriting of the exit sequence

  The issue here is that only the last iteration of the body 'counts'.
  To formalize, I will name fuel consumed:

     C_pre      fuel consumed by speculative rewrites in entry sequence
     C_i        fuel consumed by speculative rewrites in iteration i of body
     C_post     fuel consumed by speculative rewrites in exit sequence

  These quantities should be related as follows:

     f_0    = f_pre - C_pref
     f_i    = f_0 - C_i            where i > 0
     f_post = f_n - C_post         where iteration converges after n steps

When the fuel supply is passed explicitly as parameter and result, it
is fairly easy to see how to keep reusing f_0 at every iteration, then
extract f_n for use before the exit sequence.  It is not obvious to me
how to do it cleanly using the fuel monad.


Norman
