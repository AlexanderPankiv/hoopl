\iffalse % submission abstract

We present Hoopl, a Haskell library that makes it easy for compiler
writers to implement program transformations based on dataflow
analyses. The compiler writer must identify (a) logical assertions on
which the transformation will be based; (b) a representation of such
assertions, which should form a lattice of finite height; (c) transfer
functions that approximate weakest preconditions or strongest
postconditions over the assertions; and (d) rewrite functions whose
soundness is justified by the assertions. Hoopl uses the algorithm of
Lerner, Grove, and Chambers (2002), which can compose very simple
analyses and transformations in a way that achieves the same precision
as complex, handwritten "superanalyses." Hoopl will be the workhorse
of a new back end for the Glasgow Haskell Compiler (version 6.12,
forthcoming).

Because the major claim in the paper is that Hoopl makes it easy to
implement program transformations, the paper is filled with examples,
which are written in Haskell.  The paper also sketches the
implementation of Hoopl, including some excerpts from the
implementation. 

\fi


% ====> THIS IS A DERIVED FILE; DO NOT EDIT IT <======
% ===========> SIMON, THIS MEANS YOU! <===============
     % If you are Simon, run un-preprocessed code
     % It not Simon, use version with def-use information

\newif\ifpagetuning \pagetuningtrue  % adjust page breaks

\newif\ifnoauthornotes\noauthornotestrue
\newif\iftimestamp\timestamptrue  % show MD5 stamp of paper

\IfFileExists{timestamp.tex}{}{\timestampfalse}

\newif\ifcutting \cuttingtrue % cutting down to submission size


\newif\ifgenkill\genkillfalse  % have a section on gen and kill
\genkillfalse


\newif\ifnotesinmargin \notesinmargintrue 
% \IfFileExists{notesinline.tex}{\notesinmarginfalse}{\relax}

\documentclass[blockstyle,preprint,natbib,nocopyrightspace]{sigplanconf}

\newcommand\ourlib{Hoopl}
   % higher-order optimization library
   % ('Hoople' was taken -- see hoople.org)
\let\hoopl\ourlib

\newcommand\fs{\ensuremath{\mathit{fs}}} % dataflow facts, possibly plural

\newcommand\vfilbreak[1][\baselineskip]{%
  \vskip 0pt plus #1 \penalty -200 \vskip 0pt plus -#1 }

\usepackage{alltt}
\usepackage{array}
\newcommand\lbr{\char`\{}
\newcommand\rbr{\char`\}}
 
\clubpenalty=10000
\widowpenalty=10000

\usepackage{verbatim} % allows to define \begin{smallcode}
\newenvironment{smallcode}{\par\unskip\small\verbatim}{\endverbatim}

\newcommand\lineref[1]{line~\ref{line:#1}}
\newcommand\linepairref[2]{lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\linerangeref[2]{\mbox{lines~\ref{line:#1}--\ref{line:#2}}}
\newcommand\Lineref[1]{Line~\ref{line:#1}}
\newcommand\Linepairref[2]{Lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\Linerangeref[2]{\mbox{Lines~\ref{line:#1}--\ref{line:#2}}}

\makeatletter

\let\c@table=\c@figure % one counter for tables and figures, please

\newcommand\setlabel[1]{%
  \setlabel@#1!!\@endsetlabel
}
\def\setlabel@#1!#2!#3\@endsetlabel{%
  \ifx*#1*% line begins with label or is empty
     \ifx*#2*% line is empty
        \verbatim@line{}%
     \else
       \@stripbangs#3\@endsetlabel%
       \label{line:#2}%
     \fi
  \else
     \@stripbangs#1!#2!#3\@endsetlabel%
  \fi
}
\def\@stripbangs#1!!\@endsetlabel{%
  \verbatim@line{#1}%
}


\verbatim@line{hello mama}

\newcommand{\numberedcodebackspace}{0.5\baselineskip}

\newcounter{codeline}
\newenvironment{numberedcode}
  {\endgraf
     \def\verbatim@processline{%
        \noindent
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
               %{\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\phantom{: \,}}}%
            \else
               \refstepcounter{codeline}%
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\llap{\arabic{codeline}}: \,}}%
            \fi
        \expandafter\setlabel\expandafter{\the\verbatim@line}%
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
          \vspace*{-\numberedcodebackspace}\par%
        \else
          \the\verbatim@line\par
        \fi}%
   \verbatim
   }
   {\endverbatim}

\makeatother

\newcommand\arrow{\rightarrow}

\newcommand\join{\sqcup}
\newcommand\slotof[1]{\ensuremath{s_{#1}}}
\newcommand\tempof[1]{\ensuremath{t_{#1}}}
\let\tempOf=\tempof
\let\slotOf=\slotof

\makeatletter
\newcommand{\nrmono}[1]{%
  {\@tempdima = \fontdimen2\font\relax
   \texttt{\spaceskip = 1.1\@tempdima #1}}}
\makeatother

\usepackage{times}  % denser fonts
\renewcommand{\ttdefault}{aett} % \texttt that goes better with times fonts
\usepackage{enumerate}
\usepackage{url}
\usepackage{graphicx}
\usepackage{natbib}  % redundant for Simon
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar

\usepackage[ps2pdf,bookmarksopen,breaklinks,pdftitle=dataflow-made-simple]{hyperref}

\newcommand\naive{na\"\i ve}
\newcommand\Naive{Na\"\i ve}

\usepackage{amsfonts}
\newcommand\naturals{\ensuremath{\mathbb{N}}}
\newcommand\true{\ensuremath{\mathbf{true}}}
\newcommand\implies{\supseteq}  % could use \Rightarrow?

\newcommand\PAL{\mbox{C{\texttt{-{}-}}}}
\newcommand\high[1]{\mbox{\fboxsep=1pt \smash{\fbox{\vrule height 6pt
   depth 0pt width 0pt \leavevmode \kern 1pt #1}}}}

\usepackage{tabularx}

%%
%% 2009/05/10: removed 'float' package because it breaks multiple
%% \caption's per {figure} environment.   ---NR
%%
%%  % Put figures in boxes --- WHY??? --NR
%%  \usepackage{float}
%%  \floatstyle{boxed}
%%  \restylefloat{figure}
%%  \restylefloat{table}



% ON LINE THREE, set \noauthornotestrue to suppress notes (or not)

%\newcommand{\qed}{QED}
\ifnotesinmargin
  \long\def\authornote#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}
\else
  % Simon: please set \notesinmarginfalse on the first line
  \long\def\authornote#1{{\em #1\/}}
\fi
\ifnoauthornotes
  \def\authornote#1{\unskip\relax}
\fi

\newcommand{\simon}[1]{\authornote{SLPJ: #1}}
\newcommand{\norman}[1]{\authornote{NR: #1}}
\let\remark\norman
\def\finalremark#1{\relax}
% \let \finalremark \remark % uncomment after submission
\newcommand{\john}[1]{\authornote{JD: #1}}
\newcommand{\todo}[1]{\textbf{To~do:} \emph{#1}}
\newcommand\delendum[1]{\relax\ifvmode\else\unskip\fi\relax}

\newcommand\secref[1]{Section~\ref{sec:#1}}
\newcommand\secreftwo[2]{Sections \ref{sec:#1}~and~\ref{sec:#2}}
\newcommand\seclabel[1]{\label{sec:#1}}

\newcommand\figref[1]{Figure~\ref{fig:#1}}
\newcommand\figreftwo[2]{Figures \ref{fig:#1}~and~\ref{fig:#2}}
\newcommand\figlabel[1]{\label{fig:#1}}

\newcommand\tabref[1]{Table~\ref{tab:#1}}
\newcommand\tablabel[1]{\label{tab:#1}}


\newcommand{\CPS}{\textbf{StkMan}}    % Not sure what to call it.


\usepackage{code}   % At-sign notation

\iftimestamp
\input{timestamp}
\preprintfooter{\mdfivestamp}
\fi

\hyphenation{there-by}

\begin{document}
\title{\ourlib: Dataflow Optimization Made Simple}
%\subtitle{\today}

\ifnoauthornotes
\makeatletter
\let\HyPsd@Warning=\@gobble
\label{haskell.firstuse.Warning}% automated use
\label{haskell.firstuse.=:bs}% automated use
\makeatother
\fi

% João


\authorinfo{Norman Ramsey}{Tufts University}{nr@cs.tufts.edu}
\authorinfo{Jo\~ao Dias}{Tufts University}{dias@cs.tufts.edu}
\authorinfo{Simon Peyton Jones}{Microsoft Research}{simonpj@microsoft.com}


\maketitle
 
\begin{abstract}
We present \ourlib, a Haskell library that makes it easy for compiler writers
to implement program transformations based on dataflow analyses.
The compiler writer must identify (a)~logical assertions
on which the transformation will be based;
(b)~a~representation of such assertions, which
\ifcutting
should form a lattice of finite height;
\else
must have a lattice structure
 such that every assertion can be increased at
most finitely many times;
\fi
(c)~transfer functions that approximate weakest preconditions or
strongest postconditions over the assertions; and
(d)~rewrite functions whose soundness is justified by the assertions.
%%  To~guide compiler writers,
%%  we show how dataflow analyses are related to
%%  seminal work on program 
%%  correctness. \simon{The ``next 700'' section sort of does so, but I'm not 
%%  sure it deserves mention in the abstract.}
\ourlib\ uses the algorithm of 
\citet{lerner-grove-chambers:2002}, which 
% enables compiler writers to
can
compose very simple analyses and transformations in a way that achieves
the same precision as complex, handwritten
``super-analyses.''
\ourlib\ will be the workhorse of a new
back end for the Glasgow Haskell Compiler (version~6.12, forthcoming).

\emph{Reviewers:} code examples are indexed at {\small\url{http://bit.ly/jkr3K}}
%%% Source: http://www.cs.tufts.edu/~nr/drop/popl-index.pdf
\end{abstract}

\makeatactive  

\section{Introduction}

If you write a compiler for an imperative language, you can exploit
many years' work on code improvement
(``optimization'').
The work is typically presented
as a long list of analyses and
transformations, each with a different name.
This presentation makes optimization appear complex and difficult.
Another source of complexity is the need for synergistic combinations
of optimizations; you may have to write one ``super-analysis'' per
combination. 

But optimization doesn't have to be complicated.
Most optimizations
work by applying well-understood techniques for
reasoning about programs:
assertions about states, assertions about continuations, and
substitution of equals for equals. 
What makes optimization different from classic reasoning techniques
is that in dataflow optimization, assertions are approximated,
and all assertions are computed automatically.

This paper presents \ourlib\ (higher-order optimization library), 
a~Haskell library that makes it easy to implement dataflow optimizations.
Our contributions are as follows:
\begin{itemize}
\item
\ourlib\ defines a simple interface for implementing analyses and transformations:
you provide representations for assertions and for functions that
transform assertions, 
and \ourlib\ computes assertions
 by setting up and solving recursion
equations.
Additional functions you provide use computed
assertions to justify program transformations.
Analyses and transformations built on \ourlib\ 
are small, simple, and easy to get right.
\item
% using LGC, simple implementations of complicated super-analyses using interleaving and speculative
% rewriting
Using the sophisticated algorithm of \citet{lerner-grove-chambers:2002},
%which is \emph{not} easy to get right,
\ourlib\ can perform super-analyses by \emph{interleaving}
simple analyses and transformations.
Interleaving is tricky to implement,
but by using 
generalized algebraic data types 
and continuation-passing style,
our new implementation expresses the algorithm with a clarity and a
degree of 
static checking that has not 
previously been achieved.
\item
\ourlib\ helps you write correct optimizations:
it
statically rules out transformations that violate invariants
of the control-flow graph,
and dynamically it can help find the first transformation that introduces a fault
in a test program \cite{whalley:isolation}.
\item
\ourlib's polymorphic, higher-order design makes it reusable
with many languages.
\hoopl\ is designed to help optimize imperative code
with arbitrary control flow,
including low-level intermediate languages and machine languages.
%%\ifcutting\else
As \citet{benitez-davidson:portable-optimizer} have shown, all the
classic scalar and loop optimizations can be performed over such
codes.
%% \fi
\end{itemize}






% %Many presentations obscure fundamental principles of
% %code improvement.
% %
% But~most optimizations
% work by applying reasoning techniques that have long been understood
% and used 
% to reason about programs:
% assertions about states, assertions about continuations, and
% substitution of equals for equals.  % (\secref{next-700}).
% % What distinguishes dataflow optimization from classic formal reasoning
% % about programs is that in dataflow optimization, all assertions are
% % computed automatically, and they are
% % \emph{approximated}. 
% 
% This paper presents \ourlib\ (higher-order optimization library), 
% a Haskell library the implements dataflow optimizations using
% simple representations of assertions and the functions that transform
% them.
% Its contributions are as follows:
% %The contribution of this paper is to elucidate a large body of work on code
% %improvement; the body of work known as ``dataflow optimization.''
% %This paper makes two contributions:
% \begin{itemize}
% \item
% \ourlib\ is engineered to make it \emph{easy} to implement
% dataflow-based 
% code-improvement techniques, even in a purely functional setting.
% When built on \ourlib, analyses and transformations
% are small, simple, and easy to get right.
% \item 
% Using the sophisticated algorithm of \citet{lerner-grove-chambers:2002},
%  which is \emph{not} easy to get right,
% \ourlib\ \emph{interleaves} analysis and transformation.
% Our new implementation, which uses generalized
% algebraic data 
% types % \cite{xi:guarded-recursive} 
% and continuation-passing style,
% %By~exploiting Strachey's ideas about compositional semantics and
% %the extra type checking possible with GADTs, we implement
% expresses the algorithm with a clarity and a degree of
% static checking that has not 
% previously been achieved.
% \item
% \ourlib's polymorphic, higher-order design makes it easier to reuse
% these techniques than ever before.
% \end{itemize}
% %
% \hoopl\ is designed to help optimize imperative procedures (after
% inlining). 
% \ourlib\ supports local-level codes with arbitrary control flow,
% including intermediate 
% languages and machine 
% languages.
% As \citet{benitez-davidson:portable-optimizer} have shown, all the
% classic scalar and loop optimizations can be performed over such
% codes.



We introduce dataflow optimization by analyzing and transforming
example code (\secref{example:xforms}),
thinking about and justifying classic optimizations using
Hoare logic and substitution of equals for equals.
To~support our claim that \ourlib\ makes dataflow optimization easy, 
we explain how
to create new dataflow analyses and transformations
(\secref{making-simple}), and we show complete implementations of significant
analyses (\secref{example-analyses}) and transformations
(\secref{example-rewrites}) from the Glasgow Haskell Compiler.
We also sketch a new implementation of interleaving (\secref{engine}).


\section{Dataflow analysis {\&} transformation by \rlap{example}}

\seclabel{example:transforms}
\seclabel{example:xforms}

In dataflow optimization, code-improving transformations are justified
by assertions about programs;
such assertions are often computed using
strongest postconditions or weakest liberal preconditions.
Typical transformations are
to insert assignments to unobserved variables,
to substitute equals for equals, 
and
to remove assignments to unobserved variables.
Insertion and removal can be composed to achieve 
``code motion.''
\ourlib\  expresses classic code
improvements by \emph{composing} simple \ifcutting\else analyses and \fi
transformations. 




\subsection{Simple transformations}

\seclabel{constant-propagation}

Here is a sequence of assignments separated by assertions.
We~compute assertions by starting with the weakest 
\ifcutting\else
possible
\fi
assertion (\texttt{true}) and computing strongest postconditions.
% \footnote
{Variables do not alias.}
\begin{verbatim}
    { true }
  x = 7;
    { x == 7 }
  y = 8: 
    { x == 7 && y == 8 }
  z = x + y;
\end{verbatim}
\delendum{SLPJ asks: Could we add $x=7,y=8,z=15$ as a final assertion?
We can but we should not, because a \naive\ sp function 
would produce the assertion $x=7\land y = 8
\land z = x+y$. To reach the point you desire, some sort of simplifier
would be required, and it is better to let the conclusion emerge
naturally as  the code is rewritten.}
In the assignment to~@z@, the assertion @x == 7@ justifies
\label{haskell.firstuse.z}% automated use
\label{haskell.firstuse.x}% automated use
\label{haskell.firstuse.==}% automated use
substituting~7 for~@x@, leaving @z = 7 + y@.  
\label{haskell.firstuse.y}% automated use
\label{haskell.firstuse.+}% automated use
This transformation is traditionally called ``constant propagation.''
We may also substitute 8~for~@y@.
Finally, because @7 + 8 == 15@, we may again substitute equals for
equals, leaving the final assignment as
\begin{verbatim}
  z = 15;
\end{verbatim}
The final transformation, although it also substitutes equals
for equals, has a different name: ``constant folding.''

\subsection{A complex transformation}

\finalremark{It's a pity that this transformation occupies nearly the
entire second page,  
and then plays no subsequent role in the paper whatsoever.
One possibility: move it to ``the next 700'' section, as a substantiating example
to the claims made there.
But then we'd need another example here... well the sink/reload example of
Section 4 might be perfect.}

\seclabel{induction-var-elim}


The loop optimization known as ``induction-variable elimination'' 
can be composed from simpler transformations.
We begin by showing a loop that sums red pixels from an array:
\begin{verbatim}
  struct pixel { double r, g, b; };
  double sum_r(struct pixel a[], int n) {
    double x = 0.0;
    int i;
    for (i = 0; i < n; i++)
      x += a[i].r;
    return x;
  }
\end{verbatim}
To explain induction-variable elimination, we show the same
code at the machine level, using our low-level compiler-target
language,~{\PAL}
\cite{peyton-jones-ramsey:single-intermediate}: 
\begin{verbatim}
  sum_r("address" bits32 a, bits32 n) {
       bits64 x; bits32 i;
       x = 0.0;
       i = 0;
   L1: if (i >= n) goto L2;
       x = %fadd(x, bits64[a+i*24]);
       i = i + 1;
       goto L1;
   L2: return x;
  }
\end{verbatim}
Induction-variable elimination
replaces~@i@ with a new variable~@p@, helping us
\label{haskell.firstuse.i}% automated use
\label{haskell.firstuse.p}% automated use
to remove the computation @a+i*24@ from the loop.
\label{haskell.firstuse.a}% automated use
\label{haskell.firstuse.*}% automated use
Variable~@p@ is intended to satisfy the invariant
\begin{verbatim}
   { p == a + i * 24 }
\end{verbatim}
Variable @i@ is also used in the loop-termination test.
To rewrite that test, 
we introduce another new variable \texttt{lim} satisfying
the invariant
\icode|lim == a + n * 24|,
so that @i >= n@ if and only if \icode|p >= lim|.
\label{haskell.firstuse.n}% automated use
\label{haskell.firstuse.>=}% automated use

\ifpagetuning \enlargethispage{0.9\baselineskip} \fi 
  


We implement the code improvement as a sequence of transformations.
After each transformation, the observable behavior of the program is unchanged.
%
Our first transformation declares @p@ and \texttt{lim} and inserts suitable
assignments. 
New code is \high{boxed}\,.
\begin{alltt}
  sum_r("address" bits32 a, bits32 n) \lbr
       bits64 x; bits32 i; \high{bits32 p, lim;}
       x = 0.0;
       i = 0; \high{p = a; lim = a + n * 24;}
   L1: if (i >= n) goto L2;
       x = %fadd(x, bits64[a+i*24]);
       i = i + 1; \high{p = p + 24;}
       goto L1;
   L2: return x;
  \rbr
\end{alltt}

As written, the assignments to @p@~and~\texttt{lim} have no
effect on the program, but they establish the assertions
@p == a + i * 24@ and \icode|(i >= n) == (p >= lim)|.
On~the basis of these assertions, the compiler substitutes equals for
equals, resulting in the new code in boxes below:
\begin{alltt}
  sum_r("address" bits32 a, bits32 n) \lbr
       bits64 x; bits32 i; bits32 p, lim;
       x = 0.0;
       i = 0; p = a; lim = a + n * 24;
   L1: if (\high{p >= lim}) goto L2;
       x = %fadd(x, bits64[\kern1pt{}\high{p}\kern1pt{}]);
       i = i + 1; p = p + 24;
       goto L1;
   L2: return x;
  \rbr
\end{alltt}

Here the compiler switches from reasoning about states to
reasoning about continuations.
In~particular, we reason about whether the value of a variable can be
used by a continuation; this reasoning is called ``liveness analysis.''
\Naive\ analysis would show that although @i@~is not live at
label~\texttt{L2}, it is nevertheless live immediately after 
the assignment
@i = i + 1@ in the loop body,
because the value of~@i@ could be used by the next iteration of the
loop.
But~we use Lerner, Grove, and Chambers's
\citeyearpar{lerner-grove-chambers:2002} algorithm to
\emph{interleave} liveness analysis with 
``dead-assignment elimination.'' \seclabel{interleave-introduced}%
Dead-assignment elimination removes an assignment if the variable
assigned to is not live, that is, if it cannot be used by the
assignment's continuation.
\ifcutting
No
\else
As~explained by Lerner, Grove, and Chambers, no
\fi
sequential
composition of liveness analysis and dead-assignment elimination can
get rid of these assignments to~@i@, but interleaving analysis with
transformation does the trick.\footnote
{You might be tempted to modify the
liveness analysis so that
@i = i + 1@ is not considered a ``use'' of~@i@ if @i@~is itself
dead.
This~modification is tantamount to writing a single ``super-analysis''
that \emph{combines} liveness analysis and dead-code
elimination.
In~this case, writing a super-analysis is easy,
but the approach does not scale:
most super-analyses are more complicated than the examples shown here;
the cost of writing a super-analysis does not scale linearly with
the number of analyses combined;
super-analyses often cannot be composed;
and 
some super-analyses require nonstandard, handwritten traversals of
the control-flow graph.
\citet{lerner-grove-chambers:2002} discuss these issues in detail;
\citet{click-cooper} show both the advantages of and the programming cost
of combining analyses.} 
Interleaving%
\ifcutting
\  (\secref{dfengine}) 
\else, as sketched in 
\secref{dfengine}, \fi
eliminates the boxed assignments to~@i@:
\begin{alltt}
sum_r("address" bits32 a, bits32 n) \lbr
     bits64 x; \high{bits32 i;} bits32 p, lim;
     x = 0.0;
     \high{i = 0;} p = a; lim = a + n * 24;
 L1: if ({p >= lim}) goto L2;
     x = %fadd(x, bits64[{p}]);
     \high{i = i + 1;} p = p + 24;
     goto L1;
 L2: return x;
\rbr
\end{alltt}

After the insertion of assignments to @p@~and~\texttt{lim}, the substitution
of equals for equals, and the removal of newly dead assignments
to~@i@, we have ``eliminated the induction variable:''
\begin{alltt}
sum_r("address" bits32 a, bits32 n) \lbr
     bits64 x; bits32 p, lim;
     x = 0.0;
     p = a; lim = a + n * 24;
 L1: if ({p >= lim}) goto L2;
     x = %fadd(x, bits64[{p}]);
     p = p + 24;
     goto L1;
 L2: return x;
\rbr
\end{alltt}

  
%\ifpagetuning \enlargethispage{0.9\baselineskip} \fi 


\section {Making dataflow simple}

\seclabel{making-simple}

\seclabel{create-analysis}

The goal of dataflow optimization is to compute valid
assertions, then use those assertions to justify code-improving
transformations.
%
% only Don Knuth knows why, but the paragraph break gets us three
% bullets on this page where a single paragraph gets only two!
%
Assertions are represented as
\emph{dataflow facts}.
Dataflow facts relate to
 traditional 
program logic:
\begin{itemize}
\item
A dataflow fact is usually equivalent to an assertion about program state or
about a continuation.
For example, in \secref{constant-propagation}, @x == 7@ is a dataflow
fact that describes the program state. 


%%  There are two kinds of dataflow facts.
%%  The first kind is an assertion about the paths from the procedure
%%  entry to a program point;
%%  these facts are computed by a forward dataflow analysis.
%%  A common special case is an assertion about state at a program point,
%%  such as the assertion @x == 7@ in \secref{constant-propagation}.
%%  % Assertions about program state are usually sufficient to show that
%%  % a transformation preserves semantics,
%%  % but to decide whether a transformation will improve the code,
%%  % we sometimes need an assertion that describes how the program
%%  % state was established.
%%  
%%  The second kind of dataflow fact is an assertion about paths from the program point
%%  to the procedure exit;
%%  these facts are computed by a backward dataflow analysis.
%%  In the parlance of functional programmers, these dataflow facts are assertions on
%%  \emph{continuations}.
%%  
%%  Assertions about state are easy to formalize, but path properties are harder;
%%  we describe path properties informally in our assertions.
\item
A~set of dataflow facts forms a lattice.
To ensure that analysis terminates,
it is enough if
no fact has more than finitely many distinct facts above it.
\item
Each analysis or transformation may use a different lattice of
dataflow facts.
\end{itemize}

An assertion about a continuation is an assertion about paths
\emph{from} a program point 
to the procedure {exit};
such assertions are established by a \emph{backward dataflow analysis}.
An~assertion about paths \emph{to} a program point from the procedure
{entry} is established by a \emph{forward dataflow analysis}.
As~an important special case,
an assertion, such as @x == 7@ above,
 may say simply
that all paths to a point establish a predicate
which describes the program state at that point.


A~program point is represented as an edge in
a \emph{control-flow graph}.
%%\footnote
%%{We discuss only intraprocedural optimization, done after inlining.}
%%
%%interprocedural optimizations are the work of the GHC~inliner
%%\cite{peyton-jones:secrets-inliner}.} 
%% 
%%  Note: trying to eliminate reviewer's impression that our work is
%%  conceived for and limited to GHC.  S&S's citation count must
%%  suffer.  ---NR
%%
Edges connect nodes, each of which represents a label, an assignment, or
a control transfer.

To write a dataflow \emph{analysis}, you must 
\begin{itemize}
\item
Choose a representation~$F$ of dataflow facts and a logical interpretation
thereof.
\item
Implement lattice operations over~$F$ (\secref{lattices}).
\item
Write \emph{transfer functions} that relate dataflow facts before and
after each type of node (\secref{tffuns}).
\delendum{I'd italicize key words from all three bullets, or none. NR:
It's not a question of bullets; the key concepts which are possibly
new to readers are transfer
functions and rewrite functions, which is why they are italicized.}
\end{itemize}

To write a \emph{transformation}
based on an analysis, you
must also
create a \emph{rewrite function}, which is presented with a
flow-graph node and with the dataflow facts on the edges coming
into that node (\secref{rewrite-functions}).
The function either proposes to replace the node with a
fresh subgraph, or it leaves the node alone.
If the function proposes a replacement,
the replacement must preserve
semantics;
preservation may be justified by incoming 
\ifcutting \else dataflow \fi
facts.
For example, in \secref{constant-propagation} the fact @x == 7@ 
justifies replacing @z = x + y@ with @z = 7 + y@.

\begin{table}
\centerline{%
\begin{tabular}{@{}>{\raggedright\arraybackslash}p{1.03in}>{\scshape}c>{\scshape}
      c>{\raggedright\arraybackslash}p{1.2in}@{}}
&\multicolumn1{r}{\llap{\emph{Specified}}\hspace*{-0.3em}}&
\multicolumn1{l}{\hspace*{-0.4em}\rlap{\emph{Implemented}}}&\\
\multicolumn1{c}{\emph{Part of optimizer}}
&\multicolumn1{c}{\emph{by}}&
\multicolumn1{c}{\emph{by}}&
\multicolumn1{c}{\emph{How many}}%
\\[5pt]
Control-flow graphs& Us & Us & One \\
Nodes in a control-flow graph & You & You & Two datatypes per intermediate language \\[3pt]
Dataflow fact~$F$    & You & You & One datatype per logic \\
Lattice operations & Us & You & One set per logic \\[3pt]
Transfer functions & Us & You & One set per analysis \\
Rewrite functions & Us & You & One set per \rlap{transformation} \\[3pt]
Iterative solver functions & Us & Us & Two (forward \&\ backward) \\
Solve-and-rewrite functions & Us & Us & Two (forward \&\ backward) \\
\end{tabular}%
}
\caption{Parts of an optimizer built with \ourlib}
\tablabel{parts}
\end{table}



\tabref{parts} shows how \ourlib\ interacts with your client code.
\ourlib\ defines the types of
control-flow graphs, 
lattice operations,
transfer functions, 
and 
rewrite functions.
{All}~these types are parameterized by the types of
nodes in the control-flow graph, which \emph{you} get to define, so you can use
\ourlib\ with many intermediate languages (\tabref{parts}).
Function types are also parameterized by the type of dataflow
facts, so you can define different analyses, 
using different types of facts,
all operating over one type of graph. 

\ifpagetuning\enlargethispage{0.5\baselineskip}\fi

To run an~optimization, you~pass lattice
operations, transfer functions, and rewrite functions
to one of \ourlib's
\emph{solver functions} or \emph{rewrite functions}---\ourlib's 
\emph{dataflow engine}.
A~solver function 
uses a forward or backward \emph{analysis} to compute 
a dataflow fact for each program point (\secref{zdfSolveFwd}).
A~rewrite function
uses a forward or backward \emph{transformation} to compute facts and to
rewrite a control-flow graph in light of those facts
(\secref{rewrites}).
\ifcutting\else
The engine's implementation is sketched in \secref{dfengine}.
\fi
\delendum{Interface?  Perhaps ``The signatures and expected usage of these
functions is described in xxx, while their implementation is sketched
in yyy''.  NR: I like the idea of parallel structure, but the paper is
not really very parallel here.  Faced with either repeating the
section references or distorting what those sections are about, I've
chosen instead to abandon parallel structure.}



\begin{figure}
\ifcutting
\begin{code}
data ChangeFlag = NoChange | SomeChange
data DataflowLattice a = DataflowLattice
 {fact_bot        :: a,
  fact_add_to     :: a -> a -> (a, ChangeFlag) }
\end{code}
\else
\begin{code}
--data ChangeFlag = NoChange | SomeChange
--data DataflowLattice a = DataflowLattice
-- {fact_bot        :: a,
--  fact_add_to     :: a -> a -> (a, ChangeFlag),
--  fact_name       :: String } -- for debugging
\end{code}
\fi
\caption{Representation of a dataflow lattice} \figlabel{lattice-type} \figlabel{lattice}
\label{haskell.def.fact:unadd:unto}% automated definition
\label{haskell.def.fact:unbot}% automated definition
\label{haskell.def.DataflowLattice}% automated definition
\label{haskell.def.SomeChange}% automated definition
\label{haskell.def.NoChange}% automated definition
\label{haskell.def.ChangeFlag}% automated definition
\label{haskell.firstuse.fact:unname}% automated use
\label{haskell.firstuse.String}% automated use
\label{haskell.firstuse.ChangeFlag}% automated use
\label{haskell.firstuse.NoChange}% automated use
\label{haskell.firstuse.SomeChange}% automated use
\label{haskell.firstuse.DataflowLattice}% automated use
\label{haskell.firstuse.fact:unbot}% automated use
\label{haskell.firstuse.fact:unadd:unto}% automated use
\end{figure}


\subsection{Dataflow lattices}

\seclabel{lattices}

As an example, 
we present a lattice of facts about constant propagation.
At any program point, a standard constant-propagation analysis
computes exactly one of three
facts about a variable~$x$:
\begin{itemize}
\item
The analysis shows that
$x = k$, where $k$~is a compile-time constant of type \texttt{Const}.
\item
The analysis shows that $x$~is \emph{not} a compile-time constant.
We~notate this fact as $x = \top$.
\item
The analysis shows nothing about~$x$, which we notate $x=\bot$.
\end{itemize}
The bottom element of the lattice is~$x=\bot$, and
the join operation~$\join$ approximates disjunction,
the logical operation that combines facts flowing to a single label.
A~disjunction of two inconsistent facts is represented by~$x=\top$%
\ifcutting,
so for example 
$x = 7 \lor x = 8$ is approximated by $x = \top$, 
losing information.%
\else
Here are some examples:
\begin{itemize}
\item
$i = 7 \lor i=\bot \equiv i=7$ (no loss of information)
\item
$i = 7 \lor i= 7 \equiv  i=7$ (no loss of information)
\item
$i = 7 \lor i = 8 \equiv i = \top$ (loss of information)%
\fi
\footnote
{Your client code determines how much information is lost.
For example, in a similar analysis for a functional language,
you might track whether a value\ifcutting\else~$v$\fi\ is 
the result of applying a constructor from any finite set~$\{C_i\}$.
\ifcutting
\else
In this analysis, you needn't limit the representation to a
single constructor or to~$\top$;
you could choose to represent such facts as ``$v$~is
the application of a constructor drawn from the set $\{C_1, C_2,
C_4\}$.''
Because every algebraic data type has finitely many constructors,
there are finitely many sets and therefore finitely many facts in the
lattice, so a dataflow analysis over this lattice would always reach a
fixed point.
\fi
}
%%  The compiler writer gets to choose how much information is lost;
%%  in a different analysis, it might be useful to 
%%  choose a lattice which can say
%%  that the value of a variable is one of,
%%  say, at most three constants.
\ifcutting\else
\end{itemize}
\fi

The lattice used by the analysis is the Cartesian product of the
lattices for all the local variables.
We~represent this lattice as a finite map from a variable
to a value of type \icode|Maybe Const|.
A~variable $x$ is not in the domain of the map iff $x=\bot$;
$x$~maps to @Nothing@ iff $x=\top$; $x$~maps to $@Just@\;k$ iff
\label{haskell.firstuse.Nothing}% automated use
\label{haskell.firstuse.Just}% automated use
$x=k$.

\ifcutting\else
Any one procedure has only finitely many variables;
only finitely many facts are computed at any program point;
and in this lattice any one fact can increase at most twice.
These properties
ensure that the dataflow engine will
reach a fixed point.
\fi


\ourlib's dataflow engine uses
\ifcutting joins
\else the lattice's join operation
\fi in a stylized way.
Joins occur at labels.
If~$f_{\mathit{id}}$ is the fact currently associated with the
label~$\mathit{id}$, 
and if a transfer function propagates a new fact~$f_{\mathit{new}}$
into the label~$\mathit{id}$, 
the dataflow engine replaces $f_{\mathit{id}}$ with
the join  $f_{\mathit{new}} \join f_{\mathit{id}}$.
Furthermore, the dataflow engine wants to know if
  $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$,
because if not, the analysis has not reached a fixed point.



When computing a join, 
it is often cheap to learn if the join
is equal to one of the arguments.
We therefore use a nonstandard representation of lattice operations,
\iffalse
(\figref{lattice}).
\else
as shown in \figref{lattice}.
\fi
The~join operation~$\join$ and equality test~$=$ are represented by a
single function called @fact_add_to@.
The term $@fact_add_to@\;f_{\mathit{new}}\;f_{\mathit{id}}$ is equal to
$(f_{\mathit{id}}, @NoChange@)$ 
if $f_{\mathit{new}} \join f_{\mathit{id}} = f_{\mathit{id}}$
and is equal to
\mbox{$(f_{\mathit{new}} \join f_{\mathit{id}}, @SomeChange@)$} otherwise.
The @fact_bot@ value is the bottom element\ifcutting.
\label{haskell.def.fact:unname}% automated definition
\else, 
and @fact_name@  is used for debugging.
\fi

\ifpagetuning\enlargethispage{0.5\baselineskip}\fi



\subsection{Transfer functions} \seclabel{tffuns}

A~transfer function is presented with dataflow facts on edges coming
into a node, and it computes dataflow facts on outgoing edges.
To~understand transfer functions, we must 
understand how \ourlib\ organizes the nodes and edges of a control-flow graph.

\seclabel{graph.intro}

A~control-flow graph is a collection of \emph{basic blocks}, each
labelled with a~@BlockId@.
\label{haskell.def.BlockId}% automated definition
\label{haskell.firstuse.BlockId}% automated use
A~basic block is a sequence beginning with a \emph{first node},
containing zero or more \emph{middle nodes},
and ending in a \emph{last node}.
(An~optimizer also works with \emph{subgraphs}, which,
as~discussed in \secref{subgraphs},  may omit an initial
first node or a final last node.)
A~first node is always a @BlockId@;
a~typical middle node assigns to a register or memory
location;
and
a~typical last node is a conditional, unconditional, or indirect branch.
You choose the types of middle and last nodes to suit your
intermediate representation; if~these types are @m@~and~@l@, the type
\label{haskell.firstuse.m}% automated use
\label{haskell.firstuse.l}% automated use
of a basic block is @Block m l@.
\label{haskell.def.Block}% automated definition
\label{haskell.firstuse.Block}% automated use




First nodes are the only targets of control transfers;
middle nodes never perform control transfers;
and
last nodes always perform control transfers.
So~a~first node has arbitrarily many predecessors and exactly one
successor;
a~middle node has exactly one predecessor and one successor;
and a last node has exactly one predecessor and arbitrarily many
successors. 

These constraints on number of predecessors and successors determine
the signatures of 
transfer functions, 
which are shown in \figref{transfers}.
For each type of node (first, middle, last) and for each kind of
analysis (forward, backward), there is a distinct transfer function.
Functions are grouped by kind of analysis, and each group is
parameterized over a dataflow fact of type~@a@ and over the types
@m@~and~@l@ of middle and last nodes.  

\begin{figure}
\begin{code}
newtype LastOuts a = LastOuts [(BlockId, a)]
data ForwardTransfers m l a = ForwardTransfers
 {ft_first_out  :: BlockId -> a -> a,
  ft_middle_out :: m       -> a -> a,
  ft_last_outs  :: l       -> a -> LastOuts a} 

data BackTransfers m l a = BackTransfers
 {bt_first_in  :: BlockId -> a              -> a,
  bt_middle_in :: m       -> a              -> a,
  bt_last_in   :: l       -> (BlockId -> a) -> a} 
\end{code}
\caption{Transfer functions for forward and backward analyses.}
\figlabel{transfers}
\ifpagetuning\vspace*{-1ex}\fi
%
% elided: 
%    ft_exit_out   ::            a -> a
%
\label{haskell.def.bt:unlast:unin}% automated definition
\label{haskell.def.bt:unmiddle:unin}% automated definition
\label{haskell.def.bt:unfirst:unin}% automated definition
\label{haskell.def.BackTransfers}% automated definition
\label{haskell.def.ft:unlast:unouts}% automated definition
\label{haskell.def.ft:unmiddle:unout}% automated definition
\label{haskell.def.ft:unfirst:unout}% automated definition
\label{haskell.def.l}% automated definition
\label{haskell.def.m}% automated definition
\label{haskell.def.ForwardTransfers}% automated definition
\label{haskell.def.LastOuts}% automated definition
\label{haskell.firstuse.LastOuts}% automated use
\label{haskell.firstuse.ForwardTransfers}% automated use
\label{haskell.firstuse.^}% automated use
\label{haskell.firstuse.ft:unfirst:unout}% automated use
\label{haskell.firstuse.ft:unmiddle:unout}% automated use
\label{haskell.firstuse.ft:unlast:unouts}% automated use
\label{haskell.firstuse.BackTransfers}% automated use
\label{haskell.firstuse.bt:unfirst:unin}% automated use
\label{haskell.firstuse.bt:unmiddle:unin}% automated use
\label{haskell.firstuse.bt:unlast:unin}% automated use
\end{figure}


A fact in a forward analysis typically represents an assertion
about program state,
 and because a label does not change
program state, the transfer function @ft_first_out@ is often
@flip const@---a variation on
\label{haskell.firstuse.flip}% automated use
\label{haskell.firstuse.const}% automated use
the
identity function.\footnote
{Not every fact is about program state,
so not every forward analysis can ignore labels.
For example, dominator analysis and other all-paths analyses often
compute a set of labels through which control may (or must) pass.
}
For a middle node, the transfer function @ft_middle_out@ is given a
node and a precondition and returns an approximation of the strongest
postcondition. 
For a last node, different postconditions may be propagated to
different successors; for example, the true and false successors of a
conditional branch may accumulate information implied by the truth or
falsehood of the condition.
A~collection of (successor, fact) pairs is represented by a value of
type @LastOuts a@ (\figref{transfers}).




In a forward analysis, the dataflow engine starts with the fact at the
beginning of a block and applies transfer functions to the nodes in
that block until eventually the transfer function for the last node
computes the facts that are propagated to the block's successors.
For example, in the block
\begin{verbatim}
  L1: x = 7;
      y = 8;
      z = x + y;
      goto L2;
\end{verbatim}
a forward analysis would propagate the fact 
$@x@ = 7 \land @y@ = 8$, which we will call $f_{\mathit{new}}$,
along the edge to~\texttt{L2}. 
%%  \remark{We've elected not to get to the level of detail where
%%  we show how propagating a fact~$f$ through \mbox{@x = 7;@} results in a new
%%  fact either $(f \setminus @x@) \land @x == 7@$.}
The dataflow engine then \emph{replaces} the current fact
at~\texttt{L2}~($f_{\mathtt{L2}}$) with the lattice join $f_{\mathit{new}}
\join f_{\mathtt{L2}}$. 
The dataflow engine iterates over the blocks repeatedly, creating new
facts~$f$ and joining them with facts $f_{\mathit{id}}$ until
\mbox{$f \join f_{\mathit{id}} = f_{\mathit{id}}$} at every label~$\mathit{id}$.
When the facts at labels stop changing, the dataflow
engine has reached a fixed point.
%\remark{Promissory note: compose this analysis with two
%transformations: constant propagation and constant folding}


\ifpagetuning\enlargethispage{0.5\baselineskip}\fi


\subsection{Running the dataflow engine}

\seclabel{zdfSolveFwd}

\iftrue
Given lattice operations of type @DataflowLattice a@ (\figref{lattice})
together with
transfer functions of type @ForwardTransfers m l a@
(\figref{transfers}),
\else
Given lattice operations of type @DataflowLattice a@
and 
transfer functions of type @ForwardTransfers m l a@
(\figreftwo{lattice}{transfers}),
\fi
you can run the corresponding analysis by calling \ourlib\
function @zdfSolveFwd@, which is a~part of our dataflow engine
\label{haskell.firstuse.zdfSolveFwd}% automated use
(a~backward analysis calls function @zdfSolveBwd@, which has a similar type):
\label{haskell.def.zdfSolveBwd}% automated definition
\label{haskell.firstuse.zdfSolveBwd}% automated use
\begingroup\hfuzz=1pt\relax
\begin{code}
 zdfSolveFwd 
  :: HavingSuccessors l     -- Find successors of l
  => PassName               -- Name of the analysis
  -> DataflowLattice a      -- Lattice
  -> ForwardTransfers m l a -- Transfer functions
  -> a                      -- Input fact
  -> Graph m l              -- Control-flow graph
  -> FwdFixedPoint m l a ()
\end{code}
\label{haskell.def.zdfSolveFwd}% automated definition
\label{haskell.def.PassName}% automated definition
\label{haskell.firstuse.FwdFixedPoint}% automated use
\label{haskell.firstuse.Graph}% automated use
\label{haskell.firstuse.PassName}% automated use
\label{haskell.firstuse.HavingSuccessors}% automated use
The function is polymorphic in the types of middle and last nodes
@m@~and~@l@ and in the type of the dataflow fact~@a@.
Polymorphism allows \ourlib\ to work with any intermediate
language, as long as the type of last node @l@ satisfies the constraint
@HavingSuccessors l@ by providing a function
\label{haskell.def.HavingSuccessors}% automated definition
\ifcutting
@succs@ of type @l -> [BlockId]@,
\label{haskell.def.succs}% automated definition
\label{haskell.firstuse.succs}% automated use
\else
\begin{code}
  succs :: l -> [BlockId]
\end{code}
\fi
which gives the labels of the blocks to which a last node of type~@l@
might transfer control.

\endgroup

After the type constraint, 
the first three arguments to @zdfSolveFwd@ characterize the analysis.
The next argument is the dataflow fact that holds on entry to the
graph;
because a procedure's caller may establish some facts about
parameters or about the stack,
this fact
is not always~$\bot$.
The last argument to @zdfSolveFwd@ is the graph, and the result is a 
fixed point.

\ifpagetuning\enlargethispage{0.5\baselineskip}\fi


The @FwdFixedPoint@ data structure,
\label{haskell.def.FwdFixedPoint}% automated definition
whose final type parameter~@()@ is 
explained in~\secref{engine-truth},
 is a big bag
of information about a solution.
The most significant information is
a finite map from each block label to the dataflow fact that holds at
the label, which is extracted using function @zdfFpFacts@:
\label{haskell.firstuse.zdfFpFacts}% automated use
\label{haskell.def.emptyBlockEnv}% automated definition
\begin{code}
type BlockEnv a = Data.Map BlockId a
zdfFpFacts :: FwdFixedPoint m l a g -> BlockEnv a
\end{code}
\label{haskell.def.BlockEnv}% automated definition
\label{haskell.def.zdfFpFacts}% automated definition
\label{haskell.firstuse.g}% automated use
\label{haskell.firstuse..}% automated use
\label{haskell.firstuse.Data.Map}% automated use
\label{haskell.firstuse.BlockEnv}% automated use





%%  \iffalse
%%  % never tell the whole truth!
%%  \begin{code}
%%  class DataflowSolverDirection
%%          transfers fixedpt where
%%    zdfSolveFrom :: (DebugNodes m l, Outputable a)
%%      => BlockEnv a        -- Init facts
%%      -> PassName          -- Analysis name
%%      -> DataflowLattice a -- Lattice
%%      -> transfers m l a   -- Transfers
%%      -> a                 -- Input fact
%%      -> Graph m l         -- CFG
%%      -> fixedpt m l a ()
%%  \end{code}
%%  \fi



%%  The main contribution of this paper is to present an interface which
%%  enables many powerful program transformations based on dataflow
%%  analysis while keeping the individual dataflow passes as simple as
%%  possible.
%%  We keep the \emph{concepts} simple by relating dataflow facts and
%%  transfer functions to classic work in program correctness, as
%%  discussed in \secref{next-700}.
%%  We keep the \emph{implementations of dataflow passes} simple by pushing
%%  as much work as
%%  possible into the dataflow engine, which is implemented just once.
%%  We have also made the dataflow engine and its interface polymorphic in
%%  the types of 
%%  the nodes that appear in the control-flow graph \secref{polymorphic-framework}.
%%  Parametricity ensures separation of concerns between the dataflow
%%  engine and the individual dataflow passes.





\section {Related work}

While dataflow analysis and optimization are covered
by a vast literature, 
\emph{design} of optimizers, the topic of this paper, is covered
relatively sparsely.
We therefore focus on foundations.

When transfer functions are monotone and lattices are finite in height,
iterative dataflow analysis converges to a fixed point
\cite{kam-ullman:global-iterative-analysis}. 
If~the lattice's join operation distributes over transfer
functions,
this fixed point is equivalent to a join-over-all-paths solution to
the recursive dataflow equations
\cite{kildall:unified-optimization}.\footnote
{Kildall uses meets, not joins.  
Lattice orientation is conventional, and conventions have changed.
We use Dana Scott's
orientation, in which higher elements carry more information.}
\citet{kam-ullman:monotone-flow-analysis} generalize to some
monotone functions.
Each~client of \hoopl\ must guarantee monotonicity,
but for transfer functions that
approximate weakest preconditions or strongest postconditions,
monotonicity falls out naturally.

\ifcutting
\citet{cousot:abstract-interpretation:1977}
\else
\citet{cousot:abstract-interpretation:1977,cousot:systematic-analysis-frameworks}
\fi
introduce abstract interpretation as a technique for developing
lattices for program analysis.
\citet{schmidt:data-flow-analysis-model-checking} shows that
an all-paths dataflow problem can be viewed as model checking an
abstract interpretation.

The soundness of interleaving analysis and transformation,
even when some speculative transformations are not performed on later
iterations, was shown by
\citet{lerner-grove-chambers:2002}.
\ifcutting\else
Muchnick \citeyearpar{muchnick:compiler-implementation} 
presents many examples of both particular analyses and related
algorithms.
\fi


\newcommand\T{\rule{0pt}{0.6ex}}
\newcommand\B{\rule[-0.05ex]{0pt}{0pt}}
\renewcommand\B{\relax\par\unskip\vspace*{0.8ex}%
  \hrule height 0pt depth 0pt \relax}
\renewcommand\T{\relax\par\unskip\vspace*{1.0ex}%
  \hrule height 0pt depth 0pt \relax}
\newcolumntype{C}{>{\begin{minipage}{5.35in}}l<{\end{minipage}}}
%\newcolumntype{L}{>{\T\Large\bfseries\indent}m{1.3in}%
%                  <{\fillindent \parfillskip=0pt \parskip=0pt \endgraf}}       % label
\newcolumntype{L}{>{\T\Large\bfseries}m{1.3in}<{\centering}}      
\newcommand\fillindent{\parindent=0pt \leftskip=0pt plus 1fill}
\newcommand\ltab[1]{\begin{tabular}{@{}c@{}}#1\\\end{tabular}}
\label{haskell.firstuse.c}% automated use
\renewcommand\ltab[1]{{\let\\=\relax#1}}
\newenvironment{codetable}
  {\setcounter{codeline}{0}%
   \let\code=\numberedcode
   \let\endcode=\endnumberedcode
   \begin{tabular}{CL}%
  }
  {\end{tabular}}

\begin{figure*}\hfuzz=1pt
\setcounter{codeline}{0}
\begin{codetable}
\T
\begin{numberedcode}
!AvailVars!data AvailVars = UniverseMinus VarSet | AvailVars VarSet
!extendAvail!extendAvail  :: AvailVars -> LocalVar  -> AvailVars  -- add var to set
delFromAvail :: AvailVars -> LocalVar  -> AvailVars  -- remove var from set
elemAvail    :: AvailVars -> LocalVar  -> Bool       -- set membership
interAvail   :: AvailVars -> AvailVars -> AvailVars  -- set intersection
!smallerAvail!smallerAvail :: AvailVars -> AvailVars -> Bool       -- compare sizes
\end{numberedcode}
\B
& \ltab{Dataflow fact\\ and operations}\\
\hline

\T\hfuzz=45pt
\begin{code}
availVarsLattice :: DataflowLattice AvailVars
availVarsLattice = DataflowLattice empty add
    where empty = UniverseMinus emptyVarSet
          add new old = let join = interAvail new old in
                        (if join `smallerAvail` old then SomeChange else NoChange, join)
\end{code}
\B
& Lattice\\
\hline

%%  \T
%%  \begin{code}
%%  agen  :: UserOfLocalVars    a => a -> AvailVars -> AvailVars
%%  akill :: DefinerOfLocalVars a => a -> AvailVars -> AvailVars
%%  agen  a avail = foldVarsUsed extendAvail  avail a
%%  akill a avail = foldVarsDefd delFromAvail avail a
%%  \end{code}\B
%%  & Gen/Kill \mbox{functions}\\
%%  \hline
%%  
\T\hfuzz=87pt
\begin{code}
availTransfers :: ForwardTransfers CmmMiddle CmmLast AvailVars
!avail.first!availTransfers = ForwardTransfers (flip const) middleAvail lastAvail

middleAvail :: CmmMiddle -> AvailVars -> AvailVars
!reload1!middleAvail (MidAssign (CmmLocal x) (CmmLoad l) avail | l `isStackSlotOf` x = extendAvail avail x
!assign.avail.1!middleAvail (MidAssign lhs _expr) avail = foldVarsDefd delFromAvail avail lhs
!store.avail.spill.1!middleAvail (MidStore l (CmmVar (CmmLocal x))) avail | l `isStackSlotOf` x = avail
!store.avail.otherslot.1!middleAvail (MidStore l _) avail | isStackSlot l = delFromAvail avail (varOfSlot l)
!store.avail.other!middleAvail (MidStore _ _) avail = avail

lastAvail :: CmmLast -> AvailVars -> LastOuts AvailVars
!avail.LastCall!lastAvail (LastCall _ (Just k) _ _) _ = LastOuts [(k, AvailVars emptyVarSet)]
lastAvail l avail = LastOuts $ map (\id -> (id, avail)) $ succs l
\end{code}
\B
& \hfuzz=1.8pt \mbox{\phantom{SPACE}}
\mbox{\phantom{XXXX}Transfer} \mbox{\phantom{XXXX}functions}\\
% \ltab{Transfer \\Functions}\\
\hline

\T
\begin{code}
cmmAvailableVars :: Graph CmmMiddle CmmLast -> BlockEnv AvailVars
cmmAvailableVars g = zdfFpFacts fp
!avail.solve.1!  where fp = zdfSolveFwd "available variables" availVarsLattice 
!avail.solve.2!                 availTransfers (fact_bot availVarsLattice) g
\end{code}
\B
&
\def\baselinestretch{0.8}\hspace{-0.3in}\parbox{1.6in}{\center Available-variables analysis}
%\B
\\
%\hline

\end{codetable}
% \caption{Available-variable analysis}
\caption{Dataflow analysis pass to compute available variables}
\figlabel{avail-all}
\figlabel{avail}
\figlabel{avail-lattice}
\figlabel{avail-gen-kill}
\figlabel{avail-transfers}
\figlabel{avail-running}
\label{haskell.def.fp}% automated definition
\label{haskell.def.cmmAvailableVars}% automated definition
\label{haskell.def.lastAvail}% automated definition
\label{haskell.def.:unexpr}% automated definition
\label{haskell.def.lhs}% automated definition
\label{haskell.def.avail}% automated definition
\label{haskell.def.middleAvail}% automated definition
\label{haskell.def.availTransfers}% automated definition
\label{haskell.def.join}% automated definition
\label{haskell.def.old}% automated definition
\label{haskell.def.new}% automated definition
\label{haskell.def.add}% automated definition
\label{haskell.def.empty}% automated definition
\label{haskell.def.availVarsLattice}% automated definition
\label{haskell.def.smallerAvail}% automated definition
\label{haskell.def.interAvail}% automated definition
\label{haskell.def.elemAvail}% automated definition
\label{haskell.def.delFromAvail}% automated definition
\label{haskell.def.extendAvail}% automated definition
\label{haskell.def.UniverseMinus}% automated definition
\label{haskell.def.AvailVars}% automated definition
\label{haskell.firstuse.cmmAvailableVars}% automated use
\label{haskell.firstuse.fp}% automated use
\label{haskell.firstuse.availTransfers}% automated use
\label{haskell.firstuse.CmmMiddle}% automated use
\label{haskell.firstuse.CmmLast}% automated use
\label{haskell.firstuse.middleAvail}% automated use
\label{haskell.firstuse.lastAvail}% automated use
\label{haskell.firstuse.MidAssign}% automated use
\label{haskell.firstuse.CmmLocal}% automated use
\label{haskell.firstuse.CmmLoad}% automated use
\label{haskell.firstuse.avail}% automated use
\label{haskell.firstuse.isStackSlotOf}% automated use
\label{haskell.firstuse.lhs}% automated use
\label{haskell.firstuse.:unexpr}% automated use
\label{haskell.firstuse.foldVarsDefd}% automated use
\label{haskell.firstuse.MidStore}% automated use
\label{haskell.firstuse.CmmVar}% automated use
\label{haskell.firstuse.isStackSlot}% automated use
\label{haskell.firstuse.varOfSlot}% automated use
\label{haskell.firstuse.LastCall}% automated use
\label{haskell.firstuse.k}% automated use
\label{haskell.firstuse.map}% automated use
\label{haskell.firstuse.id}% automated use
\label{haskell.firstuse.$}% automated use
\label{haskell.firstuse.:bs}% automated use
\label{haskell.firstuse.availVarsLattice}% automated use
\label{haskell.firstuse.empty}% automated use
\label{haskell.firstuse.add}% automated use
\label{haskell.firstuse.emptyVarSet}% automated use
\label{haskell.firstuse.new}% automated use
\label{haskell.firstuse.old}% automated use
\label{haskell.firstuse.join}% automated use
\label{haskell.firstuse.AvailVars}% automated use
\label{haskell.firstuse.UniverseMinus}% automated use
\label{haskell.firstuse.VarSet}% automated use
\label{haskell.firstuse.extendAvail}% automated use
\label{haskell.firstuse.LocalVar}% automated use
\label{haskell.firstuse.delFromAvail}% automated use
\label{haskell.firstuse.elemAvail}% automated use
\label{haskell.firstuse.Bool}% automated use
\label{haskell.firstuse.interAvail}% automated use
\label{haskell.firstuse.smallerAvail}% automated use
\end{figure*}

\section{Example analysis passes}

\seclabel{example-analyses}


\ourlib\
makes it easy to write compiler passes based on dataflow.
To show \emph{how} easy, we present
two analyses;
related transformations appear in \secref{example-rewrites}. 
The examples help solve a real problem in the Glasgow Haskell
Compiler:
because most calls are tail calls, GHC uses no 
callee-saves registers.
Therefore, at each (rare) non-tail call, all live
variables must be spilled to the stack.
\ifcutting\else
To reduce register pressure,
such variables are spilled as early as possible and reloaded as late as
possible. 
\fi

To illustrate the results of the example analyses and transformations,
here is a contrived example program in the style of \secref{example:xforms}:
\begin{alltt}
f (bits32 a) \lbr
  bits32 w, x, y, z;  // local variables
  x = a * a;
  w = a + a + a;
  y = g(w);           // call; x must be spilled
  z = y + y;
  if (y > 0) \lbr
    return z;
  \rbr else \lbr
    return z + x;
  \rbr
\rbr
\end{alltt}
%\ifpagetuning\par\vfilbreak\fi % this is hell, but if we break here,
%  downstream suffers
A~spill and a reload should be inserted as follows:%
\seclabel{spill-reload-example}
\newcommand\bigstrut{%
  \leavevmode\vrule width 0pt height 11pt depth 6pt }
\begin{alltt}
f (bits32 a) \lbr
  bits32 w, x, y, z;  
  x = a * a;
  \high{SPILL x;}
  w = a + a + a;    // no register pressure from x
  y = g(w);
  z = y + y;        // no register pressure from x
  if (y > 0) \lbr
    return z;       // x does not need reloading
  \rbr else \lbr
    \high{RELOAD x;}
    return z + x;
  \rbr
\rbr
\end{alltt}
Although the \texttt{SPILL} and \texttt{RELOAD} operations are introduced because of
the call to @g(a)@, they are moved as far from the call as possible:
@x@~is spilled immediately after being assigned @a * a@,
and @x@~is reloaded not
immediately after the call to~@g@, but just before its use in the
expression @z + x@.
On the control-flow path to @return z@, @x@~needn't be reloaded
\label{haskell.firstuse.return}% automated use
at all.




Spills and reloads are inserted 
\ifcutting\else in the right places \fi
by a sequence of 
\ifcutting\else three \fi
dataflow passes:
\finalremark{Uses of ``passes'' is not explained and is not very consistent.
Would be nice to say ``pass = analysis + (possibly degenerate)
transformation.''}%
\begin{enumerate}
\item
\label{insert-spills}
A backward analysis computes liveness
to identify the variables that should be spilled at call sites
(\secref{liveness} and \figref{liveness}).
An accompanying transformation (not shown) inserts reloads immediately
after each call 
site and inserts spills not immediately before call sites, but
rather immediately after the reaching definitions.
\item
\label{reload-duplication}
A forward analysis finds
``available variables'' which have been reloaded 
from the stack (\secref{avail} and \figref{avail}), 
and an accompanying transformation
inserts redundant reloads before their uses
(\secref{sink-reloads} and \figref{avail-rewrites}).
By keeping variables on the stack longer, this pass reduces register pressure.
% \simon{Why is the second pass a second pass?  The first
% pass added spills and reloads in; could the first pass not have 
% added the reloads immediately before the
% reloaded variables are used as well?  Maybe the text can say?
% I think the answer is that this is a \emph{forwards} analysis, since
% it is propagating forward the information about which variables currently
% have an up-to-date stack slot.}
\item
\label{remove-dead-reloads}
A backward analysis (the same as in pass~\ref{insert-spills}) computes
liveness, 
and an accompanying transformation (\figref{dead-elim} in
\secref{dead-code-elimination}), dead-assignment elimination, 
removes redundant reloads.
\end{enumerate}
%%%%%Pass~\ref{insert-spills} is not shown in this paper.
Passes
\ref{reload-duplication}~and~\ref{remove-dead-reloads} cooperate to ``sink''
reloads away from the call site.
%%  The analyses used in
%%  passes~\ref{reload-duplication}~and~\ref{remove-dead-reloads}
%%  are described in \secreftwo{avail}{liveness};
%%  the transformations are described in
%%  \secreftwo{sink-reloads}{dead-code-elimination}.


%% \ifpagetuning \enlargethispage\baselineskip \fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  desperately trying to get figures to emerge in a decent order
%
\begin{figure*}\hfuzz=1pt
\begin{codetable}
\T
\begin{code}
!Live!type Live = VarSet
\end{code}
\B
& Dataflow fact\\
\hline

\T\hfuzz=21pt
\begin{code}
!liveLattice!liveLattice :: DataflowLattice Live
liveLattice = DataflowLattice emptyVarSet add
  where add new old =
          let join = unionVarSets new old in
!liveLattice.end!          (if sizeVarSet join > sizeVarSet old then SomeChange else NoChange, join)
\end{code}
\B
& Lattice\\
\hline

%%  \T
%%  \begin{code}
%%  gen  :: UserOfLocalVars    a => a -> Live -> Live
%%  kill :: DefinerOfLocalVars a => a -> Live -> Live 
%%  gen  a live = foldVarsUsed extendVarSet  live a
%%  kill a live = foldVarsDefd delFromVarSet live a
%%  \end{code}\B
%%  & Gen/Kill \mbox{functions}\\
%%  \hline

\T\hfuzz=7pt
\begin{code}
liveTransfers :: BackTransfers CmmMiddle CmmLast Live
liveTransfers = BackTransfers (flip const) middleLiveness lastLiveness

middleLiveness :: CmmMiddle -> Live -> Live
lastLiveness   :: CmmLast -> (BlockId -> Live) -> Live
!middleLiveness!middleLiveness m = addUsed m . remDefd m
!lastLiveness!lastLiveness   l = addUsed l . remDefd l . lastLiveOut l 

!liveness.addUsed.sig!addUsed :: UserOfLocalVars    a => a -> Live -> Live
remDefd :: DefinerOfLocalVars a => a -> Live -> Live 
addUsed a live = foldVarsUsed extendVarSet  live a
!liveness.remDefd.def!remDefd a live = foldVarsDefd delFromVarSet live a

lastLiveOut :: CmmLast -> (BlockId -> Live) -> Live
!lastLiveOut.1!lastLiveOut l env = last l 
!live.lastBranch!  where last (LastBranch id)        = env id 
!live.condBranch!        last (LastCondBranch _ t f) = unionVarSets (env t) (env f)
!live.lastSwitch!        last (LastSwitch _ tbl)     = unionManyVarSets $ map env (catMaybes tbl)
!live.lastCall!        last (LastCall { })         = emptyVarSet
\end{code}
\B
& Transfer \mbox{functions}\\
\hline

\T
\begin{code}
cmmLiveness :: Graph CmmMiddle CmmLast -> BlockEnv Live
cmmLiveness g = zdfFpFacts fp
!live.zdfSolveBwd!   where fp = zdfSolveBwd "liveness" liveLattice liveTransfers emptyVarSet g
\end{code}
\B
& Liveness \mbox{analysis}\\
\end{codetable}
\caption{Dataflow analysis pass to compute liveness}
\figlabel{liveness-all}
\figlabel{liveness}
\figlabel{live-lattice}
\figlabel{live-transfers}
\figlabel{live-running}
\label{haskell.def.cmmLiveness}% automated definition
\label{haskell.def.catMaybes}% automated definition
\label{haskell.def.tbl}% automated definition
\label{haskell.def.last}% automated definition
\label{haskell.def.env}% automated definition
\label{haskell.def.lastLiveOut}% automated definition
\label{haskell.def.remDefd}% automated definition
\label{haskell.def.live}% automated definition
\label{haskell.def.addUsed}% automated definition
\label{haskell.def.lastLiveness}% automated definition
\label{haskell.def.middleLiveness}% automated definition
\label{haskell.def.liveTransfers}% automated definition
\label{haskell.def.liveLattice}% automated definition
\label{haskell.def.Live}% automated definition
\label{haskell.firstuse.cmmLiveness}% automated use
\label{haskell.firstuse.liveTransfers}% automated use
\label{haskell.firstuse.middleLiveness}% automated use
\label{haskell.firstuse.lastLiveness}% automated use
\label{haskell.firstuse.addUsed}% automated use
\label{haskell.firstuse.remDefd}% automated use
\label{haskell.firstuse.lastLiveOut}% automated use
\label{haskell.firstuse.UserOfLocalVars}% automated use
\label{haskell.firstuse.DefinerOfLocalVars}% automated use
\label{haskell.firstuse.live}% automated use
\label{haskell.firstuse.foldVarsUsed}% automated use
\label{haskell.firstuse.extendVarSet}% automated use
\label{haskell.firstuse.delFromVarSet}% automated use
\label{haskell.firstuse.env}% automated use
\label{haskell.firstuse.last}% automated use
\label{haskell.firstuse.LastBranch}% automated use
\label{haskell.firstuse.LastCondBranch}% automated use
\label{haskell.firstuse.t}% automated use
\label{haskell.firstuse.f}% automated use
\label{haskell.firstuse.LastSwitch}% automated use
\label{haskell.firstuse.tbl}% automated use
\label{haskell.firstuse.unionManyVarSets}% automated use
\label{haskell.firstuse.catMaybes}% automated use
\label{haskell.firstuse.liveLattice}% automated use
\label{haskell.firstuse.unionVarSets}% automated use
\label{haskell.firstuse.sizeVarSet}% automated use
\label{haskell.firstuse.>}% automated use
\label{haskell.firstuse.Live}% automated use
\end{figure*}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Choosing node types for GHC}

To~show that \ourlib\ works at scale,
we present examples that
have been implemented and tested in GHC.  
GHC's low-level intermediate code, called @Cmm@, is a 
\label{haskell.def.Cmm}% automated definition
\label{haskell.firstuse.Cmm}% automated use
 subset of 
the portable assembly language~{\PAL}
\cite{peyton-jones-ramsey:single-intermediate}.
We~specialize \ourlib\ to GHC by instantiating type parameters
@m@~and~@l@ with GHC's types @CmmMiddle@ and @CmmLast@.

\ifpagetuning
\penalty-10000

\enlargethispage{1.0\baselineskip}
\fi


A~middle node stores the value of an expression:
\begin{code}
data CmmMiddle 
  = MidAssign CmmVar  CmmExpr -- store in variable
  | MidStore  CmmExpr CmmExpr -- store in memory
\end{code}
\label{haskell.def.CmmMiddle}% automated definition
\label{haskell.def.MidAssign}% automated definition
\label{haskell.def.MidStore}% automated definition
\label{haskell.firstuse.CmmExpr}% automated use
Type @CmmVar@ represents a variable, which may be local (@CmmLocal@
\label{haskell.def.CmmVar}% automated definition
\label{haskell.def.CmmLocal}% automated definition
@LocalVar@) or global 
\label{haskell.def.LocalVar}% automated definition
(@CmmGlobal GlobalVar@).
\label{haskell.def.CmmGlobal}% automated definition
\label{haskell.def.GlobalVar}% automated definition
\label{haskell.firstuse.CmmGlobal}% automated use
\label{haskell.firstuse.GlobalVar}% automated use
Type @CmmExpr@ represents a pure expression; 
\label{haskell.def.CmmExpr}% automated definition
among its constructors are @CmmLoad@ (a~value from memory)
\label{haskell.def.CmmLoad}% automated definition
and @CmmVar@ (the value of a variable).

A~last node represents a control transfer; constructors include
unconditional, conditional, and indirect branches, as well as a call:
\begin{code}
data CmmLast
  = LastBranch     BlockId
  | LastCondBranch CmmExpr BlockId BlockId
  | LastSwitch     CmmExpr [Maybe BlockId]
  | LastCall ...     -- arguments omitted
\end{code}
\label{haskell.def.CmmLast}% automated definition
\label{haskell.def.LastBranch}% automated definition
\label{haskell.def.LastCondBranch}% automated definition
\label{haskell.def.LastSwitch}% automated definition
\label{haskell.def.LastCall}% automated definition
\label{haskell.firstuse.Maybe}% automated use






\subsection{Available variables: a forward analysis supporting pass 2} 

\seclabel{avail}




To~understand the available-variables analysis, you must know that 
each variable~$x$ is related to a stack slot~\slotof x, which is used to
save the value of~$x$. 
(GHC~represents 
the relation using Haskell functions @isStackSlot@,
@varOfSlot@, and @isStackSlotOf@.)
\label{haskell.def.isStackSlotOf}% automated definition
\label{haskell.def.isStackSlot}% automated definition
\label{haskell.def.varOfSlot}% automated definition
If the variable and the stack slot hold the same value,
that is if $x = \slotof x$,
then it is \emph{safe} to insert a reload.

To sink a reload of a variable $x$, we insert redundant reloads immediately
before uses of~$x$.
%We use an analysis that identifies not only when it is safe to insert
%such a reload, but when the inserted reload will make an earlier reload redundant.
%Specifically, 
It is \emph{profitable} to insert a reload before a use of~$x$ only if, 
on every path to the use, the most recent definition of $x$~is a reload from
$\slotof x$.
\delendum{I'm confused.  Surely that definition of profitable is also what we
mean by safe?  NR: No---safety could be establish by an arbitrary
assignment to~$x$ followed by a store to $\slotof x$.}
Safety and profitability are incomparable;
the dataflow fact computed by our analysis is the set we call
\emph{available} variables, for
which it is
safe \emph{and} profitable to insert a reload.
Because the assertion of interest is an ``all-paths'' property, 
the lattice-join operation is set intersection,
and the bottom element
is the universal set containing all variables.
%%It~is optimistic to assume that any variable can be safely reloaded from the stack,
%%but this optimistic assumption is what enables us to improve the
%%code---and if the assumption is not justified, the transfer functions
%%will correct it before
%%the analysis reaches a fixed point.
\delendum{%I'm guessing that you intend this sentence to address my long bleat:
I'm puzzled about why you are treating this example so differently
to constant-prop in Section 3.1.  It looks almost identical to me.  We could
keep a fact for every variable: $x=\bot$ means nothing is known; $x=s_x$ means
x's stack slot is up to date; $x=\top$ means x's stack slot is out of date.
Then keep a finite map as we do for constant prop.  If there is a difference
that drives the rep you have here, let's say so. If the difference is
purely accidental, we should eliminate it.

I am still unhappy on this point.  I think the difficulty is that we
want to represent \emph{both} the universal set \emph{and} the empty set.
But I'm unclear about the trade-offs in approximation that arise 
from this either-or representation.

NR: The lattice here has only two values per variable: either it is
safe and profitable to reload $x$ at this point (an all-paths
property) or it isn't.   A finite map to @Bool@ would be suitable (and
is traditional) but requires both the tedium and the run-time cost of
enumerating all the variables.
}

%\ifpagetuning\enlargethispage{\baselineskip}\fi
  


Instead of the usual mutable bit vectors, we use a purely functional
representation of sets---one in which we can represent the set of all
variables without enumerating them.
A~set is either 
$\mathtt{UniverseMinus}\;s$, which stands for all variables except
those in the set~$s$,
or $\mathtt{AvailVars}\;s$, which stands for the variables in the set~$s$
(\figref{avail-lattice}, \lineref{AvailVars}).
%
%
The bottom element is @UniverseMinus emptyVarSet@.
To manipulate these sets, we provide the functions declared in
\linerangeref{extendAvail}{smallerAvail} of \figref{avail}.
%
\label{haskell.def.delFromVarSet}% automated definition
\label{haskell.def.extendVarSet}% automated definition
\label{haskell.def.unionVarSets}% automated definition
\label{haskell.def.sizeVarSet}% automated definition
\label{haskell.def.emptyVarSet}% automated definition
\label{haskell.def.unionManyVarSets}% automated definition
\label{haskell.def.isEmptyVarSet}% automated definition
\label{haskell.def.elemVarSet}% automated definition
\label{haskell.def.VarSet}% automated definition



%%  Among many other uses, an available-expressions analysis can be~used
%%  in code-motion optimizations.
%%  For example, when making a function call, we insert
%%  spills and reloads to save and restore the values of local variables
%%  around the call site.
%%  It is easy to insert the reloads at the function-call return site,
%%  but to avoid register pressure, it would be better to leave the variable
%%  where it was spilled on the stack.
%%  Rather than complicating the code that inserts the spills and
%%  reloads around call sites,
%%  we write an analysis to insert a redundant reload immediately
%%  before a reloaded variable is used.
%%  Then, we rely on a dead-code elimination to~remove
%%  the early reload.






\ifpagetuning
\enlargethispage{1.0\baselineskip} 
\fi

% \ifpagetuning\enlargethispage{0.5\baselineskip}\fi

The most interesting part of the analysis is the @middleAvail@ transfer
function in \figref{avail-transfers}.\finalremark
{Let us revise the paper to pretend that global variables
don't exist.}
\begin{itemize}
\item
\Lineref{reload1} 
identifies an assignment that reloads local
variable~@x@ from its stack slot.\finalremark{I propose the compiler be
modified to use @isStackSlotOf@ as I've written. JD~approves.}
After such an assignment, $x = \slotof x$,
and the last definition of $x$ is a reload,
so @x@~is added to the set of available variables.
% No other variable is affected.
\item
On \lineref{assign.avail.1},
an assignment to a local variable means that the
variable need not be equal to the value in its stack
slot, so if @lhs@ is a local variable, it is removed from the set of
available variables.
The conditional removal is done by applying @foldVarsDefd@ to @delFromAvail@;
@foldVarsDefd@ is an overloaded function which, along with its dual,
is used throughout the back end:
\begin{code}
foldVarsUsed :: UserOfLocalVars a 
        => (b -> LocalVar -> b) -> b -> a -> b
foldVarsDefd :: DefinerOfLocalVars a 
        => (b -> LocalVar -> b) -> b -> a -> b
\end{code}
\label{haskell.def.foldVarsUsed}% automated definition
\label{haskell.def.UserOfLocalVars}% automated definition
\label{haskell.def.foldVarsDefd}% automated definition
\label{haskell.def.DefinerOfLocalVars}% automated definition
\label{haskell.firstuse.b}% automated use
On \lineref{assign.avail.1},  if @lhs@ is a local variable,
@foldVarsDefd@ calls @delFromAvail@;
if @lhs@ is global, @foldVarsDefd@ does nothing.


%\ifpagetuning\smallskip\fi  
 

%% \remark{To Simon and John: I've removed ``overloaded but am a bit nervous
%% because @foldVarsDef@ shows up as overloaded later in the paper, and I
%% had thought to signpost it here.  Your thoughts?}
\item 
There are three cases for @MidStore@ nodes.
\Lineref{store.avail.spill.1}
matches a node that spills a variable~$\mathtt{x}$ to the stack.
After such a node, $\mathtt{x} = \slotOf {\mathtt{x}}$,
but the node is not a reload instruction,
so @x@~is not added to the set of available variables.
%
\Lineref{store.avail.otherslot.1}
matches a node that writes any \emph{other} value to a stack slot,
after which the variable associated with that slot is no longer available.
%
\Lineref{store.avail.other} matches a store to a location that is not
a stack slot, which leaves the set of available variables unchanged.
%%
%%
%%
%%
% \footnote
% {Although @MidStore@ may overwrite a stack slot \slotof x, GHC
% carefully arranges that all stores to \slotof x have the form
% $\slotof{\mathtt{x}}= @x@$.
% These stores could be used to extend the set of available variables,
% but it is not useful to do so.}
% \simon{Why not? asks the reader.  Perhaps
% because such saves immediately precede calls?}
% \remark{How do we know that @MidStore@ doesn't
% destroy a stack slot??  I've put in a footnote but it will probably be
% simpler to fix the code.}
% \simon{Ah, this harks backe to the start of this sub-section, where
% we say ``to understand the analysis, you must know that...''.  Another
% thing you must know is that $s_x$ is used exclusively for $x$.  That's
% all, I think.}
\end{itemize}
%% \john{I've expunged MidComment from our example} % well done ----NR


The transfer function for a last node checks to see if the node is a
function call (\lineref{avail.LastCall}); if so, the set of
available variables at the call's continuation is empty.
Other last nodes do not change values of variables or stack slots, 
so the set of available variables remains unchanged.
%
A~first node has no effect on program state, so its transfer function
is @flip const@ (\lineref{avail.first}).

%%Using the @agen@ and @akill@ functions, we define the transfer functions
%%for the available-reloads analysis (see~\figref{avail-transfers}):
%%\begin{itemize}
%%\item \emph{First nodes}:
%%  A first node cannot define a variable,
%%  so the set of available reloads is the same before and after a first node.
%%  We return the set unchanged using @flip const@.
%%\item \emph{Middle nodes}:
%%  If a middle node reloads a value from a register's slot on the stack (@RegSlot@),
%%  then we add the register the register to the set of available reloads.
%%  For any other assignment to a register, we remove the register from the
%%  set of available reloads.
%%  Similarly, because a function call can overwrite the value of any local variable,
%%  the set of available reloads is empty after a~function call.
%%  Any other middle node leaves the set of available reloads unchanged.
%%\item \emph{Last nodes}:
%%  If the last node is a function call, the outgoing set of available reloads
%%  is empty for every successor basic block.
%%  Otherwise, the last node cannot modify any variables,
%%  so the set of available reloads remains unchanged.
%%\end{itemize}

Given the lattice and the transfer functions,
we can perform the analysis by calling
the \ourlib\ function @zdfSolveFwd@ (\figref{avail},
\linerangeref{avail.solve.1}{avail.solve.2}). 
\delendum{But this is really a lie. We actually call the transformation 
function!  I'm not quite sure how to fix this pedagogical point.
NR: It's true that we don't actually have a use for the results of an
independent available-variables analysis, other than to drive the
transformation in the next section.  But just because we have no use
for the results at present does not  make the example analysis
incorrect or invalid, and I think the pedagogy is sound.
}
%The function @zdfFpFacts@ returns 
%a finite map from basic-block IDs to the set of available variables
%at the beginning of each block.
Except for the implementations of the set operations on
\linerangeref{extendAvail}{smallerAvail}, 
\figref{avail} shows the \emph{entire} analysis.

\subsection{Liveness: a backward analysis supporting passes 1~and~3} 

\seclabel{liveness}

The assertion computed by 
a backward dataflow analysis applies to a
\emph{continuation} at a program point.
The classic example is liveness analysis;
the assertion of interest is that at a particular program point,
the answer produced by the continuation does not depend on
the value of a particular variable~$x$.
If~so, $x$~is said to be \emph{dead} at that point.
If the answer produced by the continuation \emph{might} depend on the
value of~$x$, $x$~is \emph{live}.\footnote
{Liveness cannot be decided accurately; it reduces to the halting problem.
As usual, we approximate liveness by reachability.}

In a modern compiler, liveness analysis supports many program
transformations,
including
dead-assignment elimination,
which removes assignments to dead variables, 
and register allocation, which
ensures that if two variables are 
live at the same time, they are not assigned to the same register. 

The dataflow fact we use to represent liveness assertions is the set of
live variables (\figref{live-lattice}, \lineref{Live}).
The bottom element of the lattice is the empty set, and the join
operation is set union (\figref{live-lattice},
\linerangeref{liveLattice}{liveLattice.end}); 
a~variable is deemed live after a node if it is live on \emph{any} edge leaving that
node.

The transfer functions for liveness rely on two auxiliary functions
@addUsed@ and @remDefd@ (\figref{liveness}, 
\linerangeref{liveness.addUsed.sig}{liveness.remDefd.def}).
A~transfer function is given a set of variables live on the edges
going out of the node.
It~removes from that set any variable
defined by the node, then adds any variable used by the
node%
\ifcutting\ \else.
For~example, if the node is
\begin{verbatim}
  i = n - 1;
\end{verbatim}
then @i@ is not live just before the node, since if we start the
program just before the assignment to~@i@, the answer cannot 
depend on the value of~@i@, which is about to be overwritten.
But the answer \emph{might} depend on the value of~@n@, so
@n@~is considered live before the assignment.
If a variable appears on both sides of an
assignment, as in \ifpagetuning{\looseness=-1 \par}\fi
\begin{verbatim}
  i = i + 1;
\end{verbatim}
then the answer might depend on~@i@, so @i@~is considered live
before the assignment.
The transfer functions
therefore
remove defined variables \emph{before} adding used variables
\fi
(\figref{liveness}, \linepairref{middleLiveness}{lastLiveness}). 

For a last node, function @lastLiveOut@ consults the solution in
progress (parameter~@env@ on \lineref{lastLiveOut.1}) to find out what
variables are live at the \emph{successors} of a 
last node. 
For an unconditional branch, we look up the live set at the label
branched to (\lineref{live.lastBranch});
for a conditional branch, we look at both true and false edges
(\lineref{live.condBranch}), 
 and
for a switch, we consider every possible target of the
branch (\lineref{live.lastSwitch}).
The~remaining case (\lineref{live.lastCall}) is a call, 
and since a call destroys the values of all local variables, no
local variables are live at its continuation.

Given the lattice and the transfer functions,
we perform liveness analysis by calling
the dataflow-engine function @zdfSolveBwd@ (\figref{liveness},
\lineref{live.zdfSolveBwd}). 
%The function @zdfFpFacts@ returns 
%a finite map from basic-block IDs to the set of live variables
%at the beginning of each block.
\figref{liveness} shows the \emph{entire} analysis.



%%%%    \ifgenkill
%%%%    \subsection{Writing transfer functions using {\mdseries\texttt{gen}} and
%%%%    {\mdseries\texttt{kill}}}
%%%%    
%%%%    %\remark{I've tried to rewrite this section without the abstract guff;
%%%%    %Simon, can you let us know what you think?}
%%%%    
%%%%    
%%%%    \seclabel{gen-kill}
%%%%    
%%%%    If you pick up a compiler textbook, you might see dataflow
%%%%    analysis explained in terms of functions called \texttt{gen} and
%%%%    \texttt{kill}, 
%%%%    which say what dataflow facts are ``generated'' and
%%%%    ``killed'' by each node.
%%%%    Our design is compatible with explanations based on \texttt{gen} and
%%%%    \texttt{kill};
%%%%    first you define functions \texttt{gen} and \texttt{kill}, then you
%%%%    use them to write the transfer functions.
%%%%    %%  Using our design, you can stillIt's~possible to write transfer functions i thi
%%%%    %%  In~a forward analysis, for example an assignment might establish an
%%%%    %%  assertion (\texttt{gen}), or it might change state in such a way that an
%%%%    %%  assertion no longer holds (@kill@), or both.
%%%%    %%  If~you want to transliterate specifications that use @gen@ and @kill@,
%%%%    %%  it's easy.
%%%%    For example, the functions @addUsed@ and @remDefd@ in \figref{liveness}'s
%%%%    liveness analysis correspond directly to the traditional @gen@ and
%%%%    @kill@ functions.
%%%%    In the available-variables analysis of \figref{avail}, @gen@ and
%%%%    @kill@ can be defined as follows:
%%%%    \begin{smallcode}
%%%%    gen  :: UserOfLocalVars    a => a -> AvailVars -> AvailVars
%%%%    kill :: DefinerOfLocalVars a => a -> AvailVars -> AvailVars
%%%%    `gen  a avail = foldVarsUsed extendAvail  avail a
%%%%    `kill a avail = foldVarsDefd delFromAvail avail a
%%%%    \end{smallcode}
%%%%    Using @gen@ and @kill@ entails no loss of efficiency;
%%%%    for example, if the right-hand side of \lineref{reload2} of \figref{avail}
%%%%    were written
%%%%    @gen x avail@,
%%%%    then GHC~would 
%%%%    identify and % \ifpagetuning\else identify and \fi
%%%%    inline the type-specific instance of @foldVarsUsed@, which for 
%%%%    \ifpagetuning\else the case of \fi
%%%%    a local variable is the identity function, so @gen x avail@
%%%%    would reduce to @extendAvail avail x@.
%%%%    \fi

\section{Using dataflow facts to rewrite graphs\ifpagetuning\else, with examples\fi}

\seclabel{rewrites}

\seclabel{example-rewrites}


We compute dataflow facts in order to enable code-improving
transformations on control-flow graphs.
A~dataflow fact may enable a rewrite function to replace a node by a
\emph{subgraph}. 
A~subgraph is a graph that may not define all the labels to which it refers.
A~valuable, novel property of our implementation is that it uses
Haskell's static type system to control which subgraphs may replace
which nodes.
Before explaining how to
transform graphs, we
explain how graphs and subgraphs are represented.


\subsection{Representing graphs and subgraphs}

\seclabel{subgraphs}

As mentioned in \secref{graph.intro},
a~graph is a collection of basic blocks, 
and a basic block is normally a first node followed by zero or
more middle nodes followed by a last node.
But
a graph may also contain two special, incomplete
blocks:
\begin{itemize}
\item
A~graph may begin with 
an \emph{entry sequence}: zero or more middle nodes
followed by a last node (i.e., a control transfer).
Such a graph is \emph{open at the entry}.  

%% \ifpagetuning \enlargethispage{0.5\baselineskip} \fi

\item
A~graph may end with
an \emph{exit sequence}: a first node followed by zero or more
middle nodes, but \emph{not} followed by a last node.
Such a graph is \emph{open at the exit} 
(control ``falls
off the end'').
\end{itemize}
Our general type of graph, called @GF@, therefore takes \emph{four}
\label{haskell.firstuse.GF}% automated use
type parameters (\figref{subgraphs}):
@m@~is the type of a middle node;
@l@~is the type of a last node;
@entry@ is either type~@O@ or type~@C@, depending on whether the graph
\label{haskell.firstuse.entry}% automated use
\label{haskell.firstuse.O}% automated use
\label{haskell.firstuse.C}% automated use
is open or closed at the entry;
and
@exit@ is type~@O@ or type~@C@, depending on whether the graph
\label{haskell.firstuse.exit}% automated use
is open or closed at the exit.
The instantiations of type parameters @entry@ and @exit@ specify the
graph's \emph{shape}, which we refer to in shorthand.
For example,
a~full @Graph@, which represents a function or
procedure, is open at the entry and closed at the exit, or
simply ``open/closed.''

\begin{figure}
\begin{code}
type O  -- marks graph as open   at entry or exit
type C  -- marks graph as closed at entry or exit
type GF m l entry exit -- graph or subgraph
type Graph m l = GF m l O C
\end{code}

\caption{Types of graphs and subgraphs}
\figlabel{subgraphs}
\label{haskell.def.Graph}% automated definition
\label{haskell.def.exit}% automated definition
\label{haskell.def.entry}% automated definition
\label{haskell.def.GF}% automated definition
\label{haskell.def.C}% automated definition
\label{haskell.def.O}% automated definition
\end{figure}




Graphs are created using these functions:
\begin{code}
mkLabel    :: BlockId        -> GF m l C O
mkMiddle   :: m              -> GF m l O O
mkLast     :: l              -> GF m l O C
(<*>)      :: GF m l e a     -> GF m l a x 
                             -> GF m l e x
emptyGraph :: GraphClosure a => GF m l a a
\end{code}
\label{haskell.def.mkLabel}% automated definition
\label{haskell.def.mkMiddle}% automated definition
\label{haskell.def.mkLast}% automated definition
\label{haskell.def.<*>}% automated definition
\label{haskell.def.emptyGraph}% automated definition
\label{haskell.firstuse.GraphClosure}% automated use
\label{haskell.firstuse.emptyGraph}% automated use
\label{haskell.firstuse.<*>}% automated use
\label{haskell.firstuse.e}% automated use
\label{haskell.firstuse.mkLast}% automated use
\label{haskell.firstuse.mkMiddle}% automated use
\label{haskell.firstuse.mkLabel}% automated use
The infix @<*>@ function is graph concatenation; the exit of the first
argument must match the entry of the next (both open or both closed).
The @emptyGraph@ is a left and right unit of concatenation;
the constraint @GraphClosure a@ is satisfied only by types @O@~and~@C@.
\label{haskell.def.GraphClosure}% automated definition

%% If $g_1$'s shape is $e$/open and $g_2$'s shape is open/$x$,
%% then the concatenation $g_1 \mathbin{\mbox{@<*>@}} g_2$ is well
%% defined and has shape 
%% $e$/$x$.  (The exit sequence of~$g_1$ is spliced to the entry sequence
%% of~$g_2$ to form a new, complete basic block.)


%% \ifpagetuning \enlargethispage{0.5\baselineskip} \fi

A graph is normally represented by a triple:
an optional entry sequence, a @BlockEnv@ containing basic blocks,
and an optional exit sequence.
As~a special case, a single sequence of middle nodes also forms a
graph open at both entry and exit.

This new representation improves significantly on our previous work
\cite{ramsey-dias:applicative-flow-graph}:
\begin{itemize}
\item
We can find the exit point of a graph in constant time.
\item
We can concatenate data structures 
in near-constant amortized time.
Previously, we had to resort to Hughes's
\citeyearpar{hughes:lists-representation:article} technique, representing
a graph as a function.
\item
Most important, errors in concatenation are ruled out at
compile-compile time by Haskell's static
type system.
In~earlier implementations, such errors were not detected until
the compiler~ran, at which point \ourlib\ tried to compensate
for the errors%
\ifcutting---but
\else
by inserting branch instructions and then issued a warning message.
But
\fi
the compensation code harbored subtle faults%
\ifcutting\else, which were discovered while developing a new back end
for~GHC\fi. 
\hfuzz=1.6pt
\end{itemize}



%% A~graph proposed by a rewrite function as a replacement for a node is
%% a \emph{replacement graph}.
%% Every replacement graph is also a \emph{subgraph}, i.e., a graph that
%% can be analyzed or rewritten independently, but that is connected to
%% an outer graph by control-flow edges.
%% A~subgraph has the same representation as a graph, including 
%% optional entry and exit sequences.
%% \simon{I don't find the definition of replacement graph and 
%% subgraph satisfactory.  Is the ONLY difference between a graph
%% and a sub-graph its purpose in the eye of the beholder?  I think
%% perhaps the key point is this: a full graph has no free labels, 
%% whereas a subgraph may have
%% free labels?  Are subgraphs o/c or c/c or anything like that?
%% What about replacement graphs?}



\begin{figure}
\hfuzz=5.7pt
\begin{code}
type Rewrite m l e x = Maybe (GF m l e x)
data ForwardRewrites m l a = ForwardRewrites
 {fr_first  :: BlockId -> a -> Rewrite m l C O,
  fr_middle :: m       -> a -> Rewrite m l O O,
  fr_last   :: l       -> a -> Rewrite m l O C} 

data BackwardRewrites m l a = BackwardRewrites
 {br_first  :: BlockId -> a      -> Rewrite m l C O,
  br_middle :: m       -> a      -> Rewrite m l O O,
  br_last   :: l -> (BlockId->a) -> Rewrite m l O C} 
\end{code}
\caption{Types of forward and backward rewrite functions.}
\figlabel{rewrites}
\label{haskell.def.br:unlast}% automated definition
\label{haskell.def.br:unmiddle}% automated definition
\label{haskell.def.br:unfirst}% automated definition
\label{haskell.def.BackwardRewrites}% automated definition
\label{haskell.def.fr:unlast}% automated definition
\label{haskell.def.fr:unmiddle}% automated definition
\label{haskell.def.fr:unfirst}% automated definition
\label{haskell.def.ForwardRewrites}% automated definition
\label{haskell.def.Rewrite}% automated definition
\label{haskell.firstuse.Rewrite}% automated use
\label{haskell.firstuse.ForwardRewrites}% automated use
\label{haskell.firstuse.fr:unfirst}% automated use
\label{haskell.firstuse.fr:unmiddle}% automated use
\label{haskell.firstuse.fr:unlast}% automated use
\label{haskell.firstuse.BackwardRewrites}% automated use
\label{haskell.firstuse.br:unfirst}% automated use
\label{haskell.firstuse.br:unmiddle}% automated use
\label{haskell.firstuse.br:unlast}% automated use
\end{figure}



\subsection{Rewrite functions}
\seclabel{rewrite-functions}

\ourlib\ transforms its graphs by composing
 transfer functions (\secref{tffuns}) with
\emph{rewrite functions}, whose
types are
shown in \figref{rewrites}. 
%
A~rewrite function is given a dataflow fact and a node~$n$.
It~may choose to replace node~$n$ with a \emph{replacement graph}~$g$,
in which case it 
returns $@Just@\;g$, or it may do nothing, in which case it returns @Nothing@.
If it returns $@Just@\;g$, it must guarantee that given
the assertions represented by incoming dataflow facts,
graph~$g$ is observationally equivalent to node~$n$.
\finalremark{Doesn't the rewrite have to be have the following property:
for a forward analysis/transform, if (rewrite P s) = Just s',
then (transfer P s $\sqsubseteq$ transfer P s').
For backward: if (rewrite Q s) = Just s', then (transfer Q s' $\sqsubseteq$ transfer Q s).
Works for liveness.
``It works for liveness, so it must be true'' (NR).
If this is true, it's worth a QuickCheck property!
}%
\finalremark{Version 2, after further rumination.  Let's define
$\scriptstyle \mathit{rt}(f,s) = \mathit{transform}(f, \mathit{rewrite}(f,s))$.
 Then $\mathit{rt}$ should
be monotonic in~$f$.  We think this is true of liveness, but we are not sure
whether it's just a generally good idea, or whether it's actually a 
precondition for some (as yet unarticulated) property of Hoopl to hold.}%

%%  
%%  
%%  If~a rewrite function returns $@Just@\;g$, the incoming dataflow fact
%%  must guarantee that replacing the node with~$g$ does not change the
%%  observable behavior of the program.
%%  
%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixpoint. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }

A~rewrite function may replace a node only with a graph of the same shape:
\begin{itemize}
\item
\iffalse
A first node with @BlockId@~$L$ must be rewritten to a closed/open
replacement graph which also begins with @BlockId@~$L$.
\delendum{Whoa!  What does it mean to say that a graph ``begins with $L$''?
Perhaps we just mean that the replacement graph must include a block labelled $L$?}
%% Simon's right---but the details are too grotty to include. ---NR
\else
A first node must be rewritten to a closed/open
 graph.
\finalremark{Revisit graphs ``beginning with $L$''}
\fi
\item
A middle node must be rewritten to an open/open graph.
\item
A last node must be rewritten to an open/closed graph.
\end{itemize}
These conditions, which are enforced by the static type system
 (\figref{rewrites}), 
 are necessary and sufficient to ensure that 
every replacement graph can be spliced in place of the node it replaces.

\subsection{Running the dataflow engine}

\overfullrule=10pt

To write a program transformation,
you must 
\begin{itemize}
\item
Create a dataflow lattice and transfer functions for the supporting
analysis, as described in \secref{create-analysis}. 
\item
Create rewrite functions for first, middle, and last nodes.
\end{itemize}
You can then use \ourlib\ function @zdfRewriteFwd@ to transform a
\label{haskell.firstuse.zdfRewriteFwd}% automated use
control-flow graph (a~backward transformation uses function @zdfRewriteBwd@,
\label{haskell.def.zdfRewriteBwd}% automated definition
\label{haskell.firstuse.zdfRewriteBwd}% automated use
which has a similar type):\begingroup\hfuzz=1pt\relax
\begin{code}
 zdfRewriteFwd 
  :: HavingSuccessors l     -- Find successors of l
  => RewritingDepth         -- Rewrite recursively?
  -> PassName               -- Name of this pass
  -> DataflowLattice a      -- Lattice
  -> ForwardTransfers m l a -- Transfer functions
  -> ForwardRewrites  m l a -- Rewrite functions
  -> a                      -- Input fact
  -> Graph m l              -- Graph or subgraph
  -> FuelMonad (FwdFixedPoint m l a (Graph m l))
\end{code}
\label{haskell.def.zdfRewriteFwd}% automated definition
\label{haskell.firstuse.FuelMonad}% automated use
\label{haskell.firstuse.RewritingDepth}% automated use
%%  
%%  class DataflowSolverDirectiontransfers fixedpt =>
%%        DataflowDirection
%%          transfers fixedpt rewrites where
%%    zdfRewriteFwd :: (DebugNodes m l, Outputable a)
%%      => RewritingDepth    -- Recursive rewrites?
%%      -> BlockEnv a        -- Init facts
%%      -> PassName          -- Analysis name
%%      -> DataflowLattice a -- Lattice
%%      -> transfers m l a   -- Transfers
%%      -> rewrites m l a    -- Input fact
%%      -> a                 -- Input fact
%%      -> Graph m l         -- CFG
%%      -> FuelMonad (fixedpt m l a (Graph m l))
%%
%%
Function @zdfRewriteFwd@ is like @zdfSolveFwd@ in
\secref{zdfSolveFwd}, but it uses and produces extra
information:\seclabel{engine-truth} 
\endgroup
\begin{itemize}
\item
Function @zdfRewriteFwd@ requires rewrite functions as well as transfer
functions.
\item
The @RewritingDepth@ parameter controls recursive rewriting;
\label{haskell.def.RewritingDepth}% automated definition
if~a graph produced by a rewrite function should not be further rewritten,
rewriting is \emph{shallow};
\label{haskell.def.RewriteShallow}% automated definition
if~a graph produced by a rewrite function can be rewritten again,
rewriting is \emph{deep}.
\label{haskell.def.RewriteDeep}% automated definition
\ifcutting\else
Deep rewriting is essential to achieve the results of
\citet{lerner-grove-chambers:2002}, e.g., to remove the induction
variable from the loop in the example in \secref{induction-var-elim}.\john{Deep rewriting isn't needed for this example; interleaving is...}
\fi
\ifcutting\else
When deep rewriting is used, the rewrite functions must
ensure that the graphs they produce are not rewritten indefinitely.
\fi
\item
In the result type, the fourth type parameter of type constructor
@FwdFixedPoint@ is a value contained in the fixed point.
The~value is extracted using function @zdfFpContents@, which has
\label{haskell.def.zdfFpContents}% automated definition
\label{haskell.firstuse.zdfFpContents}% automated use
type @FwdFixedPoint m l a b -> b@.
Here the type parameter~@b@ is instantiated to @Graph m l@: the fixed point
contains the rewritten graph.
%%  In~@zdfSolveFwd@, @b@~is instantiated with
%%  the unit type~@()@.
%%  \simon{We can't make this point until later, where we say that solve is
%%  implemented using rewrite.  NR: I don't understand why not.
%%  We are explaining the meaning of the final type parameter to
%%  @FwdFixedPoint@, as promised.}
\item
Rewriting is monadic.
A~@FuelMonad@ holds resources needed to
\label{haskell.def.FuelMonad}% automated definition
rewrite nodes into subgraphs:
a~supply of fresh labels and a supply of \emph{optimization fuel}
(\secref{fuel}). 
\end{itemize}


\begin{figure*}\hfuzz=1pt
\begin{codetable}
\T
\begin{code}
availRewrites :: ForwardRewrites CmmMiddle CmmLast AvailVars
availRewrites = ForwardRewrites first middle last
!avail.rewrites.first!  where first _ _ = Nothing
        middle m avail = maybe_reload_before avail m (mkMiddle m)
!avail.rewrites.last!        last   l avail = maybe_reload_before avail l (mkLast l)
!maybe.reload.before.1!        maybe_reload_before avail node tail =
            let used = filterVarsUsed (elemAvail avail) node
            in  if isEmptyVarSet used then Nothing
!maybe.reload.before.2!                else Just $ reloadTail used tail
        reloadTail vars t = foldl rel t $ varSetToList vars
!mkMiddle!            where rel t r = mkMiddle (reload r) <*> t
\end{code}
\B
& Rewrite \mbox{functions}\\
\hline

\T\hfuzz=21pt
\begin{code}
insertLateReloads :: Graph CmmMiddle CmmLast -> FuelMonad (Graph CmmMiddle CmmLast)
insertLateReloads g = liftM zdfFpContents fp
!insertLateReloads.1!  where fp = zdfRewriteFwd RewriteShallow "insert late reloads" availVarsLattice
!insertLateReloads.2!               availTransfers availRewrites (fact_bot availVarsLattice) g
\end{code}
& Late-reload insertion\\
\end{codetable}
\caption{Late-reload insertion, which relies on the analysis of \figref{avail}}
\figlabel{avail-rewrites}
\label{haskell.def.insertLateReloads}% automated definition
\label{haskell.def.varSetToList}% automated definition
\label{haskell.def.rel}% automated definition
\label{haskell.def.vars}% automated definition
\label{haskell.def.reloadTail}% automated definition
\label{haskell.def.used}% automated definition
\label{haskell.def.node}% automated definition
\label{haskell.def.maybe:unreload:unbefore}% automated definition
\label{haskell.def.middle}% automated definition
\label{haskell.def.first}% automated definition
\label{haskell.def.availRewrites}% automated definition
\label{haskell.firstuse.insertLateReloads}% automated use
\label{haskell.firstuse.liftM}% automated use
\label{haskell.firstuse.RewriteShallow}% automated use
\label{haskell.firstuse.availRewrites}% automated use
\label{haskell.firstuse.first}% automated use
\label{haskell.firstuse.middle}% automated use
\label{haskell.firstuse.maybe:unreload:unbefore}% automated use
\label{haskell.firstuse.node}% automated use
\label{haskell.firstuse.tail}% automated use
\label{haskell.firstuse.used}% automated use
\label{haskell.firstuse.filterVarsUsed}% automated use
\label{haskell.firstuse.isEmptyVarSet}% automated use
\label{haskell.firstuse.reloadTail}% automated use
\label{haskell.firstuse.vars}% automated use
\label{haskell.firstuse.foldl}% automated use
\label{haskell.firstuse.rel}% automated use
\label{haskell.firstuse.varSetToList}% automated use
\label{haskell.firstuse.r}% automated use
\label{haskell.firstuse.reload}% automated use
\end{figure*}


\seclabel{dfengine-spec}

Function
@zdfRewriteFwd@ implements
interleaved analysis and transformation 
in two phases \citep{lerner-grove-chambers:2002}:\seclabel{solver-phase}
\begin{itemize}
\item
In the first phase, when a rewrite function proposes to replace a
node~$n$, the replacement graph is analyzed recursively, and the results
of that analysis are used as the new dataflow
fact(s) flowing out of node~$n$.
Then the replacement
graph is \emph{thrown away}; only the facts remain.
(In~other words, rewriting is \emph{speculative}.)
If,~on a later iteration, node~$n$ is analyzed again, perhaps
with a different input fact, the rewrite function may propose
a different replacement or even no replacement at~all.
%%As described in \secref{subgraphs}, every replacement graph is a
%%\emph{subgraph}, not a complete graph.

The first phase is called the \emph{iterator}.
It computes a fixed point of the dataflow analysis
\emph{as if} nodes were replaced, while never actually replacing a node.
%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixpoint. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }
\item
When the iterator finishes, the resulting fixed point is sound,
and the facts in the fixed point are used by the second phase, in~which
no dataflow facts change, but rewrites are not speculative:
each replacement proposed by a rewrite function is actually
performed.
This phase is therefore called the \emph{actualizer}.
\end{itemize}

%The iterator executes a complicated algorithm with complicated control-flow
%that depends on the lattice, transfer functions, and rewrite functions;
Facts computed by the iterator depend on graphs produced by rewrite
functions, which in turn depend on facts computed by the iterator.
How~do we know this algorithm is sound, or even if it terminates?
A~proof requires its own POPL paper
\cite{lerner-grove-chambers:2002}, but we can give some
intuition:
\begin{itemize} 
\item
The algorithm is sound because, given the incoming dataflow facts,
each rewrite must preserve the observable behavior of the program.
A~sound analysis of the rewritten graph
may generate only dataflow facts that could have been
generated by a more complicated analysis of the original graph.
\item
No matter what the transfer functions and rewrite functions do,
the dataflow engine uses the dataflow lattice's join operation to ensure that
facts at labels never decrease. 
As~long as
\ifcutting
 no fact may
\else
 the lattice permits no fact to 
\fi
increase infinitely many
times, analysis
\ifcutting
 terminates.
\else
 is guaranteed to terminate.
\fi
\end{itemize}
Thus to guarantee soundness and termination, client code must supply 
sound transfer functions,
sound rewrite functions,
and
a
lattice with no infinite ascending chains.
And unless client code specifies shallow rewriting,
\ifcutting
\else which tells the
dataflow engine never to rewrite a replacement graph,
\fi
rewrite functions must not return replacement
graphs which contain nodes that could be rewritten indefinitely.


Why use such a complex algorithm?
\ifcutting Because interleaving \else
\citet{lerner-grove-chambers:2002} write
\begin{quote}
\emph{Previous efforts to exploit [the mutually beneficial
interactions of dataflow analyses] either (1)~iteratively performed
each individual analysis until no further improvements are discovered
or (2)~developed [handwritten] ``super-analyses'' that manually
combine conceptually separate analyses. We have devised a new approach
that allows analyses to be defined independently while still enabling
them to be combined automatically and profitably. Our approach avoids
the loss of precision associated with iterating individual analyses
and the implementation difficulties of manually writing a
super-analysis.}
\end{quote}



%%  \simon{But do we apply rewrites even before the analysis reaches a fixed point?
%%  If so, what property do the rewrites have to satisfy to ensure soundness?
%%  If not, even a single rewrite might destroy the fixed-point property of the
%%  current facts.  Or perhaps we iterate the analysis to a fixpoint, and only \emph{then}
%%  do rewriting? If so, do we need the transfer functions at that stage?
%%  
%%  Also the fixed-point of the analysis relies on upward chains. What if
%%  the rewrite pushed it downward?  Or is it the case that a rewrite must
%%  change a node $n$ into a graph $g$ so 
%%  that $\mathit{fwdtrans}(n) \leq \mathit{fwdtrans}(g)$?
%%  
%%  Also the fixpoint calculation requires multiple passses; do the 
%%  rewrites then apply multiple times?
%%  
%%  I'm deliberately playing the role of the reader here, and not peeking at
%%  the code.  I don't think it's enough to say ``go look at Chambers paper''; 
%%  I suggest we say enough (half a column would do it) to address the obvious
%%  questions and point to Chambers for details.
%%  
%%  
%%  \textbf{NR}: Good questions, but let's have a forward reference to \secref{dfengine}}

Interleaving
\fi
 analysis with transformation makes it
possible to implement useful  transformations using startlingly simple
client code.
In the rest of this section we present two examples:
\secref{sink-reloads} shows how to insert a reload instruction just
before each use of each spilled variable, and
\secref{dead-code-elim} shows how to eliminate dead assignments.
When these two transformations are run in sequence, the effect is to
sink reloads and produce programs like the example shown in
\secref{spill-reload-example}. 





%%  After defining the lattice, the transfer functions, and the rewrite functions,
%%  the client runs the analysis by invoking the dataflow framework
%%  (see~\figref{framework-fns}).
%%  The function @zdfSolveFrom@ performs an analysis on an input control-flow graph,
%%  using a dataflow lattice and a set of transfer functions.
%%  The additional arguments to the function provide
%%  the name of the analysis,
%%  the initial set of dataflow facts (usually empty),
%%  and the initial fact (usually bottom)
%%  that flows into either the entry or exit of the graph,
%%  depending on whether the transfers define a forward or backward analysis.
%%  The result of the function is the fixed point of the analysis,
%%  which stores the dataflow fact on entry to each basic block.
%%  \john{Maybe we should export a simple version of these functions to clients?
%%    Do they always do the obvious things with the initial facts and in-fact?
%%    Initial facts can reuse results of a previous analysis, but then you lose
%%    interleaving.}
%%  
%%  To combine an analysis and a transformation,
%%  the client calls the @zdfRewriteFrom@ function,
%%  which takes the same arguments as @zdfSolveFrom@,
%%  with the addition of the set of rewrite functions
%%  and a parameter (@RewritingDepth@) that decides whether the result
%%  of a rewrite function should be considered for further rewriting.
%%  The result of the function is not only the fixed point of the
%%  analysis interleaved with the transformation,
%%  but also the transformed control-flow graph.
%%  



\begin{figure*}\hfuzz=1pt
\begin{codetable}\hfuzz=21pt
\T
\begin{code}
deadRewrites = BackwardRewrites nothing middleRemoveDeads nothing
!deadRewrites.1!  where nothing _ _ = Nothing
        middleRemoveDeads :: CmmMiddle -> VarSet -> Maybe (Graph CmmMiddle CmmLast)
!elim.dead.1!        middleRemoveDeads (MidAssign (CmmLocal x) _) live
!elim.dead.2!            | not (x `elemVarSet` live) = Just emptyGraph
!deadRewrites.2!        middleRemoveDeads _ _ = Nothing
\end{code}
\B
& Rewrite \mbox{functions}\\
\hline

\T\hfuzz=40pt
\begin{code}
removeDeadAssignments :: Graph CmmMiddle CmmLast -> FuelMonad (Graph CmmMiddle CmmLast)
removeDeadAssignments g = liftM zdfFpContents fp
!rewriteBwd.1!     where fp = zdfRewriteBwd RewriteDeep "dead-assignment elim" liveLattice
!rewriteBwd.2!                  liveTransfers deadRewrites emptyVarSet g
\end{code}
& \vspace*{12pt}\mbox{Dead-code} elimination\\
\end{codetable}
\caption{Dead-assignment elimination, which relies on the analysis of
\figref{liveness}} 
\figlabel{dead-elim}
\label{haskell.def.removeDeadAssignments}% automated definition
\label{haskell.def.middleRemoveDeads}% automated definition
\label{haskell.def.nothing}% automated definition
\label{haskell.def.deadRewrites}% automated definition
\label{haskell.firstuse.removeDeadAssignments}% automated use
\label{haskell.firstuse.RewriteDeep}% automated use
\label{haskell.firstuse.deadRewrites}% automated use
\label{haskell.firstuse.nothing}% automated use
\label{haskell.firstuse.middleRemoveDeads}% automated use
\label{haskell.firstuse.not}% automated use
\label{haskell.firstuse.elemVarSet}% automated use
\end{figure*}


\subsection{Sinking reloads: a forward transformation}

\finalremark{Incidentally, I wonder if we should
use record notation when constructing @ForwardRewrites@?}

\seclabel{sink-reloads}

We use the available-variables analysis of \secref{avail} to
insert reloads
immediately before uses of variables.
The transformation is implemented by the rewrite functions on
\linerangeref{avail.rewrites.first}{avail.rewrites.last} of \figref{avail-rewrites}.
A~first node uses no variables and so is never rewritten.
For middle and last nodes, @maybe_reload_before@ 
(\linerangeref{maybe.reload.before.1}{maybe.reload.before.2})
computes @used@, which is the set
of variables used in the node that are both safe and profitable to
reload. 
%
\label{haskell.def.filterVarsUsed}% automated definition
%
If that set is not empty, function
@reloadTail@ replaces @node@ with a new graph in which @node@ is
preceded by a (redundant) reload for each variable in the set~@used@.
A~reload node is created by function @reload@ (\lineref{mkMiddle}),
\label{haskell.def.reload}% automated definition
which has type @LocalVar -> CmmMiddle@.
%%%%% This rewrite function \emph{must} be used with shallow
%%%%% rewriting. % redundant

Our transformation is implemented by the call to @zdfRewriteFwd@
on \linerangeref{insertLateReloads.1}{insertLateReloads.2} of \figref{avail-rewrites}.
Rewriting is shallow, so a graph containing reload nodes
is not itself rewritten.
(If~it \emph{were} rewritten, a nonempty @used@ set would make the
compiler insert an infinite sequence of reloads before @node@.)
Once the reloads are inserted, the original reloads
\ifcutting\else immediately following the call site
\fi
are dead, and they can be eliminated by our
next transformation, dead-assignment elimination.

\subsection{Dead-assignment elimination: a backward \rlap{transformation}}


\seclabel{dead-code-elimination}
\seclabel{dead-code-elim}

\seclabel{bwd-rewrite}


\def\liveout{$\mathit{live_{out}}$}

We use the liveness analysis of \secref{liveness} to identify
assignments
to local variables that 
are not live.
Such \emph{dead assignments} can be removed without changing the
observable behavior of the program.
The removal is implemented by the rewrite functions on
\linerangeref{deadRewrites.1}{deadRewrites.2} of \figref{dead-elim}. 
First and last nodes are not assignments and so are never
rewritten.
A~middle node is rewritten to the empty graph if and only if it is an
assignment to a dead variable (\linerangeref{elim.dead.1}{elim.dead.2}).
On \linepairref{rewriteBwd.1}{rewriteBwd.2}, we call @zdfRewriteBwd@.
That's the whole thing.\finalremark
{JD: Need to run this version of the code in anger.}
%
\finalremark{In this space we should have some guff about
composing transformations, which should refer to the example on
eliminating the induction variable.
More generally, list some places dead-assignment elim is used and
include \secref{induction-var-elim}.
}




\begin{figure*}
\def\numberedcodebackspace{0.7\baselineskip}
\setcounter{codeline}{0}
\begin{numberedcode}
!FactKont!type FactKont a b = a          -> DFM a b
type LOFsKont a b = LastOuts a -> DFM a b
!Kont!type Kont     a b =               DFM a b

!forward.sol.sig!fwd_iter :: forall m l e x a . HavingSuccessors l => (forall b . Maybe b -> DFM a (Maybe b))
         -> RewritingDepth -> PassName -> BlockEnv a -> ForwardTransfers m l a
!forward.sol.zmaybe!         -> ForwardRewrites m l a -> ZMaybe e a -> GF m l e x -> DFM a (ZMaybe x a)
!forward.sol.args!fwd_iter with_fuel depth name start_facts transfers rewrites in_fact g =
!forward.sol.setAllFacts!     do { setAllFacts start_facts ; iter_ex g in_fact }
!solve.ex.sig!   where iter_ex    :: GF m l e x -> ZMaybe e a -> DFM a (ZMaybe x a)

!solve.first.sig!         iter_first :: BlockId             -> FactKont a b -> Kont     a b
!solve.mid.sig!         iter_mid   :: m                   -> FactKont a b -> FactKont a b
!solve.last.sig!         iter_last  :: l                   -> LOFsKont a b -> FactKont a b

!solve.block.sig!         iter_block :: BlockId -> [m] -> l -> LOFsKont a b -> Kont     a b
!solve.block.code!         iter_block f ms l = iter_first f . flip (foldr iter_mid) ms . iter_last l

!set.last.sig!         set_last :: LOFsKont a ()
!set.last.*!         set_last (LastOuts l) = mapM_ (uncurry setFact) l

!solve.mid.1!         iter_mid m k in' =
!solve.mid.case!           (with_fuel $ fr_middle rewrites in' m) >>= \x -> case x of
!solve.mid.Nothing!             Nothing -> k (ft_middle_out transfers in' m)
!solve.mid.Just!             Just g  -> do { a <- subAnalysis $ case depth of
!solve.OO!                                                  RewriteDeep    -> iter_OO g return in'
!solve.rewrite.shallow.1!                                                  RewriteShallow -> anal_f_OO g in'
!solve.mid.*!                           ; k a }
\end{numberedcode}
\caption{Excerpts from the forward iterator}
\figlabel{iterator-excerpts}
% !solve.block!     iter_block (Block id tail) = iter_first id $ iter_tail tail $ set_last
% omitted, much as I would have liked to include it...
% !solve.OO.def!         iter_OO    :: GF m l O O -> FactKont a b -> FactKont a b

\label{haskell.def.in'}% automated definition
\label{haskell.def.iter:unmid}% automated definition
\label{haskell.def.set:unlast}% automated definition
\label{haskell.def.ms}% automated definition
\label{haskell.def.iter:unblock}% automated definition
\label{haskell.def.iter:unlast}% automated definition
\label{haskell.def.iter:unfirst}% automated definition
\label{haskell.def.iter:unex}% automated definition
\label{haskell.def.in:unfact}% automated definition
\label{haskell.def.rewrites}% automated definition
\label{haskell.def.transfers}% automated definition
\label{haskell.def.start:unfacts}% automated definition
\label{haskell.def.name}% automated definition
\label{haskell.def.depth}% automated definition
\label{haskell.def.fwd:uniter}% automated definition
\label{haskell.def.Kont}% automated definition
\label{haskell.def.LOFsKont}% automated definition
\label{haskell.def.FactKont}% automated definition
\label{haskell.firstuse.FactKont}% automated use
\label{haskell.firstuse.DFM}% automated use
\label{haskell.firstuse.LOFsKont}% automated use
\label{haskell.firstuse.Kont}% automated use
\label{haskell.firstuse.fwd:uniter}% automated use
\label{haskell.firstuse.ZMaybe}% automated use
\label{haskell.firstuse.with:unfuel}% automated use
\label{haskell.firstuse.depth}% automated use
\label{haskell.firstuse.name}% automated use
\label{haskell.firstuse.start:unfacts}% automated use
\label{haskell.firstuse.transfers}% automated use
\label{haskell.firstuse.rewrites}% automated use
\label{haskell.firstuse.in:unfact}% automated use
\label{haskell.firstuse.setAllFacts}% automated use
\label{haskell.firstuse.iter:unex}% automated use
\label{haskell.firstuse.iter:unfirst}% automated use
\label{haskell.firstuse.iter:unmid}% automated use
\label{haskell.firstuse.iter:unlast}% automated use
\label{haskell.firstuse.iter:unblock}% automated use
\label{haskell.firstuse.ms}% automated use
\label{haskell.firstuse.foldr}% automated use
\label{haskell.firstuse.set:unlast}% automated use
\label{haskell.firstuse.mapM:un}% automated use
\label{haskell.firstuse.uncurry}% automated use
\label{haskell.firstuse.setFact}% automated use
\label{haskell.firstuse.in'}% automated use
\label{haskell.firstuse.>>=}% automated use
\label{haskell.firstuse.subAnalysis}% automated use
\label{haskell.firstuse.iter:unOO}% automated use
\label{haskell.firstuse.anal:unf:unOO}% automated use
\end{figure*}





\section{\ourlib's dataflow engine}
\seclabel{engine}
\seclabel{dfengine}




\delendum{The earlier sections promised that we'd reveal the lies.
Do we?  I see no mention of @HavingSuccessors@ for example, which is rather important
for polymorphism.  Indeed, a subsection on that point might be a good way
to substantiate the claims of the last bullet of the conclusion.}

In sections \ref{sec:making-simple}
through~\ref{sec:rewrites},
we use \ourlib\ to create analyses and transformations.
Here we sketch the implementation of the main part of \ourlib:
the dataflow engine.
While a full description of the implementation is beyond the scope of
this paper, a sketch 
demonstrates the new ideas that make this implementation simpler
than the original:
using pure functional code throughout;
using an explicit state monad to manage the computation of fixed
points;
giving each type of graph node its own analysis function, 
which also performs speculative rewriting;
and
using continuation-passing style to stitch these functions together.
We~sketch the implementation from the bottom~up:
\ourlib's fuel monad,
the monad that holds dataflow facts,
an~iterator,
and
an~actualizer.



%The dataflow engine comprises four functions:
%a forward iterator, a forward actualizer,
%a backward iterator, and a backward actualizer.


\subsection{Throttling the dataflow engine using ``optimization fuel''}

\seclabel{vpoiso}
\seclabel{fuel}

We have extended Lerner, Grove, and Chambers's optimization-combining algorithm with
Whalley's \citeyearpar{whalley:isolation} algorithm for isolating
faults.
Whalley's algorithm is used to test a faulty optimizer;
it~automatically
finds the first rewrite that introduces a fault in a test program.
It works by giving the optimizer a finite supply of \emph{optimization fuel}.
Each time a rewrite function proposes to replace a node, one unit of fuel is
consumed.
When the optimizer runs out of fuel, further rewrites are suppressed.
Because each rewrite leaves the observable behavior of the
program unchanged, it is safe to suppress rewrites at
any point.
In~normal operation, the optimizer has unlimited fuel, but during
debugging, a fault can be isolated quickly by doing a binary search on
the size of the fuel supply.
% To control the fuel supply in a purely functional setting, we use
% the fuel monad.
%% , which works with
%% computations of type @Fuel -> (a, Fuel)@.
The fuel supply is stored in a state monad (@FuelMonad@), which
also holds a supply of fresh labels.
Fresh labels are used for making new
blocks.


\subsection{A monad for dataflow effects}

\seclabel{dataflow-monad}

In addition to the fuel supply,
each analysis and transformation keeps
track of the values of dataflow
facts; these facts, stored in an \emph{environment}, are managed by
 a \emph{dataflow monad}.
A value in the dataflow monad has type @DFM a b@, where @a@~is the type of a
\label{haskell.def.DFM}% automated definition
dataflow fact and @b@ is the type of the value returned by the monadic action.
And to track fuel, a dataflow monad is also a fuel monad.

%%  In addition to the fuel supply,
%%  each analysis and transformation 
%%  uses an \emph{environment} which
%%  keeps
%%  track of the values of dataflow
%%  facts; the {environment}
%%  is managed by a \emph{dataflow monad}.
%%  A value in the dataflow monad has type @`DFM a b@, where @a@~is the type of a
%%  dataflow fact and @b@ is the type of the value returned by the monadic action.
%%  The state of a dataflow monad
%%  \ifcutting
%%  also includes supplies of fuel and labels.
%%  \else
%%  includes the shared state of the
%%  fuel monad.
%%  \fi

Operations on the dataflow monad include{\hfuzz=10.5pt
\begin{code}
getFact           :: BlockId ->      DFM a a
setFact           :: BlockId -> a -> DFM a ()
getAllFacts       :: DFM a (BlockEnv a)
setAllFacts       :: BlockEnv a -> DFM a ()
useOneFuel        :: DFM a ()
fuelExhausted     :: DFM a Bool
subAnalysis       :: DFM a b -> DFM a b
withDuplicateFuel :: DFM a b -> DFM a b
runDFM :: DataflowLattice a -> DFM a b -> FuelMonad b
\end{code}
\label{haskell.def.getFact}% automated definition
\label{haskell.def.setFact}% automated definition
\label{haskell.def.getAllFacts}% automated definition
\label{haskell.def.setAllFacts}% automated definition
\label{haskell.def.useOneFuel}% automated definition
\label{haskell.def.fuelExhausted}% automated definition
\label{haskell.firstuse.runDFM}% automated use
\label{haskell.firstuse.withDuplicateFuel}% automated use
\label{haskell.firstuse.fuelExhausted}% automated use
\label{haskell.firstuse.useOneFuel}% automated use
\label{haskell.firstuse.getAllFacts}% automated use
\label{haskell.firstuse.getFact}% automated use
A~computation} in the dataflow monad has two significant side effects:
it may \emph{increase stored facts} (according to a lattice ordering)
and it may \emph{consume fuel}.
The~two most interesting operations in the monad are used to control
those effects:
\begin{itemize}
\item
Computation @subAnalysis c@ computes the same results as~@c@ and
\label{haskell.def.subAnalysis}% automated definition
consumes the same fuel as~@c@, but it does not change any stored
dataflow facts.
\item
Computation @withDuplicateFuel c@ computes the same results as~@c@ and
\label{haskell.def.withDuplicateFuel}% automated definition
changes the same stored facts as~@c@, but it consumes fuel from a
\emph{copy} of the fuel supply.
The inner computation~@c@ may run out of fuel, but afterward,
@withDuplicateFuel@ restores the original fuel supply.
Using @withDuplicateFuel@ has enabled us to eliminate fuel from
arguments and results, making an implementation which
is less error-prone and \emph{much} easier to
read than the one by \citet{ramsey-dias:applicative-flow-graph}. 
\end{itemize}
%%  Using pure code and a monad makes this version much easier to get
%%  right than our Objective Caml version
%%  \cite{ramsey-dias:applicative-flow-graph}. 
%%  \john{Presumably this sentence disappears if the italic text is made permanent.}
Function @runDFM@ runs a single analysis or transformation, then
\label{haskell.def.runDFM}% automated definition
abandons the dataflow facts and returns the result in the fuel monad.
Only @FuelMonad@ is exposed to the client;
the dataflow monad is private to \ourlib.
Using the dataflow monad, \ourlib's iterators and actualizers
are significantly simpler than those in our
previous work \cite{ramsey-dias:applicative-flow-graph}.
In~\secreftwo{forward-iterator}{forward-actualizer},
we show parts of the forward iterator and actualizer. 


%%  Note that the dataflow engine is the only part of the system that is
%%  hard to get right---this is where all the hair is.
%%  Prime benefit of our system is that once this is right, everything is
%%  easy (and indeed is just logic, strongest postcondition, or weakest
%%  precondition). 
%%  


\subsection{The forward iterator}

\seclabel{forward-iterator}

An \emph{iterator} does dataflow analysis with speculative rewriting.
Analysis begins an dataflow monad whose
environment maps all labels to bottom facts.
For each block in the control-flow graph, the iterator begins with the
dataflow facts flowing into one end of the block 
(in a forward analysis, the first node; in a
backward analysis, the last node),
then uses the transfer functions and rewrite functions to compute the
dataflow facts flowing 
out the other end of the block.
The outflowing facts are joined with the facts previously stored in the
environment, and when the facts in the environment stop changing, the
iterator terminates. 

The iterator interleaves analysis and speculative rewriting
     \cite{lerner-grove-chambers:2002}. 
At a node~$n$, the iterator passes~$n$ and
any incoming dataflow facts~\fs\ to a rewriting function.
If node~$n$ is rewritten to a graph~$g$, 
the iterator continues with the same dataflow facts~\fs\
     flowing into graph~$g$.
     After graph~$g$ is analyzed,
     it is discarded;
%, and the original node~$n$ is restored;
     only the facts flowing out of~$g$ persist.


\finalremark{What does the reader gain from here on?}

\delendum{I keep tripping over a nasty misunderstanding here.
We say that ``the dataflow engine implements only composed
analysis and transformation'', but then we provide (a)
a solver, and (b) a actualizer.  Which apparently contradicts.

NR: Apparently so.  Maybe you can think of better names.
Here's the story: 
\begin{itemize}
\item
The iterator implements composed analysis and
transformation.  The output is a set of facts and a graph.
The iterator keeps the facts and discards the graph.  (And to save
allocations, it never builds the graph in the first place.)
The iterator is both iterative and recursive.
\item
The actualizer calls the iterator, then in a \emph{single} pass, rewrites
the graph.  The actualizer is not iterative, but it is recursive.
\end{itemize}
Ideas for a better way to tell this story?

SLPJ: 
I believe that the story is that the iterator needs to perform rewriting
to get the right answer; it just doesn't retain the rewritten graph.
Well, that's ok, but it's quite confusing.  Moreover, a simple (but
perhaps less efficient) way to write the iterator would be to call the
actualizer, and simply discard the returned graph.  Correct?

NR: \emph{Incorrect}.  The actualizer calls the iterator to do most of the work.
%So {\tt fwd\_iter} is simply an efficiency hack on {\tt forward\_rew}.
%If that is so, perhaps we should simply present the forward actualizer,
%thereby avoiding the confusion altogether.  Admittedly the code is slightly
%more complicated, but not much.
}


\begin{figure*}
%%  forward_rew
%%          :: forall m l a . 
%%             (DebugNodes m l, HavingSuccessors l, Outputable a)
%%          => (forall a . Fuel -> Maybe a -> Maybe a)
%%          -> RewritingDepth
%%          -> BlockEnv a
%%          -> PassName
%%          -> ForwardTransfers m l a
%%          -> ForwardRewrites m l a
%%          -> a
%%          -> Graph m l
%%          -> Fuel
%%          -> DFM a (FwdFixedPoint m l a (Graph m l), Fuel)
%%  forward_rew squash depth xstart_facts name transfers rewrites in_factx gx fuelx = fixed_pt_and_fuel
%%    where
%%      fixed_pt_and_fuel =
%%          do { (a, g, fuel) <- rewrite xstart_facts getExitFact in_factx gx fuelx
%%             ; facts <- getAllFacts
%%             ; let fp = ... facts ... g ...
%%             ; return (fp, fuel)
%%             }
%%  
%%  
%%      rewrite_blocks (Block id t : bs) rewritten fuel =
%%        ... ar_tail h (ft_first_out transfers id a) t rewritten fuel ...
%%      rewrite_blocks [] rewritten fuel = return (rewritten, fuel)
\setcounter{codeline}{0}
\def\numberedcodebackspace{0.7\baselineskip}
\begin{numberedcode}
!GraphFactKont!type GraphFactKont  m l e x a b = GF m l e x -> a -> DFM a b
!GraphKont!type GraphKont      m l e x a b = GF m l e x      -> DFM a b

!rew.first!      ar_first :: BlockId -> GraphFactKont m l e O a b -> GraphKont     m l e C a b
      ar_mid   :: m       -> GraphFactKont m l e O a b -> GraphFactKont m l e O a b
!rew.last!      ar_last  :: l       -> GraphKont     m l e C a b -> GraphFactKont m l e O a b

!rew.mid.1!      ar_mid m k head in' =
        (with_fuel $ fr_middle rewrites in' m) >>= \x -> case x of
          Nothing -> k (head <*> mkMiddle m) (ft_middle_out transfers in' m)
!rew.subAnalysis!          Just g  -> do { (g, a) <- subAnalysis $
                             case depth of
                               RewriteDeep    -> iar_OO g (curry return) in'
!rew.anal.f.OO!                               RewriteShallow -> do { a <- anal_f_OO g in'; return (g, a) }
!rew.mid.*!                        ; k (head <*> g) a }

!iar.OO!      iar_OO :: GF m l O O -> GraphFactKont m l O O a b -> FactKont a b
\end{numberedcode}
\caption{Excerpts from the forward actualizer}
\figlabel{actualizer-excerpts}
\label{haskell.def.iar:unOO}% automated definition
\label{haskell.def.ar:unmid}% automated definition
\label{haskell.def.ar:unlast}% automated definition
\label{haskell.def.ar:unfirst}% automated definition
\label{haskell.def.GraphKont}% automated definition
\label{haskell.def.GraphFactKont}% automated definition
\label{haskell.firstuse.GraphFactKont}% automated use
\label{haskell.firstuse.GraphKont}% automated use
\label{haskell.firstuse.ar:unfirst}% automated use
\label{haskell.firstuse.ar:unmid}% automated use
\label{haskell.firstuse.ar:unlast}% automated use
\label{haskell.firstuse.head}% automated use
\label{haskell.firstuse.iar:unOO}% automated use
\label{haskell.firstuse.curry}% automated use
\end{figure*}



\finalremark{How and where do we say what's new over
\citet{ramsey-dias:applicative-flow-graph}?}
%showed a backward iterator and actualizer that kept dataflow facts in
%mutable cells.
%\emph{[New stuff is pure, polymorphic, and shows how to use
%    fuel]}\remark{fix me}


\figref{iterator-excerpts} shows excerpts from the forward iterator 
@fwd_iter@.
\begin{itemize}
\item
The @with_fuel@ parameter 
\label{haskell.def.with:unfuel}% automated definition
is called on the result of each rewriting function (e.g. \lineref{solve.mid.case}).
It consumes fuel; or if no fuel is available,
it prevents any nodes from being rewritten.
\item
Analysis of a subgraph starts with known facts, not
bottom facts; they are passed as
@start_facts@
% (\lineref{forward.sol.args}) 
and
 set on \lineref{forward.sol.setAllFacts}.
\item
A~forward analysis requires an entry fact @in_fact@ if and only if the
graph being analyzed is open at the entry.
Similarly, the analysis produces an output fact if and only if the
graph being analyzed is open at the exit.
We~express these constraints using the generalized algebraic data type
@ZMaybe@ (\figref{iterator-excerpts}, \lineref{forward.sol.zmaybe}):
\begin{code}
  data ZMaybe ex a where
    ZJust    :: a -> ZMaybe O a
    ZNothing ::      ZMaybe C a
\end{code}
\label{haskell.def.ZMaybe}% automated definition
\label{haskell.def.ex}% automated definition
\label{haskell.def.ZJust}% automated definition
\label{haskell.def.ZNothing}% automated definition
\label{haskell.firstuse.ZNothing}% automated use
\label{haskell.firstuse.ZJust}% automated use
\label{haskell.firstuse.ex}% automated use
Using @ZMaybe@ to construct the types of the input and output facts
has simplified our implementation of the dataflow engine and has
eliminated dynamic tests of the shapes of subgraphs.
\end{itemize}
%A~fixed point is computed by initializing the facts using
%@setAllFacts@, which is an operation in the dataflow monad.
%
%\iffalse
%Function @solve@, on \linerangeref{solve.1}{solve.*} of
%\figref{iterator-excerpts}, 
%takes an input fact, a graph, and a fuel supply; it~returns a pair
%containing the exit fact and the 
%remaining fuel supply.
%It~also has a side effect on the state stored in the inner dataflow monad:
%it brings the facts associated with labels up to a fixed point.
%\fi


\vfilbreak[3\baselineskip]

%
\delendum{Hang on!  What kind of graph does {\tt zdfSolveFwd} take?
I assume a full graph, closed at the entry!  So what is this pesky {\tt in\_fact}??}
%

% JD didn't see the point:
% Function @zdfSolveFwd@ calls @fwd_iter@:
% \begin{smallcode}
% zdfSolveFwd name ^lattice ^transfers in_fact g = 
%   runWithoutFuel $ runDFM lattice $ ffp () $ 
%   fwd_pure_anal name emptyBlockEnv transfers in_fact g
% fwd_pure_anal name env transfers in_fact g =
%   fwd_iter (\_ _ -> Nothing) undefined name env 
%               transfers undefined in_fact g
% ffp :: b -> DFM a (ZMaybe x a) 
%     -> DFM a (FwdFixedPoint m l a b)
% runWithoutFuel :: FuelMonad a -> a
% \end{smallcode}
% Funtion @`fwd_pure_anal@ is the special case of pure analysis; it is
% also used to implement function @`anal_f_OO@ on
% \lineref{solve.rewrite.shallow.1} of \figref{iterator-excerpts}.
% Function~@`ffp@ (not shown) extracts a @FwdFixedPoint@ from the
% state stored in the dataflow monad.
% Function @`runWithoutFuel@ (not shown) exploits lazy evaluation to run
% a computation in the fuel monad while guaranteeing that the
% computation uses no fuel and no labels.
% \john{The previous paragraph was a big interruption in my reading.
% Is it serving a Higher Purpose,
% or can we put this code at the bottom of Figure 10
% and give a briefer description inline? It seems to me that the only salient points
% are that zdfSolveFwd calls forwardsol with empty rewrites,
% then extracts the results.}


The function @iter_ex@ (type on \lineref{solve.ex.sig}, implementation not
shown), solves a graph or subgraph~@g@.
Where the graph is open, @iter_ex@ converts @ZMaybe@ facts to actual
facts---the static type system precludes the possibility of a missing
or superfluous fact.

The iterator is composed of functions written in continuation-passing style:
the result of analyzing part of a graph is a function from
continuations to continuations. 
The types of the
continuations are shown on \linerangeref{FactKont}{Kont} of
\figref{iterator-excerpts}. 
\begin{itemize}
\item
Type @FactKont a b@ describes a context following a first node or middle
node: in a forward analysis, the context expects a fact of type~@a@
to flow out of the node.
The rest of the analysis consumes that
fact and produces a computation in the
dataflow monad (@DFM a@) with an answer of type~@b@.
%Type @FactKont a b@ also describes the context following the analysis of
%any replacement graph that is open at the exit.
\item
Type @LOFsKont a b@ describes a context following a last node.
The type is dictated by the type of the transfer function
@ft_last_outs@ in \figref{transfers}:
since as many facts flow out of a last node as there are control-flow
edges leaving that node, the context expects those facts to have type
@LastOuts a@.
\item
Type @Kont a b@ describes a context \emph{before} a first node (or a
basic block).
The dataflow fact flowing into the note is not passed as a parameter;
it is extracted from the dataflow monad's environment by calling
the monadic operation @getFact@.
\ifcutting\else
Thus, a @Kont@ expects no fact as argument.
\fi
\end{itemize}
\delendum{I like these continuations, but I'm very puzzled about the
fuel.  If @Kont@ takes fuel in, where does it return the depleted fuel?
It must come out eventually, because it's needed in the rest of the program.
And if it always comes out, then we should say so:
@Kont a b = DFM a b@.
And once you do that, it's plain that @Kont@ is just a state monad,
and I can't see why it isn't part of @DFM@ in the first place.}

Declarations of 
\ifpagetuning
continuation-passing
\fi
iterator functions for nodes are shown on
\linerangeref{solve.first.sig}{solve.last.sig} of
\figref{iterator-excerpts}. 
Function @iter_last@ on \lineref{solve.last.sig} maps a @LOFsKont@ to a @FactKont@;
@iter_mid@ on \lineref{solve.mid.sig} maps a @FactKont@ to another @FactKont@;
and
@iter_first@ on \lineref{solve.first.sig} maps a @FactKont@ to a @Kont@.
To analyze a basic block, with speculative rewriting, we compose these
three functions, as shown in function @iter_block@ on
\linepairref{solve.block.sig}{solve.block.code}.\footnote
{To simplify the example, we conceal \ourlib's
representation of blocks.}

Function @iter_block@ is applied to continuation @set_last@
(\linepairref{set.last.sig}{set.last.*}), 
which updates the environment of facts stored in the dataflow monad.
Applying @iter_block@ to @set_last@ produces a computation of type
@DFM a ()@.
This computation reads the stored fact flowing into a block,
propagates facts
%\ifcutting\else
 through the block
%\fi
using transfer functions and
speculative rewriting, and finally updates the stored facts flowing out to
the block's successors.
Iterator functions for graphs
\ifcutting\else
 and subgraphs
\fi
perform such a
computation for every block, then repeat until stored facts stop changing.
Each iteration
runs under @withDuplicateFuel@, so
@fwd_iter@
\emph{simulates} the effects of a fuel limit, but it does
not actually consume fuel.


\iffalse


% The result types of @iter_first@, @iter_mid@, and @iter_last@
% compose nicely, so that for any basic block we can compute a function
% of type
% @LOFsKont a b -> Kont a b@.
Once we have analyzed a block, we update the facts stored in
the dataflow monad.
\ifcutting
 Function @set_last@
(\linerangeref{set.last.sig}{set.last.*}) calls @setFact@ on each fact
flowing out of a block.
\else
The facts are updated by function @set_last@
(\linerangeref{set.last.sig}{set.last.*}), which calls @setFact@ on each fact
flowing out of a block.
\fi
By applying the composition
of @iter_first@, @iter_mid@, and @iter_last@ to @set_last@, we get
the implementation of @iter_block@.\footnote
{Similar
  compositions of @iter\_first@, @iter\_mid@, and @iter\_last@
  are used to analyze the partial blocks in a graph that is open
  at entry or exit.}
The iterator calls @iter_block@ repeatedly
\ifcutting\else
 over all the blocks
 in the control-flow graph
\fi
until facts stop changing.
\fi


\ifpagetuning\enlargethispage{0.3\baselineskip}\fi

Computations in @fwd_iter@ are compositions
of @iter_first@, @iter_mid@, and @iter_last@.
Because these functions so
resemble one another, we show only
one:
@iter_mid@, on
\linerangeref{solve.mid.1}{solve.mid.*} of \figref{iterator-excerpts}.
On~\lineref{solve.mid.case}, a~rewrite function gets an
input fact~@in'@ and a middle node~@m@.
If~the rewrite function proposes no replacement graph, 
or if no fuel is available, the application of @with_fuel@
returns @return Nothing@, and continuation~@k@ is given the output fact
(computed 
\iffalse
on \lineref{solve.mid.Nothing} by @ft_middle_out@).
\else
by @ft_middle_out@ on \lineref{solve.mid.Nothing}).
\fi
% and the current supply of fuel.
%
The interesting case occurs on \linerangeref{solve.mid.Just}{solve.mid.*},
when the rewrite function 
proposes a replacement graph~@g@.
Function @with_fuel@ decrements the fuel supply and produces~@g@.
\begin{enumerate}
\item
If we are doing \emph{deep} rewriting, then as @g@~is analyzed,
it may be rewritten further.
Because @g@~replaces a middle node, it is open at entry and exit,
so it is analyzed and rewritten 
on~\lineref{solve.OO}
by a recursive call to @iter_OO@ 
\label{haskell.def.iter:unOO}% automated definition
(implementation not shown;
type \nrmono{GF m l O O -> FactKont a b -> FactKont a b}).
The recursive call gets continuation @return@, and the
resulting @FactKont a a@ is given
the input fact.
The output fact is computed in a sub-analysis
and bound on
\lineref{solve.mid.Just}. 
%
Function @subAnalysis@ rolls back the facts mutated
by @iter_OO@%
\ifcutting
\else
as it iterates to a fixed point%
\fi, but @subAnalysis@
does account for 
fuel consumed by @iter_OO@.
\item
If we are doing \emph{shallow} rewriting,  the new graph~@g@ must not be
rewritten, but we must still find a fixed point of the transfer
equations.
We compute that fixed point using @anal_f_OO@ (\lineref{solve.rewrite.shallow.1}).
\label{haskell.def.anal:unf:unOO}% automated definition
Function @anal_f_OO@ (not shown) recursively calls
@fwd_iter@ using
\nrmono{with\char`\_fuel = \char`\\{ }\char`\_ -> return Nothing}, 
and so it does no rewriting and consumes no fuel.
\finalremark{We hope that cunning transfer functions will make this
higher-order function moot.}
\delendum{This business of passing in a different @with\_fuel@ seems terribly
clumsy to me.  The obvious thing would be to add a third constructor \texttt{NoRewrite}
to the @RewritingDepth@ type, so that we could call @fwd\_iter@ saying
``don't do any rewriting at all''.  Would that even eliminate the higher order
@with\_fuel@ parameter altogether?  What is it used for? 
NR:~If you re-examine the structure of the case expressions, you'll
see that \texttt{NoRewrite} as a third constructor would leave to
inexhaustive pattern matching.  A Boolean would do.
Function @with\_fuel@ is also
used to decrement the fuel supply.
Combining the decrement with the test in a higher-order function
simplified the code significantly (and made it impossible to forget to
decrement).
}
%
\end{enumerate}
Whether rewriting is shallow or deep, the application on
\lineref{solve.mid.*} solves the rest of the graph by applying the
continuation~@k@%
\ifcutting.
\else\
to the new fact~@a@.
\fi







Function @zdfSolveFwd@ is implemented by calling @fwd_iter@
with the transfer functions given, with undefined rewrite functions, and with 
parameter @with_fuel = \ _ -> return Nothing@.



\ifpagetuning\enlargethispage{0.3\baselineskip}\fi

\subsection{The forward actualizer}



\seclabel{forward-actualizer}


An~iterator returns dataflow facts, leaving the graph
unchanged. 
An~\emph{actualizer} takes facts and a graph, and in a single
pass uses rewrite functions to create a new graph.
The~actualizer also uses transfer functions to materialize facts
on edges within basic blocks.


The forward actualizer closely resembles the
forward iterator, but because the actualizer passes a rewritten graph as
an accumulating parameter, the continuations have
different types, as shown on \linepairref{GraphFactKont}{GraphKont} of
\figref{actualizer-excerpts}. 
When the actualizer runs, the dataflow monad already contains a fixed
point, so there is no need to propagate facts out of a block, and
so no continuation analogous 
to @LOFsKont@.

The functions that actualize rewrites are again in
continuation-passing style;
\linerangeref{rew.first}{rew.last}  of
\figref{actualizer-excerpts} give the types of the base-case
functions.
%
We show only @ar_mid@ (\linerangeref{rew.mid.1}{rew.mid.*}).
It~is much like function @iter_mid@ on 
\linerangeref{solve.mid.1}{solve.mid.*} of \figref{iterator-excerpts}.
\Lineref{rew.mid.1} shows the additional parameter @head@, which
contains the (rewritten) graph preceding middle node~@m@.
When no rewrite is proposed, the only change to the code is that
continuation~@k@ takes the additional parameter
@head <*> mkMiddle m@,
which is the graph formed by concatenating graph @head@ and node~@m@. 
When a rewrite is proposed, the sub-analysis computes not just an
output fact but also a possibly rewritten graph
(\linerangeref{rew.subAnalysis}{rew.anal.f.OO}).
Rewriting proceeds with the new graph @head <*> g@
(\lineref{rew.mid.*}).

The recursive iterate-and-actualize-rewrites function  @iar_OO@ (type on
\lineref{iar.OO}, implementation not shown) 
has no counterpart in the iterator.
It calls the iterator to set the dataflow facts to a fixed
point (using a duplicate fuel supply), then calls actualize-rewrite
functions to 
rewrite the graph based on 
those facts (using the shared fuel supply). 
Similar functions apply to graphs of other shapes; for example,
@iar_OC@ is used to implement
\label{haskell.def.iar:unOC}% automated definition
\label{haskell.firstuse.iar:unOC}% automated use
@zdfRewriteFwd@.


\section{Conclusions}

Compiler textbooks make dataflow optimization appear
difficult and complicated.
In~this paper, we show how to engineer a library, \ourlib, which makes
it easy to build analyses and transformations based on dataflow.
\ourlib\ makes dataflow simple not by using a single magic
ingredient, but by applying ideas that are well understood by 
the programming-language community.
\begin{itemize}
\item
We acknowledge only one program-analysis technique: the solution of
recursion equations over assertions.
%Like our colleagues working in imperative languages, 
We solve the equations by iterating to a fixed point.
% Many equations relate
% properties of program states; some relate properties of paths through
% programs. 
\item
We consider only two
ways of relating assertions: weakest liberal precondition and strongest 
postcondition, which
 correspond 
%\ifpagetuning\else
%respectively
%\fi
to
\ifcutting
``backward'' and ``forward'' dataflow
problems.
\else
``backward dataflow problems'' and ``forward dataflow
problems.''
\fi
\finalremark
{Can we give an example of a property of program states which is
neither, just by way of contrast; ie this we cannot do.}
%%  \item
%%  In a language that admits loops, iterating weakest preconditions or
%%  strongest postconditions typically does not reach a fixed point in
%%  finitely many steps; hence the need for loop invariants
%%  \cite{hoare:axiomatic-basis,dijkstra:discipline,gries:science-programming}.
%%  In \secref{logic-reconciled}, we show that we can guarantee to reach a
%%  fixed point by limiting what we can express in the logic.\remark{needs
%%  a fix}
%%  We~show that many classic analyses can be explained this way;
%%  the great diversity of classic analyses corresponds to a great
%%  diversity of inexpressive logics.
%%  This view leads us to a unifying principle:
%%  \emph{To implement a code-improving transformation, find the least
%%    expressive logic that can justify the transformation, then use that
%%    logic to compute strongest postconditions, which justify the
%%    transformation locally.}
\item
Although our implementation allows graph nodes to be rewritten in any
way that preserves semantics, we describe
three program-transformation techniques:
substitution of equals for equals, 
insertion of assignments to unobserved variables, 
and removal of assignments to unobserved variables
(\secref{example:transforms}).
Substitution of equals for equals is often justified by properties of program
states; for example, if variable~$x$
is always~7, we may substitute~7 for~$x$.\finalremark
{We can also justify substitution of \emph{labels} in goto
  statements by reasoning about continuations.  This is
  probably not the place to mention this fact.}
Insertion and removal of assignments are often justified by properties
of paths through programs;
for example, if an assignment's continuation does not use the variable
assigned~to, that assignment may be removed.

%%  Some compiler texts treat the removal of unreachable code as a
%%  code-improving transformation in its own right.
%%  In~our framework, unreachable code becomes unreachable in the
%%  garbage-collection sense, so no special effort is required to remove
%%  it.
\item
Complex program transformations should be composed from simple
transformations. 
For example, both ``code motion'' and ``induction-variable
elimination'' can be implemented in three stages: insert new assignments;
substitute equals for equals; remove unneeded assignments
(\secref{induction-var-elim}). 

\item 
Because each rewrite leaves the semantics
of the program unchanged, 
we can use 
``optimization fuel'' to limit the number of rewrites.
 When we isolate a fault 
\ifcutting\else in the optimizer \fi
(\secref{vpoiso}), we 
\ifcutting have to debug just \else therefore have the luxury of debugging \fi
 a single
 rewrite, not a complex transformation.
\end{itemize}

We also build on proven implementation techniques
in a way that
makes it easy
to implement classic code improvements.
\begin{itemize}
\item
We use the algorithm of \citet{lerner-grove-chambers:2002} to 
compose analyses and transformations.
This~algorithm makes it easy to compose complex transformations
from simple ones.

Using continuation-passing style and generalized algebraic data types,
we have created a new implementation%
\ifcutting\else\ of the algorithm\fi, 
which works by 
composing three relatively simple functions
(\secref{forward-iterator}). 
The functions are simple
because the static type of a node constrains the number of predecessors
and successors it may have.
And because we can
compare our code with a standard continuation semantics, we have more
confidence in this new implementation than in 
any previous implementation. 
\item
Our code is pure.
Inspired by Huet's~\citeyearpar{huet:zipper} zipper,
we use an applicative representation of
control-flow graphs
\cite{ramsey-dias:applicative-flow-graph}. 
We~improve on our prior work by storing changing dataflow facts
in an explicit dataflow monad,
which
makes it especially easy to implement such
operations as sub-analysis of a replacement graph
(\secref{dataflow-monad});
by using static types to guarantee that each replacement graph can be
spliced in place of the node it replaces
(\secreftwo{subgraphs}{rewrite-functions});
and by simplifying our implementation using continuation-passing style
(\secreftwo{forward-iterator}{forward-actualizer}). 
%
% important, but no longer mentioned in this paper:
%
%%  \item
%%  To \emph{construct} programs, we use a different representation of
%%  flow graphs, one which hides the complexity of the zipper and which
%%  provides a constant-time operation for joining flow graphs in
%%  sequence.
%%  It is inspired in part by Hughes's \citeyearpar{hughes:novel-lists}
%%  representation of lists, which supports a constant-time append operation.
\item
\ourlib\ is polymorphic in the
representations of 
assignments and control-flow operations.
%%  Although our polymorphic representations have been instantiated only
%%  with the low-level intermediate code used by the Glasgow Haskell
%%  Compiler, they are intended eventually to be instantiated with
%%  machine-dependent representations of target-machine instructions, as
%%  part of a larger project of refactoring GHC's back ends.
%
%This design seems obvious in retrospect,
%but we underestimated the degree to which polymorphism would force us to
%separate concerns.
%Introducing polymorphism has made the code simpler, easier
%to understand, and easier to maintain.
By forcing us to separate concerns, introducing polymorphism
made the code simpler, easier to understand, and easier to maintain.
\finalremark
{SLPJ: Is it possible to substantiate this claim by [more] examples?}
In particular, it forced us to make explicit \emph{exactly} what
\ourlib\ 
 must know about flow-graph nodes:
it must be able to find
targets of control-flow operations (constraint
@HavingSuccessors l@, \secref{zdfSolveFwd}).
\end{itemize}
%
% gen and kill are history
%
%%\item
%%Judicious use of Haskell type classes makes is possible to write
%%weakest precondition or strongest postcondition using the ``transfer
%%equations'' that are familiar from compiler textbooks.
%%If you like, you can even write overloaded @gen@ and @kill@ functions.
%%The benefit is that it is easy to compare the actual code with the
%%abstract treatments found in textbooks\ifgenkill \ (\secref{gen-kill})\fi.
Using \ourlib,
you can create a new code improvement in three steps:
create a lattice representation for the assertions you want to
express;
create transfer functions that approximate weakest preconditions or
strongest postconditions;
and 
create rewrite functions that use your assertions to justify
program transformations.  
You can get quickly to the real 
intellectual work of code improvement: identifying interesting
transformations and the assertions that justify them.

\finalremark{Don't forget acknowledgements!!!
Microsoft, Intel, NSF
}

\makeatother

\providecommand\includeftpref{\relax}
\IfFileExists{nrbib.tex}{\bibliography{cs,ramsey}}{\bibliography{cs,ramsey,simon,jd}}
\bibliographystyle{plainnatx}


\clearpage

\appendix


\section{Index of defined identifiers}

This appendix lists every nontrivial identifier used in the body of
the paper.  
For each identifier, we list the page on which that identifier is
defined or discussed---or when appropriate, the figure (with line
number where possible).
For those few identifiers not defined or discussed in text, we give
the type signature and the page on which the identifier is first
referred to.

Some identifiers used in the text are defined in the Haskell Prelude;
for those readers less familiar with Haskell, these identifiers are
listed in Appendix~\ref{sec:prelude}.

\newcommand\dropit[3][]{}

\newcommand\hsprelude[2]{\noindent
  \texttt{#1} defined in the Haskell Prelude\\}
\let\hsprelude\dropit

\newcommand\hspagedef[3][]{\noindent
  \texttt{#2} defined on page~\pageref{#3}.\\}
\newcommand\omithspagedef[3][]{\noindent
  \texttt{#2} not shown (but see page~\pageref{#3}).\\}
\newcommand\omithsfigdef[3][]{\noindent
  \texttt{#2} not shown (but see Figure~\ref{#3} on page~\pageref{#3}).\\}
\newcommand\hsfigdef[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} defined in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hstabdef[3][]{%
  \noindent
  \ifx!#1!
    \texttt{#2} defined in Table~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Table~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hspagedefll[3][]{\noindent
  \texttt{#2} {let}- or $\lambda$-bound on page~\pageref{#3}.\\}
\newcommand\hsfigdefll[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} {let}- or $\lambda$-bound in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} {let}- or $\lambda$-bound on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    

\newcommand\nothspagedef[3][]{\notdefd\ndpage{#1}{#2}{#3}}
\newcommand\nothsfigdef[3][]{\notdefd\ndfig{#1}{#2}{#3}}
\newcommand\nothslinedef[3][]{\notdefd\ndline{#1}{#2}{#3}}

\newcommand\ndpage[3]{\texttt{#2}~(p\pageref{#3})}
\newcommand\ndfig[3]{\texttt{#2}~(Fig~\ref{#3},~p\pageref{#3})}
\newcommand\ndline[3]{%
  \ifx!#1!%
      \ndfig{#1}{#2}{#3}%
  \else
      \texttt{#2}~(Fig~\ref{#3}, line~\lineref{#1}, p\pageref{#3})%
  \fi
}



\newif\ifundefinedsection\undefinedsectionfalse

\newcommand\notdefd[4]{%
  \ifundefinedsection
    , #1{#2}{#3}{#4}%
  \else
    \undefinedsectiontrue
    \par
    \section{Undefined identifiers}
    #1{#2}{#3}{#4}%
  \fi
}

\begingroup
\raggedright

\input{defuse}%
\ifundefinedsection.\fi

\undefinedsectionfalse


\renewcommand\hsprelude[2]{\noindent
  \ifundefinedsection
    , \texttt{#1}%
  \else
    \undefinedsectiontrue
    \par
    \section{Identifiers defined in Haskell Prelude}\label{sec:prelude}
    \texttt{#1}%
  \fi
}
\let\hspagedef\dropit
\let\omithspagedef\dropit
\let\omithsfigdef\dropit
\let\hsfigdef\dropit
\let\hstabdef\dropit
\let\hspagedefll\dropit
\let\hsfigdefll\dropit
\let\nothspagedef\dropit
\let\nothsfigdef\dropit
\let\nothslinedef\dropit

\input{defuse}
\ifundefinedsection.\fi



\endgroup


\iffalse

\section{Dataflow-engine functions}


\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward iterator}
\end{figure*}

\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward actualizer}
\end{figure*}


\fi



\end{document}



% Old captions' text:
% The dataflow fact for the available-reload analysis describes
%   the set of registers for which a reload is available.
%   We list the types of the functions that manipuate sets of available registers,
%   as well as the definition of the lattice.
% The standard gen and kill functions for available expressions
% The transfer functions for the available-reloads analysis.
% Running the available-reloads analysis and extracting the results with \texttt{zdfFpFacts}
% The rewrite functions to insert redundant reloads immediately before uses

% Probably no space for the implementations:
% interAvail (UniverseMinus s) (UniverseMinus s') =
%   UniverseMinus (s `plusVarSet`  s')
% interAvail (AvailVars     s) (AvailVars     s') =
%   AvailVars (s `timesVarSet` s')
% interAvail (AvailVars     s) (UniverseMinus s') =
%   AvailVars (s  `minusVarSet` s')
% interAvail (UniverseMinus s) (AvailVars     s') =
%   AvailVars (s' `minusVarSet` s )
% 
% smallerAvail (AvailVars _) (UniverseMinus _) = True
% smallerAvail (UniverseMinus _) (AvailVars _) = False
% smallerAvail (AvailVars     s) (AvailVars    s')  =
%   sizeVarSet s < sizeVarSet s'
% smallerAvail (UniverseMinus s) (UniverseMinus s') =
%   sizeVarSet s > sizeVarSet s'
% 
% extendAvail (UniverseMinus s) r =
%   UniverseMinus (deleteFromVarSet s r)
% extendAvail (AvailVars     s) r =
%   AvailVars (extendVarSet s r)
% 
% delFromAvail (UniverseMinus s) r =
%   UniverseMinus (extendVarSet s r)
% delFromAvail (AvailVars     s) r =
%   AvailVars (deleteFromVarSet s r)
% 
% elemAvail (UniverseMinus s) r =
%   not $ elemVarSet r s
% elemAvail (AvailVars     s) r =
%   elemVarSet r s
